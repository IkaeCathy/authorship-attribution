{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv 5531\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv 7464\n",
      "length of features used = 12995\n",
      "len(all_training_text) 383\n",
      "mean length =  9713.52219321149\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0.5512820512820513\n",
    "0.5384615384615384\n",
    "0.5384615384615384\n",
    "0.6282051282051282\n",
    "0.5\n",
    "0.6666666666666666\n",
    "0.6282051282051282\n",
    "\n",
    "  Manhattan 0.5512820512820513\n",
    "  Canberra  0.5\n",
    "  Bray-Curtis 0.628\n",
    "  Cosine  0.539\n",
    "  Jaccard 0.667\n",
    "  yule. 0.628\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.51\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.51\n",
    "  yule. 0.628\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0.5512820512820513\n",
    "0.5384615384615384\n",
    "0.5384615384615384\n",
    "0.6282051282051282\n",
    "0.5\n",
    "0.6666666666666666\n",
    "0.6282051282051282\n",
    "\n",
    "  Manhattan 0.5512820512820513\n",
    "  Canberra  0.5\n",
    "  Bray-Curtis 0.628\n",
    "  Cosine  0.539\n",
    "  Jaccard 0.667\n",
    "  yule. 0.628\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#ProbD\n",
    "\"\"\"\n",
    "0.5512820512820513\n",
    "0.5384615384615384\n",
    "0.5384615384615384\n",
    "0.6282051282051282\n",
    "0.5\n",
    "0.6666666666666666\n",
    "0.6282051282051282\n",
    "\n",
    "  Manhattan 0.5512820512820513\n",
    "  Canberra  0.5\n",
    "  Bray-Curtis 0.628\n",
    "  Cosine  0.539\n",
    "  Jaccard 0.667\n",
    "  yule. 0.628\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016ProbD.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#ProbD\n",
    "\"\"\"\n",
    "0.5512820512820513\n",
    "0.5384615384615384\n",
    "0.5384615384615384\n",
    "0.6282051282051282\n",
    "0.5\n",
    "0.6666666666666666\n",
    "0.6282051282051282\n",
    "\n",
    "  Manhattan 0.5512820512820513\n",
    "  Canberra  0.5\n",
    "  Bray-Curtis 0.628\n",
    "  Cosine  0.539\n",
    "  Jaccard 0.667\n",
    "  yule. 0.628\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt 100\n",
      "length of features used = 300\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#ProbD\n",
    "\"\"\"\n",
    "\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt 100\n",
      "length of features used = 500\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#ProbD\n",
    "\"\"\"\n",
    "\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ProbD\n",
    "\"\"\"\n",
    "\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt 100\n",
      "length of features used = 300\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ProbD\n",
    "\"\"\"\n",
    "\n",
    "  Manhattan 0.53\n",
    "  Canberra  0.53\n",
    "  Bray-Curtis 0.53\n",
    "  Cosine  0.50\n",
    "  Jaccard 0.50\n",
    "  yule. 0.50\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016GSS.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "mean length =  14816.32637075718\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "euclidean K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "canberra K-Nearest Neighbours\n",
      "0.46153846153846156\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "yule K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "selector = SelectKBest(score_func=chi2, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "canberra K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "yule K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "selector = SelectKBest(score_func=chi2, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "cosine K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "canberra K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "yule K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.47435897435897434\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.44871794871794873\n",
      "canberra K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "yule K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "yule K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv 115184\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv 139886\n",
      "length of features used = 255070\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "canberra K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6538461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "cosine K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "yule K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "canberra K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "euclidean K-Nearest Neighbours\n",
      "0.47435897435897434\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "canberra K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv 115184\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv 139886\n",
      "length of features used = 255070\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "canberra K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6538461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.47435897435897434\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "cosine K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv 115184\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv 139886\n",
      "length of features used = 255070\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5128205128205128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.48717948717948717\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.44871794871794873\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:259: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:259: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:259: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5128205128205128\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"//Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016RF.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2016TermSelection/outputNewVersion2016WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F')]\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2...f_classif...mutual_info_classif.......................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F')]\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2...f_classif...mutual_info_classif.......................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F')]\n",
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2...f_classif...mutual_info_classif.......................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.6025641025641025\n",
      "SVM****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5384615384615384\n",
      "RandomForest_200****************************************************************\n",
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv 115184\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv 139886\n",
      "length of features used = 255070\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.5\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.6025641025641025\n",
      "RandomForest_200****************************************************************\n",
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv 115184\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv 139886\n",
      "length of features used = 255070\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6153846153846154\n",
      "LogisticRegression****************************************************************\n",
      "0.5128205128205128\n",
      "SGDClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.5641025641025641\n",
      "Naive****************************************************************\n",
      "0.5\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5641025641025641\n",
      "RandomForest_200****************************************************************\n",
      "0.5512820512820513\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6025641025641025\n",
      "BaggingClassifier****************************************************************\n",
      "0.5897435897435898\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.6282051282051282\n"
     ]
    }
   ],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivevoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 26 negative = 18\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 27 negative = 17\n",
      "MLPClassifier****************************************************************\n",
      "positive= 17 negative = 16\n",
      "LogisticRegression****************************************************************\n",
      "positive= 27 negative = 18\n",
      "SGDClassifier****************************************************************\n",
      "positive= 28 negative = 18\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 20 negative = 15\n",
      "Naive****************************************************************\n",
      "positive= 26 negative = 16\n",
      "SVM****************************************************************\n",
      "positive= 21 negative = 11\n",
      "RandomForest_100****************************************************************\n",
      "positive= 13 negative = 8\n",
      "RandomForest_200****************************************************************\n",
      "positive= 13 negative = 7\n",
      "XGBClassifier****************************************************************\n",
      "positive= 9 negative = 7\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 8 negative = 5\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 0 negative = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= manhattan_k5voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= manhattan_k13voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= MLPClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= LogisticRegressionvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= SGDClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= DecisionTreeClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= Naivevoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= SVMvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= RandomForest_100voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= RandomForest_200voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= XGBClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= BaggingClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= AdaBoostClassifiervoc \n",
    "y= AdaBoostClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(X_train)= 383 len(y_train)= 383\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.48717948717948717\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769230769230769\n",
      "LogisticRegression****************************************************************\n",
      "0.5128205128205128\n",
      "SGDClassifier****************************************************************\n",
      "0.5\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6282051282051282\n",
      "Naive****************************************************************\n",
      "0.6025641025641025\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5128205128205128\n",
      "RandomForest_200****************************************************************\n",
      "0.5384615384615384\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6410256410256411\n",
      "BaggingClassifier****************************************************************\n",
      "0.6538461538461539\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.6282051282051282\n"
     ]
    }
   ],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivefs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 23 negative = 12\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 25 negative = 12\n",
      "MLPClassifier****************************************************************\n",
      "positive= 18 negative = 12\n",
      "LogisticRegression****************************************************************\n",
      "positive= 23 negative = 12\n",
      "SGDClassifier****************************************************************\n",
      "positive= 27 negative = 15\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 17 negative = 15\n",
      "Naive****************************************************************\n",
      "positive= 18 negative = 14\n",
      "SVM****************************************************************\n",
      "positive= 27 negative = 15\n",
      "RandomForest_100****************************************************************\n",
      "positive= 22 negative = 11\n",
      "RandomForest_200****************************************************************\n",
      "positive= 21 negative = 12\n",
      "XGBClassifier****************************************************************\n",
      "positive= 14 negative = 13\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 13 negative = 11\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= manhattan_k5fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= manhattan_k13fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= MLPClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= LogisticRegressionfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= SGDClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= DecisionTreeClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= Naivefs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= SVMfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= RandomForest_100fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= RandomForest_200fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= XGBClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= BaggingClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= BaggingClassifierfs \n",
    "y= AdaBoostClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 221453 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\\ntexts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\\nCLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :Tokenize and Prepare Vocabulary\n",
    "num_labels = 2\n",
    "#vocab_size = 15000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 344 samples, validate on 39 samples\n",
      "Epoch 1/30\n",
      "344/344 [==============================] - 0s 613us/step - loss: 4.3568 - acc: 0.5000 - val_loss: 10.3318 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "344/344 [==============================] - 0s 108us/step - loss: 3.0410 - acc: 0.5087 - val_loss: 0.5492 - val_acc: 0.6667\n",
      "Epoch 3/30\n",
      "344/344 [==============================] - 0s 104us/step - loss: 1.5248 - acc: 0.6221 - val_loss: 2.8318 - val_acc: 0.1795\n",
      "Epoch 4/30\n",
      "344/344 [==============================] - 0s 110us/step - loss: 0.8734 - acc: 0.6570 - val_loss: 0.2203 - val_acc: 0.9744\n",
      "Epoch 5/30\n",
      "344/344 [==============================] - 0s 103us/step - loss: 0.6368 - acc: 0.6686 - val_loss: 1.4713 - val_acc: 0.2051\n",
      "Epoch 6/30\n",
      "344/344 [==============================] - 0s 112us/step - loss: 0.4579 - acc: 0.8140 - val_loss: 0.3845 - val_acc: 0.8974\n",
      "Epoch 7/30\n",
      "344/344 [==============================] - 0s 101us/step - loss: 0.3626 - acc: 0.8605 - val_loss: 1.0430 - val_acc: 0.4103\n",
      "Epoch 8/30\n",
      "344/344 [==============================] - 0s 114us/step - loss: 0.2858 - acc: 0.8721 - val_loss: 0.3992 - val_acc: 0.8718\n",
      "Epoch 9/30\n",
      "344/344 [==============================] - 0s 109us/step - loss: 0.2297 - acc: 0.9273 - val_loss: 1.2969 - val_acc: 0.4103\n",
      "Epoch 10/30\n",
      "344/344 [==============================] - 0s 97us/step - loss: 0.2171 - acc: 0.9215 - val_loss: 0.5206 - val_acc: 0.8462\n",
      "Epoch 11/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.1524 - acc: 0.9622 - val_loss: 1.0058 - val_acc: 0.6667\n",
      "Epoch 12/30\n",
      "344/344 [==============================] - 0s 100us/step - loss: 0.1305 - acc: 0.9651 - val_loss: 0.9812 - val_acc: 0.6667\n",
      "Epoch 13/30\n",
      "344/344 [==============================] - 0s 101us/step - loss: 0.1308 - acc: 0.9419 - val_loss: 0.6030 - val_acc: 0.8205\n",
      "Epoch 14/30\n",
      "344/344 [==============================] - 0s 118us/step - loss: 0.0821 - acc: 0.9855 - val_loss: 1.0776 - val_acc: 0.7179\n",
      "Epoch 15/30\n",
      "344/344 [==============================] - 0s 108us/step - loss: 0.0512 - acc: 0.9884 - val_loss: 1.0812 - val_acc: 0.6923\n",
      "Epoch 16/30\n",
      "344/344 [==============================] - 0s 99us/step - loss: 0.0293 - acc: 0.9971 - val_loss: 0.8194 - val_acc: 0.7949\n",
      "Epoch 17/30\n",
      "344/344 [==============================] - 0s 95us/step - loss: 0.0231 - acc: 0.9971 - val_loss: 1.1509 - val_acc: 0.7179\n",
      "Epoch 18/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.0152 - acc: 0.9971 - val_loss: 0.9575 - val_acc: 0.7949\n",
      "Epoch 19/30\n",
      "344/344 [==============================] - 0s 92us/step - loss: 0.0114 - acc: 0.9971 - val_loss: 1.0250 - val_acc: 0.7692\n",
      "Epoch 20/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.2845 - val_acc: 0.7179\n",
      "Epoch 21/30\n",
      "344/344 [==============================] - 0s 95us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 1.2423 - val_acc: 0.6923\n",
      "Epoch 22/30\n",
      "344/344 [==============================] - 0s 109us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 1.1156 - val_acc: 0.7949\n",
      "Epoch 23/30\n",
      "344/344 [==============================] - 0s 108us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 1.1672 - val_acc: 0.7179\n",
      "Epoch 24/30\n",
      "344/344 [==============================] - 0s 99us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.2043 - val_acc: 0.7179\n",
      "Epoch 25/30\n",
      "344/344 [==============================] - 0s 94us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.2193 - val_acc: 0.7436\n",
      "Epoch 26/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 1.2635 - val_acc: 0.7179\n",
      "Epoch 27/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.2517 - val_acc: 0.7436\n",
      "Epoch 28/30\n",
      "344/344 [==============================] - 0s 129us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.2560 - val_acc: 0.7436\n",
      "Epoch 29/30\n",
      "344/344 [==============================] - 0s 106us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.2791 - val_acc: 0.7436\n",
      "Epoch 30/30\n",
      "344/344 [==============================] - 0s 103us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.3141 - val_acc: 0.7436\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 33us/step\n",
      "Test accuracy: 0.7435897588729858\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier****************************************************************\n",
      "0.5641025641025641\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k5tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k13tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= MLPClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= LogisticRegressiontfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SGDClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= DecisionTreeClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= Naivetfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SVMtfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= RandomForest_100tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= RandomForest_200tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= XGBClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= BaggingClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= AdaBoostClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "labels = train_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 1000\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "test_labels = test_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "test_t = Tokenizer()\n",
    "test_t.fit_on_texts(test_docs)\n",
    "test_vocab_size = len(test_t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "\n",
    "test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# create the model_5\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=False, validation_data=(test_padded_docs, test_labels), batch_size=10)\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=False)\n",
    "print(\"odel_5 Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=False)\n",
    "print(\"odel_5 Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n",
    "\n",
    "        \n",
    "# define model_1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= True))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(padded_docs, labels, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_1 Accuracy: %f' % (acc*100))\n",
    "        \n",
    "# define model_2\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= True))\n",
    "#model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= False))\n",
    "model.add(Conv1D(50, 5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(padded_docs.shape)\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_2 Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# create the model_3\n",
    "top_words = 5000\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "#model.add(Embedding(top_words, len(embedding_matrix), input_length=max_length))\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= False))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print(\"model_3 Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# create the model_4\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_4 Accuracy: %f' % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# create the model_5\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=False, validation_data=(test_padded_docs, test_labels), batch_size=10)\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=False)\n",
    "print(\"odel_5 Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=False)\n",
    "print(\"odel_5 Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
