{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len_male_content = 74 len_female_content =  73\n",
      "len(all_training_text) 147\n",
      "mean length =  2787.598639455782\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len_male_content = 39 len_female_content =  39\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    print(\"len_male_content =\", len(male_content), \"len_female_content = \", len(female_content))\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    #print(len([x for item in all_txt_per_person for x in item]))\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014-blog-txtfiles/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014PMI.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014ProbD.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt 300\n",
      "length of features used = 300\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt 500\n",
      "length of features used = 500\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014CHI.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt 300\n",
      "length of features used = 300\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt 500\n",
      "length of features used = 500\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n",
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/EB2014Selection/output2014GSS.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    " #   with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    " #       next(reader) # skip header\n",
    " #       word_list1 =  [r[0] for r in reader]\n",
    " #       print(txt_file, len(word_list1))\n",
    " #       word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "cosine K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "canberra K-Nearest Neighbours\n",
      "0.47435897435897434\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "yule K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "cosine K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "yule K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "cosine K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "canberra K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "selector = SelectKBest(score_func=chi2, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector=SelectKBest(score_func=mutual_info_classif,k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "cosine K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "canberra K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "print(\"selector=SelectKBest(score_func=mutual_info_classif,k=200)\")\n",
    "selector=SelectKBest(score_func=mutual_info_classif,k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector=SelectKBest(score_func=mutual_info_classif,k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "canberra K-Nearest Neighbours\n",
      "0.47435897435897434\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "print(\"selector=SelectKBest(score_func=mutual_info_classif,k=300)\")\n",
    "selector=SelectKBest(score_func=mutual_info_classif,k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv 1196\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv 1636\n",
      "length of features used = 2832\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector=SelectKBest(score_func=mutual_info_classif,k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "canberra K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "yule K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/2014_blog_Tokens/PAN2014_blog_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "print(\"selector=SelectKBest(score_func=mutual_info_classif,k=500)\")\n",
    "selector=SelectKBest(score_func=mutual_info_classif,k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "canberra K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6923076923076923\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "cosine K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "canberra K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "selector = SelectKBest(score_func=chi2, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "cosine K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "selector = SelectKBest(score_func=chi2, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "cosine K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "yule K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "cosine K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "cosine K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "cosine K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "canberra K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "cosine K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "canberra K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "jaccard K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "yule K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "cosine K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "canberra K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "yule K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6153846153846154\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product(78, 0.6794871794871795)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "len of truth_y_pre = 50\n",
      "len of false_y_pre = 28\n",
      "0.6410256410256411\n",
      "manhattan K-Nearest Neighbours\n",
      "len of truth_y_pre = 52\n",
      "len of false_y_pre = 26\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "len of truth_y_pre = 51\n",
      "len of false_y_pre = 27\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "len of truth_y_pre = 50\n",
      "len of false_y_pre = 28\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "len of truth_y_pre = 50\n",
      "len of false_y_pre = 28\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "len of truth_y_pre = 40\n",
      "len of false_y_pre = 38\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5256410256410257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "len of truth_y_pre = 46\n",
      "len of false_y_pre = 32\n",
      "0.5897435897435898\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "len of truth_y_pre = 46\n",
      "len of false_y_pre = 32\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "cosine K-Nearest Neighbours\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.6025641025641025\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5769230769230769\n",
      "yule K-Nearest Neighbours\n",
      "0.5512820512820513\n",
      "************************************ ****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5897435897435898\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5897435897435898\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/outputNewVersion201blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5641025641025641\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/outputNewVersion201blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/outputNewVersion201blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:258: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/outputNewVersion201blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:258: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6538461538461539\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:258: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion201blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6025641025641025\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogCHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogIG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogPMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogWLLR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list2 =  ([r.strip() for r in f])[0:100]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "\"\"\"\n",
    "8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eae4ff82a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mword_list2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt\", \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "\n",
    "        print(txt_file, len(word_list1))\n",
    "\n",
    "        word_list = word_list + word_list1\n",
    "\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "truth_y_pred = [i for i, j in zip(y_pred, y_test) if  i == j]\n",
    "\n",
    "print(\"len of truth_y_pre =\", len(truth_y_pred))\n",
    "\n",
    "false_y_pred = [i for i, j in zip(y_pred, y_test) if  i != j ]\n",
    "\n",
    "print(\"len of false_y_pre =\", len(false_y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F')]\n",
      "0.6666666666666666\n",
      "cosine K-Nearest Neighbours\n",
      "[('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F')]\n",
      "0.6538461538461539\n",
      "euclidean K-Nearest Neighbours\n",
      "[('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F')]\n",
      "0.6410256410256411\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F')]\n",
      "0.6410256410256411\n",
      "canberra K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F')]\n",
      "0.5128205128205128\n",
      "jaccard K-Nearest Neighbours\n",
      "[('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F')]\n",
      "0.5256410256410257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F')]\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.1M.txt\", \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogOR.2F.txt\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.1M.txt\", \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogRF.2F.txt\"]\n",
    "#txt_files =[ \"D:/NLP/PAN_TermSelection/2014TermSelection_Blog/outputNewVersion2014blogGSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "#        word_list2 =  ([r.strip() for r in f])[0:200]\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in word_list2])\n",
    "\n",
    "#        print(txt_file, len(word_list1))\n",
    "\n",
    "#        word_list = word_list + word_list1\n",
    "\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F')]\n",
      "0.6282051282051282\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2,f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "      \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F')]\n",
      "0.5897435897435898\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2,f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "      \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "braycurtis K-Nearest Neighbours\n",
      "[('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F')]\n",
      "0.5769230769230769\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2,f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif,mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "      \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7051282051282052\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.5512820512820513\n",
      "SVM****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.6025641025641025\n",
      "RandomForest_200****************************************************************\n",
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.5641025641025641\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5641025641025641\n",
      "RandomForest_200****************************************************************\n",
      "0.5512820512820513\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"GDClassifier****************************************************************\")\n",
    "clf = GDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv 12375\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv 18889\n",
      "length of features used = 31264\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6666666666666666\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6282051282051282\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717948717948718\n",
      "LogisticRegression****************************************************************\n",
      "0.5769230769230769\n",
      "SGDClassifier****************************************************************\n",
      "0.5\n",
      "DecisionTreeClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6153846153846154\n",
      "Naive****************************************************************\n",
      "0.5641025641025641\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5897435897435898\n",
      "RandomForest_200****************************************************************\n",
      "0.5384615384615384\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6025641025641025\n",
      "BaggingClassifier****************************************************************\n",
      "0.5512820512820513\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivevoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 8 negative = 4\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 11 negative = 4\n",
      "MLPClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "LogisticRegression****************************************************************\n",
      "positive= 16 negative = 5\n",
      "SGDClassifier****************************************************************\n",
      "positive= 30 negative = 13\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 18 negative = 10\n",
      "Naive****************************************************************\n",
      "positive= 15 negative = 3\n",
      "SVM****************************************************************\n",
      "positive= 30 negative = 13\n",
      "RandomForest_100****************************************************************\n",
      "positive= 15 negative = 5\n",
      "RandomForest_200****************************************************************\n",
      "positive= 16 negative = 2\n",
      "XGBClassifier****************************************************************\n",
      "positive= 16 negative = 7\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 19 negative = 6\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 22 negative = 7\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k5voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k13voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= MLPClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= LogisticRegressionvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SGDClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= DecisionTreeClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= Naivevoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SVMvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_100voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_200voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= XGBClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= BaggingClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc\n",
    "y= AdaBoostClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier****************************************************************\n",
      "0.5897435897435898\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6794871794871795\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7051282051282052\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717948717948718\n",
      "LogisticRegression****************************************************************\n",
      "0.5384615384615384\n",
      "SGDClassifier****************************************************************\n",
      "0.5\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.5897435897435898\n",
      "Naive****************************************************************\n",
      "0.5512820512820513\n",
      "SVM****************************************************************\n",
      "0.5\n",
      "RandomForest_100****************************************************************\n",
      "0.5897435897435898\n",
      "RandomForest_200****************************************************************\n",
      "0.6025641025641025\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5897435897435898\n",
      "BaggingClassifier****************************************************************\n",
      "0.6025641025641025\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivefs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 10 negative = 7\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 5 negative = 4\n",
      "MLPClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "LogisticRegression****************************************************************\n",
      "positive= 22 negative = 8\n",
      "SGDClassifier****************************************************************\n",
      "positive= 28 negative = 11\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 20 negative = 10\n",
      "Naive****************************************************************\n",
      "positive= 21 negative = 8\n",
      "SVM****************************************************************\n",
      "positive= 28 negative = 11\n",
      "RandomForest_100****************************************************************\n",
      "positive= 15 negative = 5\n",
      "RandomForest_200****************************************************************\n",
      "positive= 14 negative = 5\n",
      "XGBClassifier****************************************************************\n",
      "positive= 18 negative = 8\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 16 negative = 7\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 24 negative = 9\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= MLPClassifierfs \n",
    "y= manhattan_k5fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= MLPClassifierfs \n",
    "y= manhattan_k13fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= MLPClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= LogisticRegressionfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= SGDClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= DecisionTreeClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= Naivefs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= SVMfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= RandomForest_100fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= RandomForest_200fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= XGBClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= BaggingClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= AdaBoostClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier****************************************************************\n",
      "0.6025641025641025\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5256410256410257\n"
     ]
    }
   ],
   "source": [
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24093 unique words.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (0      [1, 0]\n1      [1, 0]\n2      [1, 0]\n3      [1, 0]\n4      [1, 0]\n5      [1, 0]\n6      [1, 0]\n7      [1, 0]\n8      [1, 0]\n9      [1, 0]\n10     [1, 0]\n11     [1, 0]\n12     [1, 0]\n13     [1, 0]\n14     [1, 0]\n15     [1, 0]\n16     [1, 0]\n17     [1, 0]\n18     [1, 0]\n19     [1, 0]\n20     [1, 0]\n21     [1, 0]\n22     [1, 0]\n23     [1, 0]\n24     [1, 0]\n25     [1, 0]\n26     [1, 0]\n27     [1, 0]\n28     [1, 0]\n29     [1, 0]\n        ...  \n117    [0, 1]\n118    [0, 1]\n119    [0, 1]\n120    [0, 1]\n121    [0, 1]\n122    [0, 1]\n123    [0, 1]\n124    [0, 1]\n125    [0, 1]\n126    [0, 1]\n127    [0, 1]\n128    [0, 1]\n129    [0, 1]\n130    [0, 1]\n131    [0, 1]\n132    [0, 1]\n133    [0, 1]\n134    [0, 1]\n135    [0, 1]\n136    [0, 1]\n137    [0, 1]\n138    [0, 1]\n139    [0, 1]\n140    [0, 1]\n141    [0, 1]\n142    [0, 1]\n143    [0, 1]\n144    [0, 1]\n145    [0, 1]\n146    [0, 1]\nName: type, Length: 147, dtype: object,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-382c1c6afeca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#same as the y train generated with my model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (0      [1, 0]\n1      [1, 0]\n2      [1, 0]\n3      [1, 0]\n4      [1, 0]\n5      [1, 0]\n6      [1, 0]\n7      [1, 0]\n8      [1, 0]\n9      [1, 0]\n10     [1, 0]\n11     [1, 0]\n12     [1, 0]\n13     [1, 0]\n14     [1, 0]\n15     [1, 0]\n16     [1, 0]\n17     [1, 0]\n18     [1, 0]\n19     [1, 0]\n20     [1, 0]\n21     [1, 0]\n22     [1, 0]\n23     [1, 0]\n24     [1, 0]\n25     [1, 0]\n26     [1, 0]\n27     [1, 0]\n28     [1, 0]\n29     [1, 0]\n        ...  \n117    [0, 1]\n118    [0, 1]\n119    [0, 1]\n120    [0, 1]\n121    [0, 1]\n122    [0, 1]\n123    [0, 1]\n124    [0, 1]\n125    [0, 1]\n126    [0, 1]\n127    [0, 1]\n128    [0, 1]\n129    [0, 1]\n130    [0, 1]\n131    [0, 1]\n132    [0, 1]\n133    [0, 1]\n134    [0, 1]\n135    [0, 1]\n136    [0, 1]\n137    [0, 1]\n138    [0, 1]\n139    [0, 1]\n140    [0, 1]\n141    [0, 1]\n142    [0, 1]\n143    [0, 1]\n144    [0, 1]\n145    [0, 1]\n146    [0, 1]\nName: type, Length: 147, dtype: object,)"
     ]
    }
   ],
   "source": [
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "num_labels = 2\n",
    "#vocab_size = 15000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(all_test_text) 78\n",
      "Found 24093 unique words.\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5384615384615384\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n",
      "0.6025641025641025\n",
      "LogisticRegression****************************************************************\n",
      "0.6282051282051282\n",
      "SGDClassifier****************************************************************\n",
      "0.6282051282051282\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.5641025641025641\n",
      "Naive****************************************************************\n",
      "0.6025641025641025\n",
      "SVM****************************************************************\n",
      "0.6153846153846154\n",
      "RandomForest_100****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5641025641025641\n",
      "RandomForest_200****************************************************************\n",
      "0.5512820512820513\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6282051282051282\n",
      "BaggingClassifier****************************************************************\n",
      "0.5641025641025641\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.6153846153846154\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 132 samples, validate on 15 samples\n",
      "Epoch 1/30\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 2.0554 - acc: 0.5303 - val_loss: 0.0944 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "132/132 [==============================] - 0s 162us/step - loss: 2.7446 - acc: 0.5076 - val_loss: 2.7116 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "132/132 [==============================] - 0s 146us/step - loss: 0.9814 - acc: 0.5606 - val_loss: 1.3158 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "132/132 [==============================] - 0s 142us/step - loss: 0.3918 - acc: 0.7576 - val_loss: 0.4533 - val_acc: 0.7333\n",
      "Epoch 5/30\n",
      "132/132 [==============================] - 0s 134us/step - loss: 0.5314 - acc: 0.8258 - val_loss: 0.5716 - val_acc: 0.7333\n",
      "Epoch 6/30\n",
      "132/132 [==============================] - 0s 132us/step - loss: 0.2509 - acc: 0.9545 - val_loss: 1.2028 - val_acc: 0.2667\n",
      "Epoch 7/30\n",
      "132/132 [==============================] - 0s 156us/step - loss: 0.2793 - acc: 0.9470 - val_loss: 1.0980 - val_acc: 0.4000\n",
      "Epoch 8/30\n",
      "132/132 [==============================] - 0s 158us/step - loss: 0.1919 - acc: 0.9924 - val_loss: 0.6191 - val_acc: 0.7333\n",
      "Epoch 9/30\n",
      "132/132 [==============================] - 0s 135us/step - loss: 0.1310 - acc: 0.9924 - val_loss: 0.4167 - val_acc: 0.8000\n",
      "Epoch 10/30\n",
      "132/132 [==============================] - 0s 146us/step - loss: 0.1089 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 0.8000\n",
      "Epoch 11/30\n",
      "132/132 [==============================] - 0s 140us/step - loss: 0.0718 - acc: 1.0000 - val_loss: 0.6865 - val_acc: 0.6667\n",
      "Epoch 12/30\n",
      "132/132 [==============================] - 0s 141us/step - loss: 0.0543 - acc: 1.0000 - val_loss: 0.8759 - val_acc: 0.5333\n",
      "Epoch 13/30\n",
      "132/132 [==============================] - 0s 135us/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.6209 - val_acc: 0.6000\n",
      "Epoch 14/30\n",
      "132/132 [==============================] - 0s 151us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.3719 - val_acc: 0.8667\n",
      "Epoch 15/30\n",
      "132/132 [==============================] - 0s 176us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2748 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "132/132 [==============================] - 0s 148us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.2608 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "132/132 [==============================] - 0s 146us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.3389 - val_acc: 0.8667\n",
      "Epoch 18/30\n",
      "132/132 [==============================] - 0s 150us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.5416 - val_acc: 0.8000\n",
      "Epoch 19/30\n",
      "132/132 [==============================] - 0s 137us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.7530 - val_acc: 0.7333\n",
      "Epoch 20/30\n",
      "132/132 [==============================] - 0s 138us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.7466 - val_acc: 0.7333\n",
      "Epoch 21/30\n",
      "132/132 [==============================] - 0s 134us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.6368 - val_acc: 0.8000\n",
      "Epoch 22/30\n",
      "132/132 [==============================] - 0s 188us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.5100 - val_acc: 0.8000\n",
      "Epoch 23/30\n",
      "132/132 [==============================] - 0s 168us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.4045 - val_acc: 0.8000\n",
      "Epoch 24/30\n",
      "132/132 [==============================] - 0s 145us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.3287 - val_acc: 0.8667\n",
      "Epoch 25/30\n",
      "132/132 [==============================] - 0s 149us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.2812 - val_acc: 0.8667\n",
      "Epoch 26/30\n",
      "132/132 [==============================] - 0s 144us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2544 - val_acc: 0.9333\n",
      "Epoch 27/30\n",
      "132/132 [==============================] - 0s 141us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2367 - val_acc: 0.9333\n",
      "Epoch 28/30\n",
      "132/132 [==============================] - 0s 148us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.9333\n",
      "Epoch 29/30\n",
      "132/132 [==============================] - 0s 140us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2158 - val_acc: 0.9333\n",
      "Epoch 30/30\n",
      "132/132 [==============================] - 0s 156us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2114 - val_acc: 0.9333\n",
      "78/78 [==============================] - 0s 33us/step\n",
      "Test accuracy: 0.6538461446762085\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 21 negative = 14\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 20 negative = 12\n",
      "MLPClassifier****************************************************************\n",
      "positive= 14 negative = 12\n",
      "LogisticRegression****************************************************************\n",
      "positive= 7 negative = 7\n",
      "SGDClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 16 negative = 11\n",
      "Naive****************************************************************\n",
      "positive= 11 negative = 9\n",
      "SVM****************************************************************\n",
      "positive= 10 negative = 9\n",
      "RandomForest_100****************************************************************\n",
      "positive= 14 negative = 9\n",
      "RandomForest_200****************************************************************\n",
      "positive= 13 negative = 7\n",
      "XGBClassifier****************************************************************\n",
      "positive= 10 negative = 10\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 14 negative = 9\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 11 negative = 10\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k5tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k13tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= MLPClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= LogisticRegressiontfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SGDClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= DecisionTreeClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= Naivetfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SVMtfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= RandomForest_100tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= RandomForest_200tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= XGBClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= BaggingClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= AdaBoostClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "len(all_training_text) 147\n",
      "len(X_train)= 147 len(y_train)= 147\n",
      "len(all_test_text) 78\n",
      "len(X_test= 78 len(y_test)= 78\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Loaded 400000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 100, 100)          2409400   \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 2,419,401\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 2,409,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 48.717949\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=[1 if x==\"M\" else 0 for x in train_tags]\n",
    "print(train_tags)\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "labels = train_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "test_labels = test_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "test_t = Tokenizer()\n",
    "test_t.fit_on_texts(test_docs)\n",
    "test_vocab_size = len(test_t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "\n",
    "test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv']\n",
      "//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv 1526\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv 2063\n",
      "length of features used = 3589\n",
      "Loaded 400000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_148 (Embedding)    (None, 1000, 100)         2409400   \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 993, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 496, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_111 (Flatten)        (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 10)                158730    \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,593,773\n",
      "Trainable params: 2,593,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 37s - loss: 0.6882 - acc: 0.5442\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.5579 - acc: 0.6599\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.4488 - acc: 0.8095\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.3480 - acc: 0.9524\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.2384 - acc: 0.9660\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.1459 - acc: 0.9864\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0860 - acc: 0.9932\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0499 - acc: 0.9932\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0174 - acc: 1.0000\n",
      "model_1 Accuracy: 41.025641\n",
      "(147, 1000)\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 33s 226ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 7.9170 - acc: 0.5034\n",
      "model_2 Accuracy: 50.000000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_150 (Embedding)    (None, 1000, 100)         2409400   \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,489,901\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 2,409,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "147/147 [==============================] - 50s 338ms/step - loss: 0.6730 - acc: 0.6259\n",
      "Epoch 2/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.6621 - acc: 0.6531\n",
      "Epoch 3/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.6497 - acc: 0.6531\n",
      "Epoch 4/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.6325 - acc: 0.6531\n",
      "Epoch 5/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.6265 - acc: 0.7007\n",
      "Epoch 6/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.6126 - acc: 0.7279\n",
      "Epoch 7/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.5869 - acc: 0.6871\n",
      "Epoch 8/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.5737 - acc: 0.6939\n",
      "Epoch 9/30\n",
      "147/147 [==============================] - 4s 28ms/step - loss: 0.5414 - acc: 0.7347\n",
      "Epoch 10/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.5257 - acc: 0.7551\n",
      "Epoch 11/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.4952 - acc: 0.7551\n",
      "Epoch 12/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.4687 - acc: 0.7755\n",
      "Epoch 13/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.4318 - acc: 0.8027\n",
      "Epoch 14/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.5236 - acc: 0.7075\n",
      "Epoch 15/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.5036 - acc: 0.7347\n",
      "Epoch 16/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.4645 - acc: 0.7891\n",
      "Epoch 17/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.4802 - acc: 0.7619\n",
      "Epoch 18/30\n",
      "147/147 [==============================] - 5s 35ms/step - loss: 0.4697 - acc: 0.7755\n",
      "Epoch 19/30\n",
      "147/147 [==============================] - 6s 39ms/step - loss: 0.4448 - acc: 0.7959\n",
      "Epoch 20/30\n",
      "147/147 [==============================] - 5s 36ms/step - loss: 0.4303 - acc: 0.8027\n",
      "Epoch 21/30\n",
      "147/147 [==============================] - 5s 33ms/step - loss: 0.4127 - acc: 0.8095\n",
      "Epoch 22/30\n",
      "147/147 [==============================] - 5s 32ms/step - loss: 0.3951 - acc: 0.8231\n",
      "Epoch 23/30\n",
      "147/147 [==============================] - 5s 31ms/step - loss: 0.3730 - acc: 0.8367\n",
      "Epoch 24/30\n",
      "147/147 [==============================] - 5s 35ms/step - loss: 0.3576 - acc: 0.8367\n",
      "Epoch 25/30\n",
      "147/147 [==============================] - 5s 35ms/step - loss: 0.3410 - acc: 0.8367\n",
      "Epoch 26/30\n",
      "147/147 [==============================] - 4s 30ms/step - loss: 0.3304 - acc: 0.8367\n",
      "Epoch 27/30\n",
      "147/147 [==============================] - 4s 29ms/step - loss: 0.3256 - acc: 0.8367\n",
      "Epoch 28/30\n",
      "147/147 [==============================] - 4s 28ms/step - loss: 0.3273 - acc: 0.8299\n",
      "Epoch 29/30\n",
      "147/147 [==============================] - 4s 27ms/step - loss: 0.3415 - acc: 0.8231\n",
      "Epoch 30/30\n",
      "147/147 [==============================] - 5s 31ms/step - loss: 0.3230 - acc: 0.8367\n",
      "model_3 Accuracy: 44.87%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_151 (Embedding)    (None, 1000, 100)         2409400   \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,775,993\n",
      "Trainable params: 366,593\n",
      "Non-trainable params: 2,409,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 22s 150ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 9s 59ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 9s 60ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 9s 60ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 9s 60ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 8s 57ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 8s 55ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 8s 53ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 9s 59ms/step - loss: 7.9170 - acc: 0.5034\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - 9s 59ms/step - loss: 7.9170 - acc: 0.5034\n",
      "model_4 Accuracy: 50.000000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_152 (Embedding)    (None, 1000, 100)         2409400   \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_113 (Flatten)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 128)               16512     \n",
      "=================================================================\n",
      "Total params: 2,654,136\n",
      "Trainable params: 244,736\n",
      "Non-trainable params: 2,409,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_227 to have shape (128,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-bdb2de087b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_227 to have shape (128,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"//Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2014blog/2014.1-blog-txtfiles/PAN2014_blog_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "#print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "#print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "#X_train = np.nan_to_num(X_train)\n",
    "#X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "labels = train_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 1000\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "test_labels = test_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "test_t = Tokenizer()\n",
    "test_t.fit_on_texts(test_docs)\n",
    "test_vocab_size = len(test_t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "\n",
    "test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# define model_1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= True))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(padded_docs, labels, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_1 Accuracy: %f' % (acc*100))\n",
    "        \n",
    "# define model_2\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= False))\n",
    "#model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= False))\n",
    "model.add(Conv1D(50, 5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(padded_docs.shape)\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_2 Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# create the model_3\n",
    "top_words = 5000\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "#model.add(Embedding(top_words, len(embedding_matrix), input_length=max_length))\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= False))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=30, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print(\"model_3 Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# create the model_4\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_4 Accuracy: %f' % (scores[1]*100))\n",
    "\n",
    "# create the model_4\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(35))  # global max pooling\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_5 Accuracy: %f' % (scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "# train word2vec model\n",
    "#model = Word2Vec(padded_docs, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv 6328\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv 8793\n",
      "length of features used = 15121\n",
      "len(all_training_text) 383\n",
      "len(all_test_text) 78\n",
      "Found 221453 unique words.\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5641025641025641\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5256410256410257\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n",
      "0.6923076923076923\n",
      "LogisticRegression****************************************************************\n",
      "0.6923076923076923\n",
      "SGDClassifier****************************************************************\n",
      "0.5897435897435898\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.5641025641025641\n",
      "Naive****************************************************************\n",
      "0.6282051282051282\n",
      "SVM****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "RandomForest_100****************************************************************\n",
      "0.6410256410256411\n",
      "RandomForest_200****************************************************************\n",
      "0.5512820512820513\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6282051282051282\n",
      "BaggingClassifier****************************************************************\n",
      "0.6410256410256411\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.5769230769230769\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 344 samples, validate on 39 samples\n",
      "Epoch 1/30\n",
      "344/344 [==============================] - 0s 805us/step - loss: 4.5869 - acc: 0.5203 - val_loss: 11.5138 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "344/344 [==============================] - 0s 103us/step - loss: 4.8218 - acc: 0.5523 - val_loss: 3.0925 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "344/344 [==============================] - 0s 105us/step - loss: 1.3310 - acc: 0.4826 - val_loss: 0.6507 - val_acc: 0.6154\n",
      "Epoch 4/30\n",
      "344/344 [==============================] - 0s 105us/step - loss: 0.7429 - acc: 0.6831 - val_loss: 1.3480 - val_acc: 0.2051\n",
      "Epoch 5/30\n",
      "344/344 [==============================] - 0s 99us/step - loss: 0.5709 - acc: 0.7006 - val_loss: 0.4376 - val_acc: 0.8462\n",
      "Epoch 6/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.4670 - acc: 0.8227 - val_loss: 1.2952 - val_acc: 0.3077\n",
      "Epoch 7/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.3770 - acc: 0.8692 - val_loss: 0.5283 - val_acc: 0.7436\n",
      "Epoch 8/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.3009 - acc: 0.8808 - val_loss: 1.1476 - val_acc: 0.5385\n",
      "Epoch 9/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.2553 - acc: 0.9099 - val_loss: 0.9295 - val_acc: 0.6667\n",
      "Epoch 10/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.2094 - acc: 0.9302 - val_loss: 0.8203 - val_acc: 0.7179\n",
      "Epoch 11/30\n",
      "344/344 [==============================] - 0s 99us/step - loss: 0.1576 - acc: 0.9506 - val_loss: 1.1288 - val_acc: 0.6667\n",
      "Epoch 12/30\n",
      "344/344 [==============================] - 0s 101us/step - loss: 0.1124 - acc: 0.9593 - val_loss: 0.8544 - val_acc: 0.7179\n",
      "Epoch 13/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.0805 - acc: 0.9826 - val_loss: 1.2912 - val_acc: 0.6410\n",
      "Epoch 14/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.0520 - acc: 0.9855 - val_loss: 0.7759 - val_acc: 0.7436\n",
      "Epoch 15/30\n",
      "344/344 [==============================] - 0s 97us/step - loss: 0.0453 - acc: 0.9942 - val_loss: 1.6451 - val_acc: 0.5641\n",
      "Epoch 16/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.0284 - acc: 0.9971 - val_loss: 1.1317 - val_acc: 0.7179\n",
      "Epoch 17/30\n",
      "344/344 [==============================] - 0s 99us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 1.5123 - val_acc: 0.6667\n",
      "Epoch 18/30\n",
      "344/344 [==============================] - 0s 104us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 1.3092 - val_acc: 0.6923\n",
      "Epoch 19/30\n",
      "344/344 [==============================] - 0s 101us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.3884 - val_acc: 0.6923\n",
      "Epoch 20/30\n",
      "344/344 [==============================] - 0s 107us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 1.5204 - val_acc: 0.6667\n",
      "Epoch 21/30\n",
      "344/344 [==============================] - 0s 104us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 1.4110 - val_acc: 0.6923\n",
      "Epoch 22/30\n",
      "344/344 [==============================] - 0s 97us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 1.4972 - val_acc: 0.6923\n",
      "Epoch 23/30\n",
      "344/344 [==============================] - 0s 100us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 1.6025 - val_acc: 0.6923\n",
      "Epoch 24/30\n",
      "344/344 [==============================] - 0s 108us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.6215 - val_acc: 0.6923\n",
      "Epoch 25/30\n",
      "344/344 [==============================] - 0s 98us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.5982 - val_acc: 0.6923\n",
      "Epoch 26/30\n",
      "344/344 [==============================] - 0s 102us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.6066 - val_acc: 0.6923\n",
      "Epoch 27/30\n",
      "344/344 [==============================] - 0s 100us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.6266 - val_acc: 0.6923\n",
      "Epoch 28/30\n",
      "344/344 [==============================] - 0s 96us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.6481 - val_acc: 0.6923\n",
      "Epoch 29/30\n",
      "344/344 [==============================] - 0s 97us/step - loss: 9.2124e-04 - acc: 1.0000 - val_loss: 1.6915 - val_acc: 0.6667\n",
      "Epoch 30/30\n",
      "344/344 [==============================] - 0s 101us/step - loss: 8.0772e-04 - acc: 1.0000 - val_loss: 1.6698 - val_acc: 0.6923\n",
      "78/78 [==============================] - 0s 33us/step\n",
      "Test accuracy: 0.7051281929016113\n"
     ]
    }
   ],
   "source": [
    "#changing the classifiers..........\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2016/output2016OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2016/2016.1_textfiles/PAN2016_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 21 negative = 11\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 20 negative = 7\n",
      "MLPClassifier****************************************************************\n",
      "positive= 10 negative = 10\n",
      "LogisticRegression****************************************************************\n",
      "positive= 0 negative = 0\n",
      "SGDClassifier****************************************************************\n",
      "positive= 17 negative = 9\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 22 negative = 12\n",
      "Naive****************************************************************\n",
      "positive= 15 negative = 10\n",
      "SVM****************************************************************\n",
      "positive= 13 negative = 11\n",
      "RandomForest_100****************************************************************\n",
      "positive= 13 negative = 9\n",
      "RandomForest_200****************************************************************\n",
      "positive= 19 negative = 8\n",
      "XGBClassifier****************************************************************\n",
      "positive= 14 negative = 9\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 13 negative = 9\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 16 negative = 7\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= manhattan_k5tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= manhattan_k13tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= MLPClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= LogisticRegressiontfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= SGDClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= DecisionTreeClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= Naivetfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= LogisticRegressiontfidf\n",
    "y= SVMtfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= LogisticRegressiontfidf\n",
    "y= RandomForest_100tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= LogisticRegressiontfidf\n",
    "y= RandomForest_200tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= XGBClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf \n",
    "y= BaggingClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= LogisticRegressiontfidf\n",
    "y= AdaBoostClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
