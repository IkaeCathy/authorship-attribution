{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_chosen_set.csv 9936\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_chosen_set.csv 9240\n",
      "length of features used = 19176\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5003144654088051\n",
      "cosine K-Nearest Neighbours\n",
      "0.5044418238993711\n",
      "euclidean K-Nearest Neighbours\n",
      "0.502122641509434\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5029088050314465\n",
      "canberra K-Nearest Neighbours\n",
      "0.4995283018867924\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5011399371069183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.49772012578616354\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt']\n",
      "['[img]url;[/img]\\n', 'smartart\\n', 'oic\\n', '/o\\n', 'floodtech\\n', '-2013.url\\n', '-563-4131\\n', 'tivoli\\n', '', 'barbara\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt 100\n",
      "['hoto\\n', '', 'neal\\n', 'ee\\n', 'lich\\n', '&#7883;ch\\n', '443\\n', 'sia\\n', '[author]\\n', 'mikayla\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt']\n",
      "['[img]url;[/img]\\n', 'smartart\\n', 'oic\\n', '/o\\n', 'floodtech\\n', '-2013.url\\n', '-563-4131\\n', 'tivoli\\n', '', 'barbara\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt 100\n",
      "['hoto\\n', '', 'neal\\n', 'ee\\n', 'lich\\n', '&#7883;ch\\n', '443\\n', 'sia\\n', '[author]\\n', 'mikayla\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt']\n",
      "['[img]url;[/img]\\n', 'smartart\\n', 'oic\\n', '/o\\n', 'floodtech\\n', '-2013.url\\n', '-563-4131\\n', 'tivoli\\n', '', 'barbara\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt 150\n",
      "['hoto\\n', '', 'neal\\n', 'ee\\n', 'lich\\n', '&#7883;ch\\n', '443\\n', 'sia\\n', '[author]\\n', 'mikayla\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt']\n",
      "['[img]url;[/img]\\n', 'smartart\\n', 'oic\\n', '/o\\n', 'floodtech\\n', '-2013.url\\n', '-563-4131\\n', 'tivoli\\n', '', 'barbara\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt 250\n",
      "['hoto\\n', '', 'neal\\n', 'ee\\n', 'lich\\n', '&#7883;ch\\n', '443\\n', 'sia\\n', '[author]\\n', 'mikayla\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt']\n",
      "['i\\n', 'u\\n', 'and\\n', '?\\n', 'it\\n', 'me\\n', 'date\\n', 'my\\n', 'in\\n', '???\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt 100\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt']\n",
      "['i\\n', 'u\\n', 'and\\n', '?\\n', 'it\\n', 'me\\n', 'date\\n', 'my\\n', 'in\\n', '???\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt 150\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt']\n",
      "['i\\n', 'u\\n', 'and\\n', '?\\n', 'it\\n', 'me\\n', 'date\\n', 'my\\n', 'in\\n', '???\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt 250\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013ProbD.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt 250\n",
      "length of features used = 200\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt 250\n",
      "length of features used = 300\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-833b3735ba6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mall_test_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(all_test_text)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len(X_test=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"len(y_test)=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-833b3735ba6b>\u001b[0m in \u001b[0;36mWordFeatures\u001b[0;34m(word_list, all_training_text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mWordFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mfvs_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# normalise by dividing each row by number of tokens for each author........\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-833b3735ba6b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mWordFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mfvs_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# normalise by dividing each row by number of tokens for each author........\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-833b3735ba6b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mWordFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mfvs_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_training_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# normalise by dividing each row by number of tokens for each author........\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt 250\n",
      "length of features used = 200\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt 250\n",
      "length of features used = 300\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt']\n",
      "['you\\n', 'a\\n', '.\\n', 'your\\n', 'with\\n', 'that\\n', 'to\\n', 'swinger\\n', 'will\\n', 'club\\n']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 13106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5\n",
      "cosine K-Nearest Neighbours\n",
      "0.5\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5\n",
      "canberra K-Nearest Neighbours\n",
      "0.5\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_tokens/PAN2013_Tokens/PAN2013_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2016/2016txtfiles/PAN2016_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        #reader = csv.reader(f, delimiter=\",\")\n",
    "        #next(reader) # skip header\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(word_list1[0:10])\n",
    "        print(txt_file, len(word_list1))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    " #       reader = csv.reader(f, delimiter=\",\")\n",
    "  #      next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.4972877358490566\n",
      "cosine K-Nearest Neighbours\n",
      "0.5043238993710691\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5036163522012579\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5038522012578617\n",
      "canberra K-Nearest Neighbours\n",
      "0.4995283018867924\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5006682389937107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.4970518867924528\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5018474842767295\n",
      "cosine K-Nearest Neighbours\n",
      "0.49422169811320754\n",
      "euclidean K-Nearest Neighbours\n",
      "0.49996069182389935\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.49897798742138366\n",
      "canberra K-Nearest Neighbours\n",
      "0.4979166666666667\n",
      "jaccard K-Nearest Neighbours\n",
      "0.49984276729559746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.4979952830188679\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5011399371069183\n",
      "cosine K-Nearest Neighbours\n",
      "0.5017295597484277\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5041666666666667\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5053459119496856\n",
      "canberra K-Nearest Neighbours\n",
      "0.5009827044025157\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5037735849056604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5005110062893082\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "selector = SelectKBest(score_func=chi2, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=chi2, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5050314465408805\n",
      "cosine K-Nearest Neighbours\n",
      "0.5032232704402516\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5040880503144655\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.5018867924528302\n",
      "canberra K-Nearest Neighbours\n",
      "0.5043632075471698\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5031446540880503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5006289308176101\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "selector = SelectKBest(score_func=chi2, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.49547955974842767\n",
      "cosine K-Nearest Neighbours\n",
      "0.5007468553459119\n",
      "euclidean K-Nearest Neighbours\n",
      "0.4991745283018868\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.4983490566037736\n",
      "canberra K-Nearest Neighbours\n",
      "0.49677672955974844\n",
      "jaccard K-Nearest Neighbours\n",
      "0.49897798742138366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5026729559748427\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5023977987421384\n",
      "cosine K-Nearest Neighbours\n",
      "0.503066037735849\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5031446540880503\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.49783805031446543\n",
      "canberra K-Nearest Neighbours\n",
      "0.5012185534591195\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5049135220125787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.49901729559748426\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5018081761006289\n",
      "cosine K-Nearest Neighbours\n",
      "0.5022012578616353\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5012971698113208\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.4992138364779874\n",
      "canberra K-Nearest Neighbours\n",
      "0.4959119496855346\n",
      "jaccard K-Nearest Neighbours\n",
      "0.498938679245283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5020440251572327\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5008254716981132\n",
      "cosine K-Nearest Neighbours\n",
      "0.5010220125786163\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5057783018867924\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.4995676100628931\n",
      "canberra K-Nearest Neighbours\n",
      "0.5032232704402516\n",
      "jaccard K-Nearest Neighbours\n",
      "0.5024764150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5002751572327044\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=f_classif, k=300)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5034984276729559\n",
      "cosine K-Nearest Neighbours\n",
      "0.4993317610062893\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5011792452830188\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.49669811320754714\n",
      "canberra K-Nearest Neighbours\n",
      "0.5011006289308176\n",
      "jaccard K-Nearest Neighbours\n",
      "0.4982311320754717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.501375786163522\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv 9721\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv 9145\n",
      "length of features used = 18866\n",
      "len(all_training_text) 13106\n",
      "len(X_train)= 13106 len(y_train)= 13106\n",
      "len(all_test_text) 25440\n",
      "len(X_test= 25440 len(y_test)= 25440\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "5\n",
      "manhattan K-Nearest Neighbours\n",
      "0.5012971698113208\n",
      "cosine K-Nearest Neighbours\n",
      "0.5011006289308176\n",
      "euclidean K-Nearest Neighbours\n",
      "0.5007861635220126\n",
      "braycurtis K-Nearest Neighbours\n",
      "0.49504716981132074\n",
      "canberra K-Nearest Neighbours\n",
      "0.5022798742138365\n",
      "jaccard K-Nearest Neighbours\n",
      "0.49406446540880505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yule K-Nearest Neighbours\n",
      "0.5007468553459119\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_chosen_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2013/output2013OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2013/2013.1_textfiles/PAN2013_tweet_male_ vocubulary_set.1.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        #reader = csv.reader(f, delimiter=\",\")\n",
    "#        #next(reader) # skip header\n",
    "#        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "#print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "#selector = SelectKBest(score_func=chi2, k=500)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "8. cosine K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"cosine K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "8. euclidean K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"euclidean K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import *\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"braycurtis K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "       \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"canberra K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "  \n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"jaccard K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"yule K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module TermSelection.py processing\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import math, codecs, string\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "print \"Read TermSelection.py\"\n",
    "\n",
    "\n",
    "def doAll(aFileM, aFileF, aYear=\"2013\"):\n",
    "   aFileName = \"output\" + aYear\n",
    "   \n",
    "   aL = doDifferenceSymetric(\"CHI\", aFileM,  aFileF, aFileName+\"CHI.txt\", 550, 0)\n",
    "   aL = doDifferenceSymetric(\"GSS\", aFileM,  aFileF, aFileName+\"GSS.txt\", 550, 0)\n",
    "   aL = doDifferenceSymetric(\"CC\", aFileM, aFileF, aFileName+\"CC.txt\", 550, 0);\n",
    "   aL = doDifferenceSymetric(\"GR\", aFileM, aFileF, aFileName+\"GR.txt\", 550, 0);\n",
    "   aL = doDifferenceSymetric(\"IG\", aFileM, aFileF, aFileName+\"IG.txt\", 550, 0);\n",
    "   \n",
    "   aL = doDifferenceAsymetric(\"RS\", aFileM, aFileF, aFileName+\"RS.\", 550, 0);\n",
    "   aL = doDifferenceAsymetric(\"PMI\", aFileM, aFileF, aFileName+\"PMI.\", 550, 0);\n",
    "   aL = doDifferenceAsymetric(\"WLLR\", aFileM, aFileF, aFileName+\"WLLR.\", 550, 0);\n",
    "   aL = doDifferenceAsymetric(\"RF\", aFileM,  aFileF, aFileName+\"RF.\", 550, 0)\n",
    "   aL = doDifferenceAsymetric(\"OR\", aFileM,  aFileF, aFileName+\"OR.\", 550, 0)\n",
    "   return(True)\n",
    "\n",
    "\n",
    "#\n",
    "# Difference between two dictionaries\n",
    "#\n",
    "\n",
    "def doDifferenceProbD(aFileNameBase, aFileNameComp, outputFile=\"output.txt\", aNumber=100, minDF=5):\n",
    "   (aDictM, aNm) = createDictionary(aFileNameBase)\n",
    "   print len(aDictM),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictM, minDF)\n",
    "      print len(aDictM)\n",
    "   else:\n",
    "      print\n",
    "   aDictM = normalizeSimpleDict(aDictM)\n",
    "   (aDictF, aNf) = createDictionary(aFileNameComp)\n",
    "   print len(aDictF),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictF, minDF)\n",
    "      print len(aDictF)\n",
    "   else:\n",
    "      print\n",
    "   aDictF = normalizeSimpleDict(aDictF)\n",
    "   \n",
    "   aD     = diffDictionary(aDictM, aDictF)\n",
    "   if (aNumber > 0):\n",
    "      aT1 = extractTopSortedDict(aD, aNumber, True)\n",
    "      dumpDictionary(aT1, outputFile+\"1M.txt\")\n",
    "   else:\n",
    "      aT1 = aD\n",
    "      dumpDictionaryAll(aD, outputFile+\"1M.txt\", True)\n",
    "      \n",
    "   aD = diffDictionary(aDictF, aDictM)\n",
    "   if (aNumber > 0):\n",
    "      aT2 = extractTopSortedDict(aD, aNumber, True)\n",
    "      dumpDictionary(aT2, outputFile+\"2F.txt\")\n",
    "   else:\n",
    "      aT2 = aD\n",
    "      dumpDictionaryAll(aD, outputFile+\"2F.txt\", True)      \n",
    "   return(aT1, aT2)\n",
    "\n",
    "#\n",
    "# Difference with a given method\n",
    "#\n",
    "def doDifferenceAsymetric(aMethod, aFileNameBase, aFileNameComp, outputFile=\"outputOR.txt\", aNumber=100, minDF=5):\n",
    "   (aDictM, aNm) = createDFDictionary(aFileNameBase)\n",
    "   print len(aDictM),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictM, minDF)\n",
    "      print len(aDictM)\n",
    "   else:\n",
    "      print\n",
    "   (aDictF, aNf) = createDFDictionary(aFileNameComp)\n",
    "   print len(aDictF),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictF, minDF)\n",
    "      print len(aDictF)\n",
    "   else:\n",
    "      print\n",
    "   if (aMethod == \"OR\"):\n",
    "      aD = computeOddRatio(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"PMI\"):\n",
    "      aD = computePMI(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"RF\"):\n",
    "      aD = computeRF(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"RS\"):\n",
    "      aD = computeRS(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"WLLR\"):\n",
    "      aD = computeWLLR(aDictM, aDictF, aNm, aNf)\n",
    "      \n",
    "   aT1 = extractTopSortedDict(aD, aNumber)\n",
    "   dumpDictionary(aT1, outputFile+\"1M.txt\")\n",
    "   if (aMethod == \"OR\"):\n",
    "      aD = computeOddRatio(aDictF, aDictM, aNf, aNm)\n",
    "   if (aMethod == \"PMI\"):\n",
    "      aD = computePMI(aDictF, aDictM, aNf, aNm)\n",
    "   if (aMethod == \"RF\"):\n",
    "      aD = computeRF(aDictF, aDictM, aNf, aNm)\n",
    "   if (aMethod == \"RS\"):\n",
    "      aD = computeRS(aDictF, aDictM, aNf, aNm)\n",
    "   if (aMethod == \"WLLR\"):\n",
    "      aD = computeWLLR(aDictF, aDictM, aNf, aNm)\n",
    "      \n",
    "   aT2 = extractTopSortedDict(aD, aNumber)\n",
    "   dumpDictionary(aT2, outputFile+\"2F.txt\")      \n",
    "   return(aT1, aT2)\n",
    "\n",
    "def doDifferenceSymetric(aMethod, aFileNameBase, aFileNameComp, outputFile=\"output.txt\", aNumber=100, minDF=5):\n",
    "   (aDictM, aNm) = createDFDictionary(aFileNameBase)\n",
    "   print len(aDictM),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictM, minDF)\n",
    "      print len(aDictM)\n",
    "   else:\n",
    "      print\n",
    "   (aDictF, aNf) = createDFDictionary(aFileNameComp)\n",
    "   print len(aDictF),\n",
    "   if (minDF > 0):\n",
    "      filterDictTF(aDictF, minDF)\n",
    "      print len(aDictF)\n",
    "   else:\n",
    "      print\n",
    "   if (aMethod == \"GSS\"):\n",
    "      aD = computeGSS(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"CHI\"):\n",
    "      aD = computeCHI(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"GR\"):\n",
    "      aD = computeGR(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"IG\"):\n",
    "      aD = computeIG(aDictM, aDictF, aNm, aNf)\n",
    "   if (aMethod == \"CC\"):\n",
    "      aD = computeCC(aDictM, aDictF, aNm, aNf)\n",
    "   aDictF = aDictM = 0\n",
    "   aB = extractBottomSortedDict(aD, aNumber)\n",
    "   aT = extractTopSortedDict(aD, aNumber)\n",
    "   dumpDictionary(aT, outputFile)\n",
    "   dumpDictionary(aB, outputFile)\n",
    "   return(aT, aB)\n",
    "\n",
    "#\n",
    "# Term Selection procedure \n",
    "#\n",
    "#\n",
    "# GSS  return a Dict (word:gss) from a corpusDFDict, subcorpusDFDict, \n",
    "def computeRF(aDict1DF, aDict2DF, subCorpusSize1, subCorpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   acValue = float(subCorpusSize1)\n",
    "   bdValue = float(subCorpusSize2)\n",
    "   nValue = acValue + bdValue\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0\n",
    "      maxValue = max(bValue,1.0)\n",
    "#     print aKey, aValue, bValue, maxValue, math.log (2 + (aValue/maxValue),2)\n",
    "      aDictValue [aKey] =  math.log (2 + (aValue/maxValue),2)\n",
    "   return(aDictValue)\n",
    "\n",
    "def computeGSS(aDict1DF, aDict2DF, subCorpusSize1, subCorpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   acValue = float(subCorpusSize1)\n",
    "   bdValue = float(subCorpusSize2)\n",
    "   nValue = acValue + bdValue\n",
    "   n2Value = nValue * nValue\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0001\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0001\n",
    "      cValue = acValue - aValue\n",
    "      dValue = bdValue - bValue\n",
    "#     print aKey, aValue, bValue, cValue, dValue, (((aValue*dValue) - (cValue*bValue)) / n2Value)\n",
    "      aDictValue [aKey] = ((aValue*dValue) - (cValue*bValue)) / n2Value\n",
    "   return(aDictValue)\n",
    "\n",
    "def computeOddRatio(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   acValue = float(corpusSize1)\n",
    "   bdValue = float(corpusSize2)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0001\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0001\n",
    "      cValue = acValue - aValue\n",
    "      dValue = bdValue - bValue\n",
    "      prob1 = aValue / acValue\n",
    "      prob2 = bValue / bdValue\n",
    "      if ((bValue <= 0) or (abs(aValue - acValue) <= 0.00000001)) :\n",
    "         oddValue = 0.0\n",
    "      else:\n",
    "         oddValue = (prob1*(1-prob2)) / ((1-prob1)*prob2)\n",
    "#      print aKey, aValue, bValue, cValue, dValue, oddValue\n",
    "      aDictValue[aKey] = oddValue\n",
    "   return(aDictValue)\n",
    "\n",
    "def computeOddRatioOLD(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0001\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0001\n",
    "      cValue = float(corpusSize1) - aValue\n",
    "      dValue = float(corpusSize2) - bValue\n",
    "      if ((cValue*bValue) <= 0):\n",
    "         oddValue = 0.0\n",
    "      else:\n",
    "         oddValue = (aValue*dValue) / (cValue*bValue)\n",
    "#      print aKey, aValue, bValue, cValue, dValue, oddValue\n",
    "      aDictValue[aKey] = oddValue\n",
    "   return(aDictValue)\n",
    "\n",
    "# CHI Chi-square value  return a Dict (word:chi) from a corpusDFDict, subcorpusDFDict, \n",
    "def computeCHI(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue = float(corpusSize1) + corpusSize2\n",
    "   acValue = float(corpusSize1)\n",
    "   bdValue = float(corpusSize2)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0\n",
    "      abValue = aValue + bValue\n",
    "      cValue = acValue - aValue\n",
    "      dValue = bdValue - bValue\n",
    "      num = nValue * pow( (float(aValue) * dValue)  - (cValue * bValue), 2)\n",
    "      denum = float(abValue) * acValue * (bValue+dValue) * (cValue + dValue)\n",
    "      if ((num <= 0) | (denum <= 0)):\n",
    "         chiValue = 0.0\n",
    "      else:\n",
    "         chiValue =   num / denum\n",
    "      aDictValue [aKey] = chiValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, chiValue\n",
    "   return(aDictValue)\n",
    "\n",
    "# PMI Point Mutual Information   return a Dict (word:pmi) from a corpusDFDict, subcorpusDFDict, \n",
    "def computePMI(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue = float(corpusSize1 + corpusSize2)\n",
    "   acValue = float(corpusSize1)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0001\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0001\n",
    "      abValue = aValue + bValue\n",
    "      if (((abValue * acValue) <= 0) | (aValue <= 0)):\n",
    "         pmiValue = 0.0\n",
    "      else:\n",
    "         pmiValue = math.log (  (float (aValue*nValue) / (abValue * acValue)), 2)\n",
    "      aDictValue[aKey] = pmiValue\n",
    "#      print aKey, aValue, bValue, pmiValue\n",
    "   return(aDictValue)\n",
    "\n",
    "\n",
    "# Gain Ratio value  return a Dict (word:chi) from a corpusDFDict, subcorpusDFDict, \n",
    "def computeGR(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue  = float(corpusSize1) + corpusSize2\n",
    "   acValue = float(corpusSize1)\n",
    "   bdValue = float(corpusSize2)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0\n",
    "      abValue = aValue + bValue\n",
    "      cValue = acValue - aValue\n",
    "      dValue = bdValue - bValue\n",
    "      part1 = ((aValue) * nValue) / (abValue * acValue)\n",
    "      part2 = (cValue * nValue) / (acValue * (cValue + dValue))\n",
    "      if (aValue > 0):\n",
    "         grValue =  (aValue / nValue) * math.log(part1,2)\n",
    "      else:\n",
    "         grValue = 0.0\n",
    "      if (cValue > 0):\n",
    "         grValue = grValue +  (cValue/nValue) * math.log(part2,2)\n",
    "      aDictValue [aKey] = grValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, grValue\n",
    "   return(aDictValue)\n",
    "\n",
    "\n",
    "# Information gain value  return a Dict (word:chi) from a corpusDFDict, subcorpusDFDict, \n",
    "def computeIG(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue  = float(corpusSize1) + corpusSize2\n",
    "   acValue = float(corpusSize1)\n",
    "   bdValue = float(corpusSize2)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0\n",
    "      abValue = aValue + bValue\n",
    "      cValue  = acValue - aValue\n",
    "      dValue  = bdValue - bValue\n",
    "\n",
    "      if (aValue > 0):\n",
    "         part1 = aValue / nValue\n",
    "         part1 = part1 * math.log ((aValue * nValue) / (abValue * acValue))\n",
    "      else:\n",
    "         part1 = 0\n",
    "      if (bValue > 0):        \n",
    "         part2 = bValue / nValue\n",
    "         part2 = part2 * math.log ((bValue * nValue) / (abValue * bdValue))\n",
    "      else:\n",
    "         part2 = 0\n",
    "      if (cValue > 0):\n",
    "         part3 = cValue / nValue\n",
    "         part3 = part3 * math.log ((cValue * nValue) / (acValue * (cValue+dValue)))\n",
    "      else:\n",
    "         part3 = 0\n",
    "      if (dValue > 0):\n",
    "         part4 = dValue / nValue\n",
    "         part4 = part4 * math.log ((dValue * nValue) / ((bdValue) * (cValue+dValue)))\n",
    "      else:\n",
    "         part4 = 0\n",
    "      igValue = part1 + part2 + part3 + part4         \n",
    "      aDictValue [aKey] = igValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, igValue\n",
    "   return(aDictValue)\n",
    "\n",
    "\n",
    "def computeCC(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue = float(corpusSize1) + corpusSize2\n",
    "   acValue = float(corpusSize1)\n",
    "   bdValue = float(corpusSize2)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = float(aDict2DF[aKey])\n",
    "      else:\n",
    "         bValue = 0.0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = float(aDict1DF[aKey])\n",
    "      else:\n",
    "         aValue = 0.0\n",
    "      abValue = aValue + bValue\n",
    "      cValue = acValue - aValue\n",
    "      dValue = bdValue - bValue\n",
    "      num = math.sqrt(nValue) * ((aValue * dValue)  - (cValue * bValue))\n",
    "      denum = math.sqrt(abValue * acValue * bdValue * (cValue + dValue))\n",
    "      if ((num <= 0) | (denum <= 0)):\n",
    "         ccValue = 0.0\n",
    "      else:\n",
    "         ccValue =   num / denum\n",
    "      aDictValue[aKey] = ccValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, ccValue\n",
    "   return(aDictValue)\n",
    "\n",
    "# Relevance score  return a Dict \n",
    "def computeRS(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue = float(corpusSize1 + corpusSize2)\n",
    "   acValue = float(corpusSize1)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = aDict2DF[aKey]\n",
    "      else:\n",
    "         bValue = 0\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = aDict1DF[aKey]\n",
    "      else:\n",
    "         aValue = 0\n",
    "      cValue = acValue - aValue\n",
    "      dValue = nValue - acValue - bValue\n",
    "      num = (float(aValue) / acValue) + 1.0\n",
    "      denum = (float(bValue) / (bValue + dValue)) + 1.0\n",
    "      rsValue = math.log ( (num / denum), 2) \n",
    "      aDictValue [aKey] = rsValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, rsValue\n",
    "   return(aDictValue)\n",
    "\n",
    "\n",
    "# WLLR Eighted Log LikelihoodRatio Measure  return a Dict (word:wllr) from a corpusDFDict, subcorpusDFDict, \n",
    "def computeWLLR(aDict1DF, aDict2DF, corpusSize1, corpusSize2, aSet=[]):\n",
    "   aDictValue = {}\n",
    "   nValue = float(corpusSize1 + corpusSize2)\n",
    "   acValue = float(corpusSize1)\n",
    "   if (len(aSet) == 0):\n",
    "      aKeySet = union(aDict1DF.keys(), aDict2DF.keys())\n",
    "   else:\n",
    "      aKeySet = aSet\n",
    "   for aKey in aKeySet:\n",
    "      if (aDict2DF.has_key(aKey)):\n",
    "         bValue = aDict2DF[aKey]\n",
    "      else:\n",
    "         bValue = 0.0001\n",
    "      if (aDict1DF.has_key(aKey)):\n",
    "         aValue = aDict1DF[aKey]\n",
    "      else:\n",
    "         aValue = 0\n",
    "      cValue = acValue - aValue\n",
    "      dValue = nValue - acValue - bValue\n",
    "      num = float(aValue) / acValue\n",
    "      denum = float(bValue) / (bValue + dValue)\n",
    "      if (num <= 0) :\n",
    "         wllrValue = -1.0\n",
    "      elif (denum <= 0):\n",
    "         wllrValue = 1.0 + (float(aValue) / nValue) \n",
    "      else:\n",
    "         wllrValue =  num *  math.log ( (num / denum), 2) \n",
    "      aDictValue[aKey] = wllrValue\n",
    "#      print aKey, aValue, bValue, cValue, dValue, wllrValue\n",
    "   return(aDictValue)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Read files \n",
    "#\n",
    "\n",
    "def createDictionary(aFileName):\n",
    "   myFile = codecs.open(aFileName, encoding='utf-8', mode=\"r\")\n",
    "   aDict = {}\n",
    "   aNumber = 0\n",
    "   aLine = myFile.readline()\n",
    "   myLine = string.split(aLine)\n",
    "   myLine = myLine[3:]\n",
    "   while aLine:\n",
    "      aNumber = aNumber + 1\n",
    "      for aWord in myLine:\n",
    "         if (aDict.has_key(aWord)):\n",
    "            aValue = aDict[aWord]\n",
    "         else:\n",
    "            aValue = 0\n",
    "         aDict[aWord] = aValue + 1\n",
    "      aLine = myFile.readline()\n",
    "      myLine = string.split(aLine)\n",
    "      myLine = myLine[3:]\n",
    "   myFile.close()\n",
    "   return(aDict, aNumber)\n",
    "\n",
    "def createDFDictionary(aFileName, startPos=3):\n",
    "   myFile = codecs.open(aFileName, encoding='utf-8', mode=\"r\")\n",
    "   aDict = {}\n",
    "   aNumber = 0\n",
    "   aLine = myFile.readline()\n",
    "   myLine = string.split(aLine)\n",
    "   myLine = myLine[startPos:]\n",
    "   while aLine:\n",
    "      aDict1 = {}\n",
    "      aNumber = aNumber + 1\n",
    "      for aWord in myLine:\n",
    "         aDict1[aWord] = 1\n",
    "      for aWord in aDict1.keys():\n",
    "         if (aDict.has_key(aWord)):\n",
    "            aValue = aDict[aWord]\n",
    "         else:\n",
    "            aValue = 0\n",
    "         aDict[aWord] = aValue + 1\n",
    "      aLine = myFile.readline()\n",
    "      myLine = string.split(aLine)\n",
    "      myLine = myLine[startPos:]\n",
    "   myFile.close()\n",
    "   return(aDict, aNumber)\n",
    "\n",
    "\n",
    "def readDictionary(aFileName):\n",
    "   myFile = codecs.open(aFileName, encoding='utf-8', mode=\"r\")\n",
    "   aDict = {}\n",
    "   aLine = myFile.readline()\n",
    "   aLine = aLine.replace(u\"\\n\", \"\")\n",
    "   aPos = aLine.find(u':')\n",
    "   aKey = aLine[:aPos]\n",
    "   aValue = aLine[aPos+1:]\n",
    "   while aLine:\n",
    "      aDict[aValue] = float(aKey)\n",
    "      aLine = myFile.readline()\n",
    "      aLine = aLine.replace(u\"\\n\", \"\")\n",
    "      aPos = aLine.find(u':')\n",
    "      aKey = aLine[:aPos]\n",
    "      aValue = aLine[aPos+1:]\n",
    "   myFile.close()\n",
    "   return(aDict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Union and intersection\n",
    "#\n",
    "\n",
    "def intersection (*args):\n",
    "   res = []\n",
    "   for x in args[0]:\n",
    "      for other in args[1:]:\n",
    "         if x not in other: break\n",
    "      else:\n",
    "        res.append(x)\n",
    "   return res\n",
    "\n",
    "def union (*args):\n",
    "   res = []\n",
    "   for seq in args:\n",
    "      for x in seq:\n",
    "         if x not in res:\n",
    "            res.append(x)\n",
    "   return res\n",
    "\n",
    "#\n",
    "# Manipulating a dictionary\n",
    "#\n",
    "\n",
    "# remove entries for which the tf value <= minTF\n",
    "def filterDictTF(aDict, minTF):\n",
    "   for aKey in aDict.keys():\n",
    "      if (aDict[aKey] <= minTF):\n",
    "         del aDict[aKey]\n",
    "\n",
    "# generate a new dict by normalizing the values (tf -> ntf)\n",
    "# use to create a dict with relative frequencies\n",
    "def reverseDict(aDict):\n",
    "   aNewDict = {}\n",
    "   for aKey in aDict.keys():\n",
    "      aValue = aDict[aKey]\n",
    "      if (aNewDict.has_key(aValue)):\n",
    "         aList = aNewDict[aValue]\n",
    "      else:\n",
    "         aList = []\n",
    "      aList.append(aKey)\n",
    "      aNewDict[aValue] = aList\n",
    "   return(aNewDict)\n",
    "\n",
    "def dumpDictionary(aDictA, aFileName=\"\"):\n",
    "   aDict = reverseDict(aDictA)\n",
    "   if (len(aFileName) > 0):\n",
    "      myFile = codecs.open(aFileName, encoding='utf-8', mode=\"a\")\n",
    "   for aKey in sorted(aDict.keys(),None, None, True):\n",
    "      aListKeyA = aDict[aKey]\n",
    "      for aKeyA in aListKeyA:\n",
    "         aValueA = aDictA[aKeyA]\n",
    "         if (len(aFileName) > 0):\n",
    "            myFile.write(str(aValueA)+\":\"+aKeyA+\"\\n\")\n",
    "         else:\n",
    "            print str(aValueA)+\"  :  \"+aKeyA\n",
    "   if (len(aFileName) > 0):\n",
    "      myFile.close()\n",
    "   return(aDict)\n",
    "\n",
    "def extractTopSortedDict(aDict, aLimit, positiveOnly=False):\n",
    "   if not(positiveOnly):\n",
    "      return(extractTopSortedDictPrivate(aDict, aLimit))\n",
    "   aNewDict = reverseDict(aDict)\n",
    "   printed = 0\n",
    "   aTopDict = {}\n",
    "   for aKey in sorted(aNewDict.keys(),None,None,True):\n",
    "      if (printed < aLimit):\n",
    "         for aWord in aNewDict[aKey]:\n",
    "            if (aKey < 0):\n",
    "               break\n",
    "            aTopDict[aWord] = aKey\n",
    "            printed = printed + 1\n",
    "            if (printed > aLimit):\n",
    "               break\n",
    "      else:\n",
    "         break\n",
    "   return(aTopDict)\n",
    "\n",
    "def extractTopSortedDictPrivate(aDict, aLimit):\n",
    "   aNewDict = reverseDict(aDict)\n",
    "   printed = 0\n",
    "   aTopDict = {}\n",
    "   for aKey in sorted(aNewDict.keys(),None,None,True):\n",
    "      if (printed < aLimit):\n",
    "         for aWord in aNewDict[aKey]:\n",
    "            aTopDict[aWord] = aKey\n",
    "            printed = printed + 1\n",
    "            if (printed > aLimit):\n",
    "               break\n",
    "      else:\n",
    "         break\n",
    "   return(aTopDict)\n",
    "\n",
    "\n",
    "def extractBottomSortedDict(aDict, aLimit):\n",
    "   aNewDict = reverseDict(aDict)\n",
    "   printed = 0\n",
    "   aBottomDict = {}\n",
    "   for aKey in sorted(aNewDict.keys()):\n",
    "      if (printed < aLimit):\n",
    "         for aWord in aNewDict[aKey]:\n",
    "            aBottomDict[aWord] = aKey\n",
    "            printed = printed + 1\n",
    "            if (printed > aLimit):\n",
    "               break\n",
    "      else:\n",
    "         break\n",
    "   return(aBottomDict)\n",
    "\n",
    "# print only when not imported\n",
    "if __name__ == '__main__':\n",
    "   print \"The module TermSelection.py was uploaded\\n\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
