{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv 1028\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv 1092\n",
      "length of features used = 2120\n",
      "len(all_training_text) 152\n",
      "mean length =  1252.296052631579\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.8028169014084507\n",
      "0.7676056338028169\n",
      "0.8028169014084507\n",
      "0.795774647887324\n",
      "0.6267605633802817\n",
      "0.8380281690140845\n",
      "0.7746478873239436\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7464788732394366\n",
      "0.7605633802816901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816901408450704\n",
      "0.7746478873239436\n",
      "0.6338028169014085\n",
      "0.823943661971831\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7183098591549296\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.6197183098591549\n",
      "0.7746478873239436\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "0.755868544600939\n",
      "0.7535211267605634\n",
      "0.7699530516431925\n",
      "0.7816901408450704\n",
      "0.6267605633802817\n",
      "0.812206572769953\n",
      "0.7511737089201878\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,8,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv 1028\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv 1092\n",
      "length of features used = 2120\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.8028169014084507\n",
      "0.7676056338028169\n",
      "0.8028169014084507\n",
      "0.795774647887324\n",
      "0.6267605633802817\n",
      "0.8380281690140845\n",
      "0.7746478873239436\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7464788732394366\n",
      "0.7605633802816901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816901408450704\n",
      "0.7746478873239436\n",
      "0.6338028169014085\n",
      "0.823943661971831\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7183098591549296\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.6197183098591549\n",
      "0.7746478873239436\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "9\n",
      "0.676056338028169\n",
      "0.7253521126760564\n",
      "0.704225352112676\n",
      "0.7605633802816901\n",
      "0.6338028169014085\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6619718309859155\n",
      "0.6901408450704225\n",
      "0.7183098591549296\n",
      "0.7112676056338029\n",
      "0.6338028169014085\n",
      "0.7253521126760564\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7112676056338029\n",
      "0.6830985915492958\n",
      "0.6971830985915493\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "0.7605633802816901\n",
      "0.676056338028169\n",
      "************************************ ****************************\n",
      "15\n",
      "0.6971830985915493\n",
      "0.6690140845070423\n",
      "0.6690140845070423\n",
      "0.6690140845070423\n",
      "0.6901408450704225\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "17\n",
      "0.6901408450704225\n",
      "0.647887323943662\n",
      "0.6830985915492958\n",
      "0.6901408450704225\n",
      "0.6901408450704225\n",
      "0.7253521126760564\n",
      "0.676056338028169\n",
      "************************************ ****************************\n",
      "19\n",
      "0.6971830985915493\n",
      "0.6549295774647887\n",
      "0.6971830985915493\n",
      "0.6901408450704225\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.676056338028169\n",
      "************************************ ****************************\n",
      "0.7112676056338029\n",
      "0.7034428794992176\n",
      "0.7198748043818466\n",
      "0.7269170579029733\n",
      "0.651017214397496\n",
      "0.7574334898278561\n",
      "0.7097026604068858\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.49295774647887325\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.5915492957746479\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5211267605633803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5140845070422535\n",
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.4984350547730829\n",
      "0.5453834115805947\n",
      "0.543035993740219\n",
      "0.5312989045383412\n",
      "0.49921752738654146\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.49295774647887325\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.5915492957746479\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.4984350547730829\n",
      "0.5453834115805947\n",
      "0.543035993740219\n",
      "0.5312989045383412\n",
      "0.49921752738654146\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5492957746478874\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.5845070422535211\n",
      "0.49295774647887325\n",
      "0.5845070422535211\n",
      "0.5774647887323944\n",
      "0.5633802816901409\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5070422535211268\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.4859154929577465\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5140845070422535\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5453834115805947\n",
      "0.4984350547730829\n",
      "0.5406885758998435\n",
      "0.5383411580594679\n",
      "0.525039123630673\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015OR.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.49295774647887325\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.5915492957746479\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5140845070422535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.4984350547730829\n",
      "0.5453834115805947\n",
      "0.543035993740219\n",
      "0.5312989045383412\n",
      "0.49921752738654146\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.49295774647887325\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.5915492957746479\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.49295774647887325\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.4984350547730829\n",
      "0.5453834115805947\n",
      "0.543035993740219\n",
      "0.5312989045383412\n",
      "0.49921752738654146\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5492957746478874\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.5845070422535211\n",
      "0.49295774647887325\n",
      "0.5845070422535211\n",
      "0.5774647887323944\n",
      "0.5633802816901409\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5211267605633803\n",
      "0.49295774647887325\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5070422535211268\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.4859154929577465\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5563380281690141\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5140845070422535\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5563380281690141\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5633802816901409\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5453834115805947\n",
      "0.4984350547730829\n",
      "0.5406885758998435\n",
      "0.5383411580594679\n",
      "0.525039123630673\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015PMI.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt 100\n",
      "length of features used = 200\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.45774647887323944\n",
      "0.5\n",
      "0.45774647887323944\n",
      "0.4507042253521127\n",
      "0.45774647887323944\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5352112676056338\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4859154929577465\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.49295774647887325\n",
      "0.49295774647887325\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5\n",
      "0.5\n",
      "0.5046948356807511\n",
      "0.49608763693270735\n",
      "0.4953051643192488\n",
      "0.5015649452269171\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt 150\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt 150\n",
      "length of features used = 300\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.45774647887323944\n",
      "0.5\n",
      "0.45774647887323944\n",
      "0.4507042253521127\n",
      "0.45774647887323944\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.528169014084507\n",
      "0.528169014084507\n",
      "0.5352112676056338\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4859154929577465\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.49295774647887325\n",
      "0.49295774647887325\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5\n",
      "0.5\n",
      "0.5046948356807511\n",
      "0.49608763693270735\n",
      "0.4953051643192488\n",
      "0.5015649452269171\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:150]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt 250\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt 250\n",
      "length of features used = 500\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.47183098591549294\n",
      "0.5140845070422535\n",
      "0.47183098591549294\n",
      "0.4788732394366197\n",
      "0.4859154929577465\n",
      "0.5211267605633803\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.5070422535211268\n",
      "0.49295774647887325\n",
      "0.5140845070422535\n",
      "0.5140845070422535\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.49295774647887325\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4647887323943662\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5211267605633803\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.49295774647887325\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.45774647887323944\n",
      "0.5\n",
      "0.4788732394366197\n",
      "0.4647887323943662\n",
      "0.4647887323943662\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.49295774647887325\n",
      "0.5\n",
      "0.5023474178403755\n",
      "0.49295774647887325\n",
      "0.48513302034428796\n",
      "0.5031298904538342\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list1 =  ([(r.split(\":\"))[1] for r in f])[0:250]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.47183098591549294\n",
      "0.5070422535211268\n",
      "0.47183098591549294\n",
      "0.4788732394366197\n",
      "0.4859154929577465\n",
      "0.5140845070422535\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.4859154929577465\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.47183098591549294\n",
      "0.5\n",
      "0.49295774647887325\n",
      "0.4788732394366197\n",
      "0.4647887323943662\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.528169014084507\n",
      "0.5\n",
      "0.5211267605633803\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5070422535211268\n",
      "0.47183098591549294\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.4647887323943662\n",
      "0.5\n",
      "0.4788732394366197\n",
      "0.47183098591549294\n",
      "0.4647887323943662\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5352112676056338\n",
      "0.5070422535211268\n",
      "0.4788732394366197\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.49608763693270735\n",
      "0.49921752738654146\n",
      "0.5054773082942097\n",
      "0.49608763693270735\n",
      "0.4843505477308294\n",
      "0.5023474178403756\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt 300\n",
      "length of features used = 300\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5070422535211268\n",
      "0.38028169014084506\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.528169014084507\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5422535211267606\n",
      "0.4859154929577465\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.44366197183098594\n",
      "0.4859154929577465\n",
      "0.47183098591549294\n",
      "0.5492957746478874\n",
      "0.5070422535211268\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.47183098591549294\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5774647887323944\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.4788732394366197\n",
      "0.5211267605633803\n",
      "0.5070422535211268\n",
      "0.5422535211267606\n",
      "0.49295774647887325\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.4859154929577465\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5492957746478874\n",
      "0.5070422535211268\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5352112676056338\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5140845070422535\n",
      "0.528169014084507\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.49374021909233173\n",
      "0.48826291079812206\n",
      "0.5062597809076682\n",
      "0.5312989045383412\n",
      "0.5054773082942097\n",
      "0.5289514866979655\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt 500\n",
      "length of features used = 500\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5070422535211268\n",
      "0.38028169014084506\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.528169014084507\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5422535211267606\n",
      "0.4859154929577465\n",
      "0.5070422535211268\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.44366197183098594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4859154929577465\n",
      "0.47183098591549294\n",
      "0.5492957746478874\n",
      "0.5070422535211268\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.47183098591549294\n",
      "0.5070422535211268\n",
      "0.5070422535211268\n",
      "0.5774647887323944\n",
      "0.5\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.4788732394366197\n",
      "0.5211267605633803\n",
      "0.5070422535211268\n",
      "0.5422535211267606\n",
      "0.49295774647887325\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.4859154929577465\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5492957746478874\n",
      "0.5070422535211268\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.49295774647887325\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.5140845070422535\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5352112676056338\n",
      "0.47183098591549294\n",
      "0.528169014084507\n",
      "0.5140845070422535\n",
      "0.528169014084507\n",
      "0.5352112676056338\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.49374021909233173\n",
      "0.48826291079812206\n",
      "0.5062597809076682\n",
      "0.5312989045383412\n",
      "0.5054773082942097\n",
      "0.5289514866979655\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015CHI.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt 200\n",
      "length of features used = 200\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt 300\n",
      "length of features used = 300\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5774647887323944\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.5\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.6056338028169014\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.49295774647887325\n",
      "0.49295774647887325\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5140845070422535\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5211267605633803\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5704225352112676\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.5\n",
      "0.5500782472613459\n",
      "0.5461658841940532\n",
      "0.5461658841940532\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:300]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt 500\n",
      "length of features used = 500\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5774647887323944\n",
      "0.5774647887323944\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6126760563380281\n",
      "0.5\n",
      "0.6126760563380281\n",
      "0.6056338028169014\n",
      "0.6056338028169014\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "7\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5492957746478874\n",
      "0.5352112676056338\n",
      "0.5352112676056338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "9\n",
      "0.5070422535211268\n",
      "0.5\n",
      "0.5070422535211268\n",
      "0.49295774647887325\n",
      "0.49295774647887325\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "11\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "13\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5704225352112676\n",
      "0.5492957746478874\n",
      "0.5492957746478874\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "15\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5140845070422535\n",
      "0.5140845070422535\n",
      "0.5140845070422535\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "17\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5211267605633803\n",
      "0.5211267605633803\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "19\n",
      "0.5422535211267606\n",
      "0.5\n",
      "0.5422535211267606\n",
      "0.5704225352112676\n",
      "0.5704225352112676\n",
      "0.5\n",
      "0.5\n",
      "************************************ ****************************\n",
      "0.5500782472613459\n",
      "0.5\n",
      "0.5500782472613459\n",
      "0.5461658841940532\n",
      "0.5461658841940532\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:500]\n",
    "        print(txt_file, len(word_list))\n",
    "        #word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=1000)\")\n",
    "##selector = SelectKBest(score_func=chi2, k=1000)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv 1028\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv 1092\n",
      "length of features used = 2120\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "[(0.18490316254752193, 'register'), (0.1563234597244998, 'love'), (0.134860113491315, 'hit'), (0.12852280063692345, 'happy'), (0.12247095774297523, 'publish'), (0.11267198478610752, 'v'), (0.11164728830480564, 'wish'), (0.11063426019130085, 'smile'), (0.11026734178548936, '&'), (0.11020601143197628, 'study'), (0.10929751994405867, 'draw'), (0.10770264856677136, 'nick'), (0.10679808894510634, 'fav'), (0.10456088901053806, 'twitter'), (0.10434650178957172, 'college'), (0.1036498300402684, 'discovery'), (0.10119396815619464, 'teacher'), (0.10092810914420558, 'merry'), (0.10038812464040503, '4'), (0.09882574330828686, 'fat'), (0.09843415755485907, 'accidentally'), (0.09728384319343797, 'afternoon'), (0.09592455093341035, 'compare'), (0.0946085241239627, 'catch'), (0.0944190727299834, 'hilarious'), (0.09169227649989753, 'original'), (0.09166437397586513, 'education'), (0.09161652343459359, '#tbt'), (0.09143899628338215, 'tire'), (0.09118882613189627, 'tomorrow'), (0.09021543913877261, 'until'), (0.08963878625948785, 'by'), (0.08846168370472718, '????????????????'), (0.08710703170083689, 'bother'), (0.086552316807887, 'girl'), (0.0864975744011709, 'bank'), (0.08513170708135598, 'glad'), (0.08495868930965411, '15'), (0.08494602903775683, '...'), (0.08476293670484591, 'value'), (0.08468242135246506, '4th'), (0.08458393613884829, 'scientist'), (0.08445150009793712, 'theme'), (0.08417904132605858, 'nature'), (0.08387683987254158, 'tell'), (0.08215852894370457, 'ice'), (0.08150284325512391, 'bite'), (0.08111928503487809, 'most'), (0.08080211921950498, 'sentiment'), (0.08010405929055042, ';)'), (0.07972711532808008, 'wauw'), (0.07910012632551044, 'episode'), (0.07891051599174403, 'knowledge'), (0.07830554713019611, 'crash'), (0.07771275541898359, 'skin'), (0.07769630354010904, 'personal'), (0.07750661856907715, '!!!'), (0.0771454316608704, '.s.'), (0.07650692543025861, '.......'), (0.07639688822758672, 'literally'), (0.07629206353776508, 'class'), (0.07620353729792329, 'flash'), (0.07592552315090728, 'fell'), (0.07590084291628152, 'scheme'), (0.0754343334444556, 'my'), (0.07539198390418389, 'mexico'), (0.07537039748452035, 'potential'), (0.07495439353233202, 'swift'), (0.07478772905645692, 'ah'), (0.07458244274919701, '..'), (0.07449525836924953, 'showcase'), (0.07447255629062632, 'interactive'), (0.07423730177905608, '\"'), (0.07401814958621955, 'lip'), (0.07383335630932097, 'full'), (0.07381107413243382, 'again'), (0.0732613497340251, 'activity'), (0.0732399712586993, 'church'), (0.07300299149869316, ':p'), (0.07296279047165011, 'fake'), (0.07223960101490778, 'fold'), (0.07219398153720635, 'inch'), (0.07188013774426039, 'jealous'), (0.07183131047503832, 'chapter'), (0.07171829301004706, '#'), (0.0716537833238271, ':)'), (0.07158508106375505, 'people'), (0.07135697669478369, 'put'), (0.07134005282917566, 'ur'), (0.07131429076399076, 'vc'), (0.07127905753791697, 'aren'), (0.07122064555516294, 'auto'), (0.07119352538001644, 'spanish'), (0.07109296535782317, 'short'), (0.07076584633391647, 'team'), (0.07070507925012315, 'approach'), (0.07052921989276228, 'name'), (0.07041064597871594, 'gif'), (0.0703116769652734, 'female'), (0.07020447422649179, 'linguistic'), (0.06987797324209577, 'family'), (0.0696943913441177, '45'), (0.0695760273809396, 'machine'), (0.06899050179541422, 'tree'), (0.06889390613960789, 'rainbow'), (0.06863202438101568, '60'), (0.06861843234439724, 'campaign'), (0.06855959133160883, ':('), (0.06853013431202015, 'idk'), (0.06789329856005288, 'direction'), (0.0675730569037043, 'enter'), (0.0674402949288373, 'hate'), (0.06718824703263282, 'weird'), (0.06668689348851609, 'entire'), (0.06637206759803238, 'car'), (0.06629407994636471, 'smartphone'), (0.06558575595645766, 'niet'), (0.06537291141795509, '23'), (0.06529361388799981, 'gran'), (0.06524684048596296, 'preach'), (0.06519654356066629, 'reveal'), (0.06508442979723661, 'api'), (0.06503537432054962, '40'), (0.06482991189082243, 'accord'), (0.06464298527684487, 'kanye'), (0.06451798235578932, 'process'), (0.06442295408254894, 'challenge'), (0.06415222431781742, 'data'), (0.06395881929662628, 'don'), (0.06365123214693358, 'reason'), (0.06360928761485773, 'championship'), (0.06352615619948931, 'shock'), (0.06346102109764074, 'close'), (0.06340891233224544, 'mail'), (0.06329037428139128, 'day'), (0.06322893611221514, 'only'), (0.06321738237097874, 'refresh'), (0.06317933562240552, ':-d'), (0.06301076401038674, '2020'), (0.06294864526013644, 'ugh'), (0.0629434968997249, 'offline'), (0.06260742945839426, 'da'), (0.062466376303602855, 'humanity'), (0.06244484533199435, 'other'), (0.06235388183642554, 'toch'), (0.06232573016353715, 'noise'), (0.06218964548797867, 'wolf'), (0.062160763551201814, 'learn'), (0.062053359989851176, 'princess'), (0.0620403612926419, '2007'), (0.0617695611863156, 'not'), (0.061747940005638435, 'promote'), (0.061591668903159924, 'round'), (0.061175838388645154, 'involve'), (0.06103356204184096, 'worth'), (0.06096624996794997, 'unlock'), (0.06075131773386144, 'teen'), (0.060118817923730195, 'asian'), (0.05993471839752518, '->'), (0.0597615251816197, 'bird'), (0.05966075721508912, 'attack'), (0.05965959147766142, 'shut'), (0.05958187314336416, 'figure'), (0.059125236449304897, 'pattern'), (0.05883189213022533, 'hint'), (0.058722571626167896, '18'), (0.05868094658454304, 'awake'), (0.058678901845656384, ''), (0.058401054692808874, 'jack'), (0.05836267932548633, '25'), (0.05825202608983315, 'dark'), (0.058204202933457294, 'por'), (0.05770708491989196, 'yr'), (0.057684678705535175, 'dj'), (0.057599234071909367, 'm'), (0.05727700513125944, 'gift'), (0.05708559383787448, 'w'), (0.05706374069102127, ')'), (0.0569381268522231, 'startup'), (0.056917605758702194, 'clue'), (0.05675909285347869, '2014'), (0.056725299892054215, 'nobody'), (0.056535358254744095, 'reward'), (0.05647730227563552, 'badge'), (0.05631354059871585, '??????'), (0.05625539333662144, 'crazy'), (0.056248726993848086, 'request'), (0.05604414625037424, 'initiative'), (0.055949111533629026, 'cc'), (0.055929038405003295, 'political'), (0.05565989964902229, 'el'), (0.05552892682068111, ''), (0.05541628805290766, 'white'), (0.05526062786817176, 'mental'), (0.05521688506127109, 'advice'), (0.05513816261412763, 'dear'), (0.05488771861368358, '\".'), (0.054755787313989, 'drop'), (0.05444585419155579, 'door'), (0.05424276780425896, 'play'), (0.05415108317310069, '9'), (0.054106035019499954, '???'), (0.05406509460027009, 'just'), (0.05398211836597788, 'epic'), (0.05385078040569269, 'difference'), (0.05372103212589141, '!!'), (0.05364714632047951, 'spot'), (0.05360714987471682, 'tonight'), (0.05359982522052675, 'goodbye'), (0.05349587215604745, '10th'), (0.05338109292942628, 'fam'), (0.0531279245895544, 'add'), (0.0531277643866237, 'tool'), (0.05312588395711182, 'monitor'), (0.05308103125649288, 'remind'), (0.052884988157005486, 'easily'), (0.05269749290372072, 'treasure'), (0.05268401685019053, 'gp'), (0.052673510856763306, 'yay'), (0.0525815087416841, 'mistake'), (0.05245619545847613, 'grand'), (0.052411932338555234, 'better'), (0.05239733618909059, 'wife'), (0.05214638100392466, 'miss'), (0.052031669434104755, 'conference'), (0.05159965068745764, 'mouth'), (0.051534670088421164, 'out'), (0.05150393134831699, 'color'), (0.05135835324221283, 'friday'), (0.051358042880060406, 'hipster'), (0.05124784605933708, 'student'), (0.05114831109138129, 'translation'), (0.0510909671195634, 'non'), (0.05109001777650879, 'hungry'), (0.05104404766474935, 'quote'), (0.05074609999838042, 'guide'), (0.05066018445193876, 'crack'), (0.050624109415863705, 'beautiful'), (0.05056944916383488, 'agree'), (0.05040196314108569, 'twice'), (0.05028654493356233, 'everybody'), (0.04999232161302314, 'follower'), (0.04988165012077306, 'll'), (0.049726030886206196, 'stuff'), (0.049713543373718805, 'finally'), (0.04967326932028704, 'ander'), (0.04964676517812161, 'fun'), (0.0495612468398432, 'tuesday'), (0.049461876181261966, 'sit'), (0.049393523481330304, 'kardashian'), (0.04935647171730495, 'global'), (0.049261765619309505, '!'), (0.048931330338216394, 'rank'), (0.048920987975900054, 'lot'), (0.04889829502557563, 'goal'), (0.04881278112427223, 'report'), (0.04862785092289457, 'blog'), (0.04849878590896117, 'mean'), (0.04849679593986611, 'after'), (0.04842868057569816, 'attempt'), (0.04829925955811909, 'train'), (0.048201514414952795, 'more'), (0.04799200102717638, 'olympic'), (0.04799085998985109, '?!'), (0.047714865881619994, 'free'), (0.04768503460178897, 'vibe'), (0.0475572019081667, 'sir'), (0.04726905803449655, 'tattoo'), (0.047263781877904476, 'ad'), (0.0472304635880072, 'website'), (0.04702291346269405, 'priority'), (0.046992014171926444, 'loui'), (0.04691255877610101, 'also'), (0.046775801291239816, 'send'), (0.04653996256855897, 'wind'), (0.046499469435960616, 'where'), (0.04627960523583341, 'third'), (0.04621214710916455, 'jean'), (0.04592772758132391, '8'), (0.04584131910675748, 'season'), (0.045826768598786094, 'deal'), (0.04577970040640067, ','), (0.04573437196033692, 'airport'), (0.04566191152603416, 'launch'), (0.0456332212835282, 'french'), (0.045525589514712195, 'brand'), (0.04541396867940706, 'along'), (0.04538720182311229, 'south'), (0.04529829487952286, 'naked'), (0.045103114277078804, 'gotta'), (0.04479459536266539, 'mayor'), (0.04472370499572231, 'performance'), (0.04465390089631316, 'guilty'), (0.04461248580555588, 'force'), (0.04459254960140924, ''), (0.0445884236104408, 'che'), (0.04438668821133729, 'lmao'), (0.04433501935645623, 'say'), (0.044261661664097174, 'opinion'), (0.0442205714333781, 'excuse'), (0.04405814248805462, '.....'), (0.044052252050662766, 'successful'), (0.04379429814526281, 'underestimate'), (0.04366938650719354, 'real'), (0.043650472672490004, 'myself'), (0.04361947297043778, 'strange'), (0.04356782505694756, 'headache'), (0.04348386296640672, 'wear'), (0.04325602063330125, 'trend'), (0.04299639356446372, 'executive'), (0.04291150038746516, 'pull'), (0.042848250406452015, 'create'), (0.04280508268236338, 'mark'), (0.04274251732374523, 'sad'), (0.042707026498780776, 'en'), (0.04270152281298745, '5th'), (0.04267947914039394, 'smith'), (0.04258167670237856, 'replace'), (0.04254706219407978, 'head'), (0.042499510201868373, 'girlfriend'), (0.0423380058994971, 'root'), (0.042295057932838276, 'smell'), (0.04229476767407725, '....'), (0.042288731343643526, 'america'), (0.042269890535328924, 'ubuntu'), (0.04209482802474018, 'hour'), (0.04208469561329209, 'perspective'), (0.041926940356852516, 'slave'), (0.04183912612430141, 'didn'), (0.04175812160250758, 'audience'), (0.04172890191868239, 'before'), (0.04172213784941836, 'material'), (0.04169736364701282, 'weekend'), (0.0415313929218315, '70'), (0.04137800099212363, 'scientific'), (0.04133044255640739, 'school'), (0.04131627543697691, 'creepy'), (0.04120496496382442, 'modern'), (0.04112941916459456, 'winter'), (0.04106875249208586, 'year'), (0.04101772530290049, 'pl'), (0.04086741873812039, 'summer'), (0.04068571995444792, 'purpose'), (0.04055395296412834, \"':\"), (0.04043015497848801, 'several'), (0.04036709669174576, '-time'), (0.04014997742470383, 'killer'), (0.0401399663659312, 'less'), (0.040056259867750965, 'hide'), (0.039832397472835845, 'gender'), (0.03973419523647581, 'ride'), (0.03953768122417256, 'research'), (0.03950825158947957, 'darling'), (0.03949251440268964, 'person'), (0.03939307071771969, 'dick'), (0.039380730614537285, 'die'), (0.0393711053464898, 'use'), (0.0393665621385797, 'till'), (0.039317287582726124, 'break'), (0.03918049089987674, 'bullshit'), (0.03917189204259386, 'stem'), (0.039084461198583886, 'or'), (0.038952310819722946, 'fifty'), (0.038835091652581255, 'situation'), (0.03873002762046607, ':/'), (0.038723546528458774, 'easy'), (0.03852451310574101, 'weekly'), (0.03846814586187386, 'u'), (0.038466513252951584, 'sister'), (0.038457967808932514, 'event'), (0.03842749890472663, 'week'), (0.03829705147638296, 'design'), (0.03818028496546022, 'he'), (0.03799556471495058, 'issue'), (0.03798623559377923, 'platform'), (0.03787340982305887, \"'\"), (0.03783342250559474, \"'all\"), (0.03779202191272346, 'yourself'), (0.037669848764234626, 'piece'), (0.03757263238412345, 'pool'), (0.03753850167565065, 'side'), (0.03749978011124466, 'johnny'), (0.037389719536736976, 'inspire'), (0.03737199238085198, 'suffer'), (0.03732445220173286, 'introduce'), (0.03709618083859301, 'this'), (0.03693633520177375, 'pray'), (0.03685454564629964, 'example'), (0.036816480641129745, 'center'), (0.03668039596557149, 'conversation'), (0.036614416623276025, 'press'), (0.03636793806313654, 'language'), (0.036330727359323856, '+'), (0.03624607151808901, 'tf'), (0.0360670473259066, 'eu'), (0.035873874270891726, 'bus'), (0.035863413240693864, 'greet'), (0.035813810066090745, 'het'), (0.03580773061922171, 'computer'), (0.03569893955648329, 'victim'), (0.035565629883699934, 'random'), (0.035497989191059176, 'dont'), (0.035372931500212035, 'pizza'), (0.03533755021483076, 'via'), (0.03525742200970261, 'table'), (0.03525395057144021, 'dad'), (0.034999109415863705, 'lack'), (0.03499341335753603, 'relationship'), (0.034969917116934646, 'regular'), (0.03494224011557345, 'wait'), (0.03483638836498493, 'nail'), (0.03475370423624824, 'may'), (0.034738204385221794, 'chest'), (0.03472752427585735, 'something'), (0.03469612292866664, 'mind'), (0.03464903916447759, '1st'), (0.03457773802409747, 'music'), (0.0345553828208216, 'bone'), (0.03447492599694346, 'urge'), (0.0343204880716077, 'last'), (0.034257836799590935, 'scott'), (0.03419715187048533, 'define'), (0.03419216781944834, 'taste'), (0.03391782593326442, 'tough'), (0.03387667556316676, 'black'), (0.03385602735172899, 'trap'), (0.03383442979723661, 'true'), (0.03354844020203651, 'confuse'), (0.03346387564378772, 'race'), (0.033431744032708766, 'memory'), (0.03337672777703449, 'usa'), (0.033302578607490974, 'title'), (0.03327052002280073, '@username:'), (0.03308335338826551, 'election'), (0.03308001243097736, 'server'), (0.033044266013651846, 'one'), (0.032972663639417865, 'method'), (0.032952252763743806, 'ruin'), (0.03293793690674218, 'from'), (0.03277750061530749, 'rat'), (0.03272528674730424, 'tu'), (0.0326887134135172, 'nigga'), (0.032494003045625774, 'simply'), (0.032473702387798564, '\"my'), (0.032199579580781235, 'cod'), (0.03216220764804101, 'believe'), (0.03182949029229709, 'bed'), (0.03164799877469893, 'apologize'), (0.03163556738853046, 'delete'), (0.031621054722019704, 'cream'), (0.031604751580716295, 'repeat'), (0.031571999245332494, 'jane'), (0.03154516204744273, '17'), (0.03151138734261516, 'engineer'), (0.03139938311876889, 'tag'), (0.03112208366383773, 'thursday'), (0.031098587423236568, 'delay'), (0.031013575578356045, 'shout'), (0.03095111063102296, '..\"'), (0.030928563733475745, 'wonder'), (0.030892744289761964, 'score'), (0.030827623007535276, 'tip'), (0.030790598628405608, 'doesn'), (0.030758393990937494, 'without'), (0.030602628703593426, 'est'), (0.030592039877215482, 'rush'), (0.030543057426916986, 'milan'), (0.030534586365814187, 'grow'), (0.030508479431812674, '/'), (0.030328907541714356, 'connect'), (0.030294566882373974, 'experience'), (0.030257615529632798, 'congratulation'), (0.03015303261255009, 'holy'), (0.030124944837751855, 'deadline'), (0.030005656231621103, 'medicine'), (0.03000485294134414, 'management'), (0.02983833451824669, 'your'), (0.029819986638056628, 'se'), (0.0297799015941016, 'semantic'), (0.029777978207890188, 'art'), (0.02970845708573755, 'korean'), (0.029649561303157546, 'blue'), (0.02947511951687365, 'chili'), (0.029357400978102577, 'yesterday'), (0.029318642222238678, 'almost'), (0.029298563724025284, 'run'), (0.029159079562675938, 'sweet'), (0.029088974229412923, 'refuse'), (0.029081069122823378, 'way'), (0.02870465460167204, 'doubt'), (0.028597853508028903, 'straight'), (0.02859246781185365, 'paul'), (0.0285075016084666, '????????????????????'), (0.02850063003441039, 'go'), (0.028419212704388075, 'comic'), (0.028331398471837188, 'unite'), (0.028310056509705417, 'master'), (0.028242726179217303, 'vote'), (0.028235624362904943, 'forget'), (0.028195405079264546, 'once'), (0.02806558241654744, 'solve'), (0.028017165920761977, 'dan'), (0.0280145650339243, 'state'), (0.027711714792942654, 'seek'), (0.02766440926479352, 'badly'), (0.027598733841146084, 'kinda'), (0.02759280957535326, '.'), (0.02757035396079255, 'bubble'), (0.02753530129416082, 'cold'), (0.027445916994250252, 'hat'), (0.02744255241077731, 'level'), (0.02737499924241149, 'apply'), (0.027340879018764586, 'peace'), (0.027329220824922507, 'freak'), (0.02718342363965176, 'result'), (0.027130004836233068, 'spend'), (0.027109046262642433, 'photo'), (0.02706701044758053, 'which'), (0.026973691850972648, '50'), (0.02695271502078489, 'incredible'), (0.026909994583327768, 'leadership'), (0.026793937394902212, 'cost'), (0.026744334220299093, 'chinese'), (0.026692229891879027, 'mix'), (0.026671161778705654, '-year-old'), (0.02663012094819095, 'trick'), (0.02661348918813844, 'hasn'), (0.026558737653123554, 'stupid'), (0.026535807367035424, 'grab'), (0.026511745171920564, 'nearly'), (0.02646705302196506, 'total'), (0.026440781778588773, '????\"'), (0.026375514443584436, 'kitchen'), (0.026326148604745025, 'jim'), (0.026313031917917673, '????'), (0.026267234565567676, 'annoy'), (0.02624072598642746, 'thank'), (0.026140479011180773, 'everyone'), (0.026134280896429773, 'easier'), (0.026015403063736064, 'row'), (0.02576674820981828, 'steve'), (0.025719491007955897, 'insane'), (0.02569697149530481, 'ex'), (0.025694835473431654, 'recently'), (0.025600777484637183, 'road'), (0.025539663525496747, 'dat'), (0.025521812332142346, 'tour'), (0.025459873067416705, 'form'), (0.025367047398933362, '\"a'), (0.025227521354801885, 'husband'), (0.025165528394202052, 'shouldn'), (0.025163221619449727, 'interaction'), (0.025160638310945282, 'planet'), (0.025138210081280254, 'facebook'), (0.025097388329932135, 'fly'), (0.024908542088454277, 'notification'), (0.024879276763136282, 'manage'), (0.02483024934960887, 'smart'), (0.024807199717374884, ').'), (0.024774849027129564, ':3'), (0.024752229103193857, 'iphone'), (0.024737203923694828, 'to'), (0.024702790237965733, 'upgrade'), (0.024685683806385583, 'march'), (0.02466861388799968, '\"i'), (0.02462153012381063, 'mess'), (0.024597869573834252, 'represent'), (0.02454344665756958, 'some'), (0.0245237477891862, 'receive'), (0.02452018775273146, 'detection'), (0.02441481067367013, 'enemy'), (0.024360460783793725, 'tl'), (0.024309102291065754, '\"there'), (0.02425904538632606, 'bitch'), (0.023971558750154998, 'tryna'), (0.02395627797829558, 'belong'), (0.023934990785955623, 'question'), (0.023864003766439268, '!!!!'), (0.023812762867675197, 'wall'), (0.023715783823327508, 'alive'), (0.02353934938315483, 'code'), (0.02350602464975249, 'world'), (0.023281103372199796, 'le'), (0.023138491963140995, 'public'), (0.023031690869497634, 'leuk'), (0.02290369386650082, 'joke'), (0.022855587732868443, 'super'), (0.022746732772039646, 'mr'), (0.02271597953404947, 'blow'), (0.02262506167997369, 'age'), (0.022612793246652663, 'access'), (0.022611460515056914, 'maar'), (0.02240521573644383, \"'ve\"), (0.022077902333472288, 'never'), (0.02204002902257285, 'casa'), (0.02176804223742823, 'model'), (0.021748936708454236, 'positive'), (0.021506808588036552, '24'), (0.021469948518281656, 'international'), (0.021357999064226973, 'except'), (0.021099321338444055, 'character'), (0.021029115593896197, 'thought'), (0.021008586050340394, 'haha'), (0.020945911152139063, 'url'), (0.020872665684156688, 'west'), (0.020757064910661205, 'case'), (0.020750437765876262, 'read'), (0.02074915980407188, 'bro'), (0.02069283820169776, 'soundtrack'), (0.02065042812639306, '#art'), (0.02064071561668057, 'hunt'), (0.02063958370765384, 'river'), (0.020588319182704984, 'devil'), (0.020582787433752303, 'italian'), (0.020572198607374137, 'hahaha'), (0.020455411155060332, 'movie'), (0.020376232292986574, 'search'), (0.02032473043227423, 'christma'), (0.020323516368560002, 'detail'), (0.020318468419433078, 'select'), (0.02023824893131909, 'let'), (0.020190781778588907, 'can'), (0.020188946990569834, 'her'), (0.01992823365419838, 'toward'), (0.019916585945182197, 'major'), (0.01988558624312997, 'con'), (0.019864207767804176, 'throne'), (0.019814933211950603, 'web'), (0.01977394715122771, 'manager'), (0.019765877735263615, 'voice'), (0.019751044250035488, 'six'), (0.019748077552989685, '\",'), (0.01969799970685915, 'count'), (0.01956426476275297, 'spotify'), (0.019528395918834374, 'corpus'), (0.019509628136908708, 'spring'), (0.019507346062258346, 'country'), (0.0194754517869431, '#bigdata'), (0.019428696641503596, 'haven'), (0.01942387689984182, 'mucho'), (0.019316455081893613, 'soft'), (0.019287901763866744, 'remove'), (0.019267308322220345, 'horror'), (0.019243100074328057, 'seem'), (0.019092793509547734, '#quote'), (0.01907665467761932, 'jeremy'), (0.018760596466824264, 'network'), (0.018729837859246867, 'lunch'), (0.01872126262814855, 'each'), (0.018637656541252934, 'common'), (0.01858093329374033, 'la'), (0.01857476256388546, 'user'), (0.018536569762534594, 'end'), (0.018493922351466185, 'stay'), (0.01839937143454673, 'deep'), (0.018248900560391634, 'linkedin'), (0.018220347242364765, 'tan'), (0.018185769247260453, 'fail'), (0.018145203088272943, '.'), (0.01810184366991363, 'answer'), (0.017986407205793142, 'fast'), (0.017961176588457306, 'surprise'), (0.01795602822804554, 'meet'), (0.01793744301209199, 'red'), (0.01793585468813519, 'quick'), (0.017884060721867767, 'room'), (0.01784962877954066, '!'), (0.017819247117000003, 'silly'), (0.0177849064576594, 're'), (0.017750020785196163, 'community'), (0.017739906630345192, 'coffee'), (0.017692037832476304, 'universe'), (0.01765864651619009, 'bully'), (0.017643073638775375, '2012'), (0.017622261117962834, 'need'), (0.01755459035563267, 'cinema'), (0.017531617112845232, 'bad'), (0.01751415843633053, 'hug'), (0.017478125283037382, 'l'), (0.017408658930676335, 'hall'), (0.017395842799438954, 'box'), (0.017254689911626597, 'outfit'), (0.017245506843233116, 'google'), (0.017215558902049954, 'terrify'), (0.01719925576074699, 'monday'), (0.017103207802857012, 'compete'), (0.017062785011853432, 'support'), (0.01702410196717219, 'analyze'), (0.016969898130073657, 'floor'), (0.016955895320017733, 'important'), (0.01686010295448881, 'image'), (0.016840358444612624, 'influence'), (0.0167692216136075, 'talk'), (0.01675536485632967, 'de'), (0.01671405017685723, '80'), (0.01661011536897483, \"'re\"), (0.016528234530515107, 'sport'), (0.016393026171622527, 'fact'), (0.016354103106383633, 'national'), (0.016311218359551516, 'spell'), (0.01630931967344229, 'hashtag'), (0.016262162882864528, 'speak'), (0.01618020633322237, 'worst'), (0.016134001570492718, '5'), (0.016112784719747975, 'jay'), (0.01610466321878601, 'i'), (0.016092705147617403, 'big'), (0.016073955622289082, 'perfectly'), (0.01605862008063741, 'lose'), (0.015994922812992973, 'biggest'), (0.01596478117100908, 'graphic'), (0.015923338695356337, 'detect'), (0.015836948477386814, 'truth'), (0.015808395159359945, 'journey'), (0.015627527050860124, 'through'), (0.015604195119633513, 'robert'), (0.01558361993458468, 'cuz'), (0.015567070329219224, 'ugly'), (0.015486312271487446, 'hey'), (0.015466156988174662, 'feat'), (0.015432856954874508, \"'a\"), (0.015371843407018693, 'speech'), (0.015272637057812233, 'funny'), (0.015241089657844098, 've'), (0.015173430708605995, 'uk'), (0.014904064423423735, 'follow'), (0.01487310055630231, 'video'), (0.014852479729760448, 'engine'), (0.014836724286373393, 'impress'), (0.014810306990219235, 'forward'), (0.014758020095827051, 'java'), (0.014727714144468385, 'exam'), (0.014557398349152706, 'innovative'), (0.014507776917952242, 'reply'), (0.014451929987105316, 'geek'), (0.014399929828680769, 'baby'), (0.01428365893067629, 'submit'), (0.01423626480433482, 'both'), (0.014089591302398174, 'will'), (0.014045373823970309, 'soo'), (0.013952447744202123, 'today'), (0.013913962837296179, 'ass'), (0.013826943444987139, 'sound'), (0.01373216364233909, 'group'), (0.01369064814029719, 'bore'), (0.013559638798761542, 'cast'), (0.013513340068252111, 'owner'), (0.0134400033172839, 'regard'), (0.01335425208021701, 'gay'), (0.013236223179293516, '[pic]:'), (0.013231385181034172, 'fix'), (0.013189139415104245, '??'), (0.013014697628820349, 'anything'), (0.012897453761576694, 'sense'), (0.01282387967484433, 'somebody'), (0.012705686464546062, 'shoot'), (0.012688598289563258, 'fire'), (0.012687575920119487, \"'ll\"), (0.012615261538594602, 'visit'), (0.012593353621950243, 'kid'), (0.012544864099776287, 'test'), (0.01252165727993626, 'city'), (0.012501742017180462, 'storm'), (0.012303612296024502, 'dance'), (0.0122570488448559, 'weak'), (0.012249289791044227, 'st'), (0.012223584502180973, 'radio'), (0.012210293699416397, 'fit'), (0.012186870485203727, 'roommate'), (0.01208673304954, 'pretend'), (0.01203269352181624, 'advertise'), (0.012027607052965017, 'mountain'), (0.01200468790170528, 'nexus'), (0.012002980909866734, 'weather'), (0.012001273918028188, 'shadow'), (0.012001250291843402, '\"how'), (0.011983400709365322, 'him'), (0.011978252348954221, 'dead'), (0.01194582018228485, 'august'), (0.01191706536342485, 'celebrate'), (0.011889653082723273, 'topic'), (0.0118824964966191, 'ho'), (0.011776443923461555, 'often'), (0.011740441913775213, '?'), (0.011684117626607149, 'co'), (0.011673513228425358, 'mi'), (0.01163584074009516, 't'), (0.01162115330764446, ''), (0.011612207575014244, 'price'), (0.011569213966862835, 'page'), (0.011475976846678337, 'album'), (0.01147367651543063, 'europe'), (0.01146133505572089, 'skype'), (0.01142898436547557, 'london'), (0.01136705798775961, 'text'), (0.011302521594907411, 'dit'), (0.011247349479893254, 'for'), (0.011216021159091039, 'pm'), (0.011196678294353557, 'best'), (0.011194186268835393, 'wake'), (0.011147339840410053, 'quiero'), (0.011129320578969981, 'happiness'), (0.011124792942862838, 'market'), (0.011072341739096103, 'earth'), (0.011068119095554385, \"',\"), (0.010990324654420958, 'analysi'), (0.010933591600346126, 'achieve'), (0.010912961645505481, 'click'), (0.01085225845980231, 'set'), (0.010695927217944545, 'ya'), (0.01056769287918402, 'join'), (0.01052991997956898, 'anybody'), (0.01047787109920395, 'favorite'), (0.010359428061786247, 'offer'), (0.010229106027439316, 'us'), (0.010189945626436714, 'english'), (0.010134628136908796, 'af'), (0.010116992264009639, 'bug'), (0.010092444262538747, 'ever'), (0.010064942705381386, 'whenever'), (0.010038361099852011, 'factor'), (0.009984221160843854, 'paint'), (0.009910345840258072, 'magazine'), (0.009901327081239364, 'call'), (0.009870236096200635, '????????'), (0.009861235593779272, 'measure'), (0.009821344928888864, 'pack'), (0.009781609445074224, 'ten'), (0.00975925424179791, '!!!!!!!'), (0.009687487558189378, '3'), (0.00968306946166586, 'sometime'), (0.009612489456875206, 'second'), (0.009602758690565594, 'cup'), (0.009546747450343673, 'extra'), (0.009498111875392512, 'copy'), (0.009332433255766492, 'blank'), (0.009254678408274541, 'phd'), (0.00921827475344994, 'ton'), (0.009171994279538076, 'lady'), (0.00912713782020802, 'story'), (0.009114559024734348, 'microsoft'), (0.009106745201131083, 'else'), (0.00909874612676198, '#nowplaying'), (0.009026475380200072, '!!!!!'), (0.008966623132796725, '14'), (0.008907620495427482, 'stop'), (0.008860309202037175, 'soooo'), (0.008832056439600278, 'stand'), (0.008815561604026412, 'jump'), (0.008812375827814467, 'hell'), (0.008795579758386962, 'candidate'), (0.008790595707349969, 'all'), (0.008696136073416794, '):'), (0.008673963436112597, 'welcome'), (0.008672019108510343, 'there'), (0.008623456559947895, 'bbc'), (0.008615368887386232, 'lock'), (0.008595031038101109, 'main'), (0.00845219142157716, 'work'), (0.008404450419888798, 're'), (0.008376536082764119, 'graduate'), (0.008325910538717674, 'surround'), (0.008171861371510492, 'information'), (0.008143692798552093, 'current'), (0.008114426116706541, 'swag'), (0.008101062287553473, 'animal'), (0.00799647024217176, 'chat'), (0.00796738510742867, 'victoria'), (0.0077486186785307165, 'publisher'), (0.007748454369155944, 'mom'), (0.007734351147815843, 'town'), (0.007511045579115638, 'sign'), (0.007457882368057556, 'cheer'), (0.007432177079194524, 'lonely'), (0.007424436281979974, 'gut'), (0.0072631027324885356, 'statistic'), (0.007203056784284678, '--'), (0.00720068342664848, 'sale'), (0.007184325515553702, 'complete'), (0.007123491848876284, 'ain'), (0.006992229599773303, 'check'), (0.006945684405201824, 'describe'), (0.006912557809575004, 'professional'), (0.0068325939138218494, 'like'), (0.006755274539869349, 'pink'), (0.00665433649872238, 'resource'), (0.006650064454976734, 'proceed'), (0.006623720185211512, 'asleep'), (0.00656414622374113, 'back'), (0.0065424482579641285, 'touch'), (0.006476350247787144, 'bowl'), (0.006466768219048902, 'produce'), (0.006451761296146996, 'instagram'), (0.006438287927410524, 'chill'), (0.0063678722319950865, 'sort'), (0.006360040151794255, '#love'), (0.006244329838715812, 'could'), (0.006201445091883473, 'dutch'), (0.006145744213814197, 'map'), (0.006104447790938883, 'nothing'), (0.006080385595824023, 'anyone'), (0.006025159389282386, 'ocean'), (0.006009823847630935, 'basically'), (0.005962502747678178, 'drama'), (0.0059083901935654826, 'comment'), (0.005887267310600519, '13'), (0.005872004795338226, 'century'), (0.005863412381559963, 'growth'), (0.0057986680443697924, 'fa'), (0.005777709470779824, 'flight'), (0.005707312031961065, 'feel'), (0.00568624391878747, 'air'), (0.005660611656313375, 'film'), (0.005527466292904792, 'hurt'), (0.005481660490520124, '11'), (0.005434668009317134, 'treatment'), (0.0054068266985809466, 'amount'), (0.005396456951369144, 'hoy'), (0.0053091173903454525, 'todo'), (0.005289601087934415, \"'.\"), (0.005238172253610784, 'photographer'), (0.005145118377662072, 'goodmorning'), (0.005058728159692993, 'premier'), (0.005036108235757286, 'relate'), (0.004994574477118263, 'hand'), (0.004991489112190717, 'treat'), (0.00497060356498924, 'find'), (0.0049528033827155316, 'april'), (0.004913588211921338, 'reference'), (0.004902177838669086, 'lego'), (0.004887298711947485, 'argue'), (0.004836746194289754, 'lately'), (0.004776271216051953, '<3'), (0.004758023747146289, 'three'), (0.004748384263822736, 'steal'), (0.004619291864993436, 'suck'), (0.004613413240694086, 'breakfast'), (0.004566676351851706, 'excite'), (0.004554864333460751, 'beach'), (0.00451427991787634, 'kill'), (0.004491492999771918, 'job'), (0.004461846970706507, 'attention'), (0.0043930013426503045, 'single'), (0.004336679740276184, 'everyday'), (0.00421571152720257, 'max'), (0.004167057695654286, 'movy'), (0.0038780922751098235, 'professor'), (0.003849429417499328, 'dirty'), (0.0037079042759742276, 'male'), (0.0036492093159636863, 'failure'), (0.0034854476390440148, 'iemand'), (0.0032919277086820298, 'of'), (0.0032236480351390817, 'phone'), (0.003184651943511252, 'slip'), (0.0031593300431895788, 'tbh'), (0.0031560986254846046, 'similar'), (0.0031487412168111906, 'paper'), (0.0030997222533184488, 'sery'), (0.003009899795075155, 'cheese'), (0.002987188588153167, 'within'), (0.002950237235412656, 'brother'), (0.002942879826739242, 'coolest'), (0.002921446581621856, 'button'), (0.002895887345536252, 'step'), (0.0028198668747791977, 'grant'), (0.002747137028493274, 'another'), (0.0027014792854163083, '>>>>>'), (0.0026053883708268, 'honest'), (0.002581910386822761, 'continue'), (0.0025353743205496215, 'al'), (0.002508318043493274, 'post'), (0.0024804767327575306, 'software'), (0.0023752639630707506, 'mad'), (0.0023446476495598834, 'moon'), (0.0023050125770300856, 'wild'), (0.0022406945850805826, 'netherland'), (0.0022194073927408464, 'serious'), (0.002214186005940144, '?\"'), (0.002129931809843777, 'you'), (0.0021167049051697973, 'far'), (0.0020377306243766213, '6'), (0.001714776789425665, 'sin'), (0.0016686241116941058, 'effect'), (0.0016562278821925513, 'ii'), (0.001601978403601212, 'd'), (0.0015743287871357037, 'cover'), (0.0014599329490556645, 'tht'), (0.0013151581332282447, 'record'), (0.001294820283942899, 'nap'), (0.0011988636090387583, 'night'), (0.0011177678042588735, 'eat'), (0.0010365989730900527, 'whatever'), (0.0010342256154536322, '2013'), (0.000984786750225064, 'cute'), (0.0009466943601592614, 'g'), (0.0009401858832558396, 'low'), (0.0009297248530582003, 'mee'), (0.0009245764926464339, 'sugar'), (0.000878314275331693, 'german'), (0.0007460672438972527, 'be'), (0.0007338498216566958, 'healthcare'), (0.0006573364227751011, 'trust'), (0.0006566244154839751, 'poor'), (0.0006487923352835878, 'theory'), (0.0003343224484451124, 'sadly'), (0.0003338477769179171, 'download'), (0.0002824371991916319, 'size'), (0.0001883609537995934, 'cool'), (0.00011810956775848425, 'alone'), (4.8113774078650806e-05, 'help'), (0.0, 'face')]\n",
      "2120\n",
      "3\n",
      "0.704225352112676\n",
      "0.7112676056338029\n",
      "0.7323943661971831\n",
      "0.6901408450704225\n",
      "0.676056338028169\n",
      "0.6267605633802817\n",
      "0.5563380281690141\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6830985915492958\n",
      "0.6901408450704225\n",
      "0.704225352112676\n",
      "0.7676056338028169\n",
      "0.6408450704225352\n",
      "0.6267605633802817\n",
      "0.5845070422535211\n",
      "************************************ ****************************\n",
      "7\n",
      "0.676056338028169\n",
      "0.6971830985915493\n",
      "0.704225352112676\n",
      "0.7605633802816901\n",
      "0.6549295774647887\n",
      "0.6408450704225352\n",
      "0.5774647887323944\n",
      "************************************ ****************************\n",
      "9\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.6549295774647887\n",
      "0.647887323943662\n",
      "0.6056338028169014\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7183098591549296\n",
      "0.7816901408450704\n",
      "0.7464788732394366\n",
      "0.704225352112676\n",
      "0.6408450704225352\n",
      "0.6338028169014085\n",
      "0.5774647887323944\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7394366197183099\n",
      "0.7535211267605634\n",
      "0.7394366197183099\n",
      "0.7464788732394366\n",
      "0.6197183098591549\n",
      "0.6690140845070423\n",
      "0.6338028169014085\n",
      "************************************ ****************************\n",
      "15\n",
      "0.7253521126760564\n",
      "0.7464788732394366\n",
      "0.7112676056338029\n",
      "0.7323943661971831\n",
      "0.5985915492957746\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "************************************ ****************************\n",
      "17\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.6056338028169014\n",
      "0.676056338028169\n",
      "0.5915492957746479\n",
      "************************************ ****************************\n",
      "19\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.7183098591549296\n",
      "0.6830985915492958\n",
      "0.5985915492957746\n",
      "0.6971830985915493\n",
      "0.6126760563380281\n",
      "************************************ ****************************\n",
      "0.7112676056338029\n",
      "0.729264475743349\n",
      "0.7175273865414711\n",
      "0.7245696400625978\n",
      "0.6322378716744914\n",
      "0.6549295774647887\n",
      "0.5970266040688575\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_train.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_train.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_test.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_test.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_female_chosen_set.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/PAN2015_tweet_male_chosen_set.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "x =(dict(zip(selector.scores_, word_list)))\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "print(sorted_x)\n",
    "df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ennumerate', ',', 'aListOfWord', ',', 'append', '.']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "def tokenizeLineUTF8(newLine):\n",
    "   aListChar = []\n",
    "   aListCode = []\n",
    "   for i, c in enumerate(newLine):\n",
    "      aListCode.append(unicodedata.category(c)[0])\n",
    "      aListChar.append(c)\n",
    "   seqChar = seqPunct = seqNumber = False\n",
    "   aLen = len(aListCode)\n",
    "   aToken = u''\n",
    "   aListOfWord = []\n",
    "   for anIndex in range(aLen):\n",
    "      aCode = aListCode[anIndex]\n",
    "      aChar = aListChar[anIndex]\n",
    "      if ((aCode == \"L\") & (seqChar)):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"P\") & ((seqPunct) | (seqNumber))):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"N\") & ((seqPunct) | (seqNumber))):\n",
    "         aToken = aToken + aChar\n",
    "      elif ((aCode == \"Z\") | (aCode == \"C\") | (aCode == \"M\") | (aCode == \"S\")):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = u''\n",
    "         seqChar = seqPunct = seqNumber = False\n",
    "      elif ((aCode == \"L\") & (not seqChar)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqChar = True\n",
    "         seqPunct = seqNumber = False\n",
    "      elif ((aCode == \"P\") & (not seqPunct)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqPunct = True\n",
    "         seqChar = seqNumber = False\n",
    "      elif ((aCode == \"N\") & (not seqNumber)):\n",
    "         if (len(aToken) > 0):\n",
    "            aListOfWord.append(aToken)\n",
    "         aToken = aChar\n",
    "         seqNumber = True\n",
    "         seqChar = seqPunct = False\n",
    "   if (len(aToken) > 0):\n",
    "      aListOfWord.append(aToken)\n",
    "   return(aListOfWord)\n",
    "\n",
    "print(tokenizeLineUTF8(\"ennumerate, aListOfWord, append.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "[(0.19425441916459452, 'national'), (0.15430900458570873, 'recent'), (0.1507316968997141, 'higher'), (0.14531316287141527, ':)'), (0.13280391791146173, 'gimme'), (0.13118015789954374, 'announce'), (0.12495704986722522, 'pero'), (0.1231623689074901, 'share'), (0.11822027421597592, 'musical'), (0.11697759396405871, 'hero'), (0.11603600015670179, 'rip'), (0.1138142270467708, 'perfect'), (0.11370760851909956, 'situation'), (0.11278728876272415, 'figure'), (0.11050852324764593, 'until'), (0.10842414928827182, 'yu'), (0.10678698356441885, 'base'), (0.10632263238412332, 'theory'), (0.10586297758628405, 'skin'), (0.10571963523507355, 'compete'), (0.10551035449173729, 'most'), (0.10489449316777333, 'kardashian'), (0.10181689882459732, 'her'), (0.10080595716024665, '!!!'), (0.10068472131858086, 'ride'), (0.0989923530143706, 'chrome'), (0.09638980205655634, 'yet'), (0.09622534662894311, 'lmao'), (0.09594722025755043, 'por'), (0.09514468387064867, ','), (0.09436834945944583, 'eminem'), (0.09434230642353447, 'gap'), (0.09426968167985716, 'beach'), (0.09345461589768611, 'advertise'), (0.09294758455183882, 'river'), (0.0924081842591491, 'roommate'), (0.0923426978449784, 'translator'), (0.09190842816728773, 'football'), (0.09137584671365362, 'like'), (0.09123780993872188, 'eye'), (0.09028813826536597, 'straight'), (0.09017660735651956, 'este'), (0.08965466037075687, 'employment'), (0.08936380539240196, 'contest'), (0.087779351834264, 'tl'), (0.08750413739012552, 'cinema'), (0.08733734135935878, 'involve'), (0.08721137996694983, 'simple'), (0.08706686987178225, 'plus'), (0.08696001634540806, 'love'), (0.086558350613263, 'brown'), (0.08626439577209433, ''), (0.08623183137226986, 'program'), (0.08584490105112907, 'water'), (0.08571160167517444, 'joy'), (0.0854542646407559, 'five'), (0.08503100744989034, 'member'), (0.08497252781033482, 'gran'), (0.08470308782050018, 'wish'), (0.08463861464747424, 'common'), (0.08460434701452257, 'top'), (0.08420310352117366, 'html'), (0.08408576837094373, 'olympic'), (0.08361472990648444, 'sign'), (0.08316990791560963, 'teacher'), (0.08259442345854606, 'tour'), (0.08257673925936038, 'ouch'), (0.08256228003437482, 'head'), (0.08233881284109335, 'finger'), (0.08229404766474957, 'enjoy'), (0.08211167983048528, 'stick'), (0.0818590842363649, 'identify'), (0.08179423680309639, 'chili'), (0.0814500360477115, 'happy'), (0.08125404234842826, ')'), (0.08122919511963356, 'sadly'), (0.08013696680701066, 'whenever'), (0.07987668625938471, 'ed'), (0.07941589511554414, 'sister'), (0.07890105130262093, 'author'), (0.07855086102682596, 'del'), (0.07835712201729739, 'third'), (0.07829028461493359, 'york'), (0.07823228340561661, 'italy'), (0.07810281674654473, 'cast'), (0.07808681483909541, 'soooo'), (0.07785413450773082, 'question'), (0.07711877702895231, '1.'), (0.0769373794791337, 'comment'), (0.07673818174704139, '2014:'), (0.0762595667947421, 'hungry'), (0.07580481697369978, \".'\"), (0.0756871677026063, 'further'), (0.07519621129059706, 'group'), (0.07496179658249824, ',...'), (0.07486669796766288, 'stage'), (0.07471860045114442, 'cup'), (0.0746008088859842, 'deal'), (0.0743484394927989, 'accept'), (0.07432920548938071, '!)'), (0.07432911152160115, 'pic'), (0.07391780219968824, 'totally'), (0.07336872591048027, 'round'), (0.07325392842726175, 've'), (0.07325288780122108, 'shoot'), (0.07324007166998392, 'wood'), (0.07315740579784435, 'fix'), (0.07306223415662005, 'arrive'), (0.07299278606085613, 'corner'), (0.07203617795448025, 'today'), (0.07197238968645947, 'similar'), (0.07136879247188577, 'ice'), (0.07129761536897505, 'immediately'), (0.0712053373984074, 'huh'), (0.07113649177035142, 'within'), (0.07095628089935113, 'place'), (0.07039949119782452, 'dropbox'), (0.070253201084429, 'bigdata'), (0.06989831109138112, 'anniversary'), (0.06987512521293193, 'weekend'), (0.06982853437686742, 'usa'), (0.0692089392961659, 'crush'), (0.06917317730703676, 'heb'), (0.06897680934751094, 'anyone'), (0.06883210891459979, 'girlfriend'), (0.06875751110189676, 'status'), (0.06861416030065137, 'para'), (0.06851618627175649, 'loud'), (0.06803477673181857, 'hour'), (0.06792590484265904, 'uk'), (0.06774574605665684, 'r'), (0.06747217094755542, 'english'), (0.06721864426697755, 'during'), (0.06697417017381935, 'the'), (0.06675329273254715, 'en'), (0.06670028470256528, 'rich'), (0.0666475231366459, 'forget'), (0.06664749575175, 'driver'), (0.06655987321346957, 'cause'), (0.0665364499992569, 'workshop'), (0.06653621266349319, 'shame'), (0.06617887628642016, 'spend'), (0.06567847208535804, 'tea'), (0.06558646796374856, '('), (0.06545434364151714, 'my'), (0.06533732931000458, 'march'), (0.06529304793348634, 'fair'), (0.06527358640086689, 'nd'), (0.0651581681933433, 'site'), (0.06477591156108686, 'shape'), (0.06444900624675887, 'kanye'), (0.06436305150232924, 'when'), (0.0643491818580415, 'brand'), (0.0641347033540891, 'blank'), (0.064056364295487, 'g'), (0.06372902350762, 'smart'), (0.0637224328760293, 'bed'), (0.06369873581285845, 'enough'), (0.06369632325723384, 'iemand'), (0.06351326704186366, '?)'), (0.06348604645543232, 'chest'), (0.06341879627310121, 'soft'), (0.0631525166811131, 'che'), (0.06296701139692362, 'link'), (0.06278896393269195, 'analyze'), (0.06266749366319524, ''), (0.06259923224624964, 'facebook'), (0.06254951953206334, 'luck'), (0.06247919511963351, 'np'), (0.06246298326131661, 'confirm'), (0.06223766033862521, 'dinner'), (0.06215236727702478, 'add'), (0.061983254656587716, 'ain'), (0.061936243918787826, 'africa'), (0.06180359148350356, 'application'), (0.06164654823435489, 'other'), (0.060889392378515206, 'respond'), (0.06087821934102622, 'require'), (0.06061086704677754, 'introduce'), (0.05995866192475807, 'deep'), (0.05933299921027979, 'influence'), (0.05928757679643626, 'eat'), (0.05926355487322721, 'fat'), (0.05918734807778647, 'itune'), (0.05912920081569184, 'citation'), (0.0590417152018905, 'mix'), (0.05834931549633282, 'note'), (0.05825516622455207, '--'), (0.05823300003075271, 'dance'), (0.05816990791560972, 'chill'), (0.05812859323613706, 'terror'), (0.05811981181288206, 'smartphone'), (0.05807068716313002, 'strong'), (0.05802677619353025, 'rather'), (0.05789505484470392, 'nearly'), (0.057853521086064674, 'page'), (0.057845378643711864, 'paint'), (0.05784284097670067, 'hoy'), (0.05758599535414288, 'pop'), (0.0575530540092819, 'bike'), (0.057529156123542036, 'hire'), (0.057491145888163286, '25'), (0.05741404827817109, 'pussy'), (0.0571879585783972, 'hack'), (0.057068341381777854, 'last'), (0.056929098286642166, 'service'), (0.05685046712248476, 'lose'), (0.056748248434739246, 'predict'), (0.05667237401675984, 'smh'), (0.056638599311932714, 'un'), (0.0563671328398081, 'sustainable'), (0.05607793921179893, 'listen'), (0.055822182541568344, ''), (0.055811319866232, 'mark'), (0.05566678238616829, 'represent'), (0.05565509816395764, 'crazy'), (0.05557732505986901, 'adventure'), (0.05554716516128777, 'creative'), (0.05547958836673739, 'realize'), (0.05541710516280696, 'marry'), (0.05523428359840632, 'yes'), (0.05517447498596617, 'state'), (0.05515904816132888, 'grow'), (0.0551446410213412, 'model'), (0.055076573983459776, 'smoke'), (0.05470495181907453, 'quote'), (0.05467544915799283, '??????'), (0.05460842918965714, 'topic'), (0.05433667974027623, '2015'), (0.05418447448938668, 'perform'), (0.05397839402014837, 'explain'), (0.053958220480237795, 'silly'), (0.05383951475695148, 'circle'), (0.053837799965080535, 'grand'), (0.053752739793913484, 'walk'), (0.05364837019075597, 'conversation'), (0.05349318843625839, 'min'), (0.05342974676097478, 'smith'), (0.05336566610478877, 'former'), (0.05309596139429473, 'screen'), (0.05301282085062775, 'healthy'), (0.052957758953460665, '100%'), (0.05284360045114411, 'shitty'), (0.05279739300362096, 'replace'), (0.052769740702361956, 'news'), (0.05250138053655573, 'lie'), (0.052442621678454815, 'strip'), (0.05242525052621527, '!!!!!'), (0.051838720827843465, 'linguistic'), (0.0518020980938525, 'las'), (0.05170439859217901, '?????'), (0.05154070013508605, 'fold'), (0.051506213422967795, 'kitchen'), (0.05137306805955921, 'prize'), (0.050983673097795945, 'fail'), (0.0509112856898819, 'photography'), (0.05074144456609364, 'somebody'), (0.05013450467031144, 'be'), (0.05002903563000061, 'sexy'), (0.04998422212736964, '????????????????'), (0.049908469062065475, 'university'), (0.04929543078455345, 'mountain'), (0.04913024509305197, 'killer'), (0.048967286706409485, 'easy'), (0.04877124736563343, 'on'), (0.048665968013062555, 'victoria'), (0.04861696086266254, 'worst'), (0.048616102802593986, 'credit'), (0.04841120901217377, 'holy'), (0.04839032346497252, 'compare'), (0.04826975689703761, 'refresh'), (0.04813191690335383, 'wake'), (0.04800463459244142, 'breath'), (0.047778070221140334, 'leuk'), (0.04773668251527896, '????????'), (0.0477259293795258, 'issue'), (0.04752611092312842, 'lately'), (0.04744373715654393, 'worry'), (0.04726936839664897, '\":'), (0.047239601014908095, 'public'), (0.04715449788704151, ').'), (0.04713002867720095, 'stupid'), (0.0467181287006726, 'af'), (0.046700848831418895, 'negative'), (0.046187683268911206, 'creativity'), (0.04617451113402882, 'coolest'), (0.04611425659148405, '6'), (0.04610760446398743, 'scheme'), (0.04600889641249273, 'industry'), (0.04597859046113428, 'design'), (0.045974555753152124, 'statistic'), (0.045888710548305234, 'area'), (0.04585446385674441, 'jay'), (0.04574711506518514, 'wouldn'), (0.04547536561580401, 'after'), (0.04538240302284158, 'ander'), (0.0453472043034322, 'jason'), (0.045287633026755536, 'ur'), (0.045275200284059736, '-.-'), (0.045267203894484576, 'detection'), (0.04524925765943277, 'tire'), (0.045106253733534274, 'jam'), (0.045078047290854295, 'difference'), (0.04501896894230217, 'journey'), (0.04491767865622087, 'original'), (0.04488345022125717, '????'), (0.04485670430635347, 'wonder'), (0.0448245179254827, 'believe'), (0.04475828299082685, '9'), (0.04469776237109557, 'medical'), (0.04461588153263585, 'depend'), (0.044532978324732575, 'agree'), (0.04450663405496735, 'development'), (0.04430347464128159, 'ads'), (0.04421984116949007, 'cut'), (0.04395900916523732, 'absolutely'), (0.04395150570378625, 'shop'), (0.04377008989737052, 'hip'), (0.04369758113678124, 'teen'), (0.04368718668946725, 'music'), (0.04368678504432899, 'october'), (0.0436534119846399, 'analysi'), (0.04364733253777109, 'put'), (0.04364327957319181, 'dit'), (0.043383561221368216, 'august'), (0.04337651417484745, 'him'), (0.04337587519394526, 'si'), (0.043363935379374, 'championship'), (0.04332816802065764, 'jealous'), (0.04291710516280678, 'award'), (0.0428355529430966, 'cancer'), (0.04266465025187682, '...'), (0.04254624064720547, 'naked'), (0.042462388096247805, 'hahaha'), (0.04231142429396795, 'delay'), (0.04218277918177038, 'artist'), (0.04209513570209911, 'com'), (0.04197006243944834, 'turn'), (0.041927342001991, 'sound'), (0.0418005499344094, 'mission'), (0.041685369062649524, 'once'), (0.041536848959601747, 'nothing'), (0.041290880510266526, 'math'), (0.04126161518494831, 'asshole'), (0.04125034173617492, '80'), (0.04103448485913397, 'prevent'), (0.0409695643994763, 'town'), (0.04096764477197645, 'mom'), (0.04083711278676172, 'logo'), (0.04080268084443506, 'till'), (0.04076776241748892, 'organization'), (0.04071369818966275, 'non'), (0.040656737606386883, 'santa'), (0.040565199028005816, 'target'), (0.04050016902876541, 'clear'), (0.04046615698817435, 'ones'), (0.040422487885926284, '....'), (0.040386631250753924, 'designer'), (0.04034011344107835, 'cover'), (0.04016725152871614, '\"'), (0.04002944092645855, 'mexico'), (0.0398774456264368, 'cute'), (0.039703104251437527, 'reference'), (0.03961309922722189, 'middle'), (0.03951546294537511, 'server'), (0.03951038761135228, 'humanity'), (0.03938715558022565, 'ill'), (0.03933034104972677, 'bet'), (0.039206896382938616, 'unlock'), (0.03919655670541644, 'by'), (0.03887662809601378, 'fall'), (0.038864925617206, 'set'), (0.03879996864435453, 'dead'), (0.03869039254793649, 'storm'), (0.03867116835108053, 'enemy'), (0.03864885878929725, 'major'), (0.038531231533512234, 'essential'), (0.038520981528099574, 'proud'), (0.03842694985028294, 'have'), (0.03838019470484366, 'nah'), (0.038322759450040156, 'cool'), (0.03830011214120854, 'little'), (0.03819934485294141, 'uh'), (0.03819169533871292, 'detect'), (0.03817935387900295, 'saturday'), (0.03817079153491432, 'discover'), (0.03814688452087589, 'position'), (0.03807744555341053, 'goodmorning'), (0.038006628212856164, 'of'), (0.037933547054248784, 'airport'), (0.037893747672344213, 'fighter'), (0.03777223176135447, 'selfie'), (0.03777047779975673, 'favorite'), (0.03732476256388506, 'favourite'), (0.03730221566633829, 'stay'), (0.03728583949864639, 'oh'), (0.037257359207008234, 'b'), (0.03725160837888897, 'tf'), (0.0372288971719672, 'serious'), (0.03721660779725511, 'sky'), (0.037123638760787836, 'cost'), (0.03705586114366799, 'anti'), (0.037044539368607854, 'hard'), (0.037000561910737195, 'approach'), (0.036969160563546266, 'el'), (0.03692793716705989, 'man'), (0.03666757983433411, 'smarter'), (0.03666691078374251, 'grant'), (0.03659201309371318, 'date'), (0.03652320666364517, 'down'), (0.03647962816611927, 'strategy'), (0.03645793932864083, 'medicine'), (0.036290781924641635, 'data'), (0.03624678352538013, 'package'), (0.036232159991019586, 'speech'), (0.03615747225185828, 'war'), (0.03606260828799246, 'overrate'), (0.03601917852803793, 'push'), (0.035985951521126935, 'didnt'), (0.03594845247046985, 'remove'), (0.03591546279932234, 'against'), (0.03591175671008995, 'de'), (0.035685101055802804, 'search'), (0.03554189630733462, 'bae'), (0.03553899350837919, 'fresh'), (0.03544095558139415, 'sense'), (0.03539194574619997, '3'), (0.035251872004152585, 'notice'), (0.03524745390762929, 'support'), (0.035226477077441753, 'structure'), (0.035143336533774994, 'shout'), (0.03505924664705362, 'heaven'), (0.03500338145960935, 'bout'), (0.03490026014422032, 'whatever'), (0.03480568076848778, 'translate'), (0.03446669227160459, 'isnt'), (0.034448271365025596, 'font'), (0.034385596466824486, 'without'), (0.034353532512547025, 'android'), (0.034348073789982836, 'about'), (0.03428157037595625, 'waste'), (0.034132450489994026, 'anyway'), (0.034017744289761565, 'sorry'), (0.03401321396886137, 'robert'), (0.03395435738426955, 'step'), (0.03392319337284233, 'message'), (0.033866707461093215, 'hint'), (0.03384469913316379, ';-)'), (0.033815753298297224, 'one'), (0.0337845619019741, 'user'), (0.033672968451564644, 'south'), (0.033503182097567974, 'chase'), (0.03337993180984378, 'prove'), (0.03336490663034519, 'bar'), (0.033362697582083545, 'io'), (0.03320420293345716, 'pc'), (0.03318515217427476, 'impact'), (0.033178124740878445, 'semi'), (0.032949477760968904, 'iphone'), (0.032941408345004586, 'family'), (0.032879226374927795, 'pool'), (0.0328112022937459, 'gender'), (0.032617919699147624, '???'), (0.032602584157496395, 'sure'), (0.03257548599765814, 'week'), (0.03221650921163044, 'purpose'), (0.0321953890134592, 'soul'), (0.03215959695464088, 'throw'), (0.0321529971947514, 'muy'), (0.03214629770184141, 'shut'), (0.032109875112155795, 'song'), (0.03204895284728604, 'young'), (0.032004644085872114, 'u'), (0.03191295945471384, 'nail'), (0.031845391788461797, 'everyday'), (0.03181809817564174, 'side'), (0.03180003327270842, 'move'), (0.03178012445345768, 'urban'), (0.031637395054780804, 'actually'), (0.03161043851074541, '5'), (0.031608457669948686, 'sport'), (0.03147965737667491, 'club'), (0.03139438081113499, 'they'), (0.03139366880384409, 'nlp'), (0.031138222495766366, 'build'), (0.031028862793720746, 'bird'), (0.03096608104072973, 'unless'), (0.03096014764663879, 'easier'), (0.030934259791803642, 'youtube'), (0.030902694135237718, 'provide'), (0.030902383773085296, 'urllink'), (0.030862319670521332, 'italian'), (0.030858093268268716, 'night'), (0.03067083910945878, 'respect'), (0.030667301088312282, 'impossible'), (0.030649621184796416, 'stat'), (0.030602701729982362, 'happiness'), (0.030595508630683943, '2)'), (0.030564362875854068, 'auto'), (0.03049559027418658, 'yesterday'), (0.030449072464511007, 'cake'), (0.030357387833352734, 'disney'), (0.030147453215574105, 'fun'), (0.030075104327384805, '2'), (0.029984131703517658, 'chat'), (0.029766330498874227, 'hang'), (0.029727736052385323, 'vote'), (0.029720451670100623, 'control'), (0.02965795933787163, '??'), (0.029560149618351295, 'finally'), (0.029538010809447846, 'write'), (0.029508583859548576, 'path'), (0.029336442404512608, 'bug'), (0.02933556608784671, 'simply'), (0.02932812652448602, 'single'), (0.0292813805073453, 'and'), (0.029140476090125045, 'reach'), (0.028986682515278916, 'v'), (0.028983022067539554, 'button'), (0.028903126601464102, 'go'), (0.028872049341434947, 'rid'), (0.028724663832207398, 'hope'), (0.028707666940210652, 'over'), (0.028697217073102577, 'partner'), (0.02853659349611104, 'bottom'), (0.02840450769360392, '????'), (0.028398610134443336, 'best'), (0.028317457553341585, 'international'), (0.028243958499528654, 'ma'), (0.028172164431023994, 'reward'), (0.028167089097001163, 'bust'), (0.028123108954336784, 'become'), (0.02812033395156166, 'beer'), (0.02802618467978113, 'pass'), (0.027992008329815077, 'tie'), (0.027955458622212825, '????????????????????'), (0.02779240895258428, 'wtf'), (0.027762743338655138, ':-)'), (0.027762237240911203, 'es'), (0.027757374542549895, 'ios'), (0.027576597717036133, 'di'), (0.027556424177125782, 'strike'), (0.027523288453200623, 'shot'), (0.02736014750058624, 'alcohol'), (0.027141323926499483, 'coursera'), (0.027109520934170073, 'sit'), (0.027088471077593823, 'fish'), (0.02707045181615353, 'nick'), (0.026989118675609713, 'from'), (0.02698228802346181, '!!!!'), (0.026908351489579374, 'doesn'), (0.026895444075356156, 'fry'), (0.02670808574655048, 'minute'), (0.026638208620752613, 'insane'), (0.026603521086064896, 'bite'), (0.026548477445494933, 'window'), (0.02653295933787181, '_'), (0.02653051295384623, 'due'), (0.02652686163440543, 'kiss'), (0.026481363509379463, 'touch'), (0.02638904258211272, 'don'), (0.02632222343634627, 'jump'), (0.02626927930445455, 'expectation'), (0.026264186392098487, 'nexus'), (0.026217065436451303, 'cream'), (0.026135969631671196, 'now'), (0.025971258611696912, '@'), (0.025937794269022207, 'ugh'), (0.025911632565229104, 'experience'), (0.025886694053448345, 'chance'), (0.025775292297309793, 'big'), (0.025642548579039692, 'keep'), (0.025548180228092576, 'creepy'), (0.025540092555530913, 'even'), (0.02545165759867518, 'some'), (0.025431757907722785, '!'), (0.02542458038022799, 'empower'), (0.025331711755044894, 'edit'), (0.025263523364488227, 'london'), (0.025242710843675464, 'hrs'), (0.025145704414432313, 'nice'), (0.025056016195874165, 'treatment'), (0.02502887132062548, 'corpus'), (0.02501092508557412, 'click'), (0.02496590431686907, 'smell'), (0.024943193109947526, 'mess'), (0.02489479487075963, 'ho'), (0.02486994764196515, 'asian'), (0.024845027386781515, 'serve'), (0.024814484099659362, 'deserve'), (0.024681649098403424, 'remember'), (0.024671507558656547, 'grade'), (0.024668595631402557, 'scientific'), (0.02466386717272684, 'daughter'), (0.024579594720033127, 'sick'), (0.024553899237732324, 'rt'), (0.024539722311739842, 'album'), (0.024512282646142358, 'headphone'), (0.02442034242262281, 'each'), (0.024420105086859323, 'exact'), (0.024419867751095836, 'city'), (0.024386969362934163, 'guide'), (0.024318525380016442, 'german'), (0.024283874358523416, 'improve'), (0.024215466888800163, 'total'), (0.024206429873184332, 'age'), (0.024192262753753857, 'between'), (0.024125334068404225, 'a'), (0.024115219913553032, 'mother'), (0.024085552943096555, 'random'), (0.023976287208830938, 'army'), (0.023946282491326132, 'band'), (0.023904776117583015, 'women'), (0.023843616516949773, 'game'), (0.023832936407585548, 'power'), (0.023814825863158973, 'struggle'), (0.023809202831220233, 'annoy'), (0.02372391917217298, 'tryna'), (0.02367970878725245, 'son'), (0.02364750414978478, 'alway'), (0.023361131166043325, 'hasn'), (0.023327785491250363, 'visit'), (0.02331381006609079, 'exactly'), (0.023295790804650496, 'bright'), (0.02329252287375083, 'te'), (0.023023548427144824, 'manager'), (0.023013105653544086, 'pull'), (0.022937158209175523, 'technical'), (0.02293496741751122, 'apparently'), (0.022648649203561355, 'future'), (0.022545791534914317, 'recognition'), (0.022528557307153863, 'long'), (0.022510026860991683, 'toy'), (0.02243767596627233, 'boyfriend'), (0.022433542881796775, 'line'), (0.022223014896348037, 'possible'), (0.022200632308176038, 'myself'), (0.0221517411408636, 'welcome'), (0.022149915481143312, 'shirt'), (0.022097781761297508, 'dat'), (0.02203767392153355, 'princess'), (0.02203124759931785, 'clue'), (0.021964172861190123, 'campaign'), (0.02195760048619677, 'shift'), (0.021901799196843097, '..\"'), (0.02183544559430528, 'rd'), (0.021799680920382647, 'player'), (0.021784966103036263, 'tan'), (0.021765431544028102, 'spread'), (0.02165846614100997, 'block'), (0.021425302009819402, 'accord'), (0.021404991545429963, 'court'), (0.021366305815955, 'solution'), (0.02127395481899863, 'not'), (0.02125912133377028, 'management'), (0.021115697506135955, 'lonely'), (0.021052876555157196, 'weak'), (0.020974227134402668, 'forest'), (0.020902074377458835, 'alone'), (0.020901127719197943, 'close'), (0.020852327834871343, 'something'), (0.020765700281138777, 'flash'), (0.02074702378219917, 'help'), (0.020711083664072927, '??????????'), (0.020585398127152432, 'together'), (0.020572344660151565, 'cheese'), (0.02043381360056773, 'instagram'), (0.020363963859665546, 'didn'), (0.020350271411762266, 'increase'), (0.020301015112506038, 'success'), (0.020261380039976462, ''), (0.020208093469713084, 'survey'), (0.02011072659984947, 'standard'), (0.020078759298145288, 'channel'), (0.020054749866291965, '!!'), (0.019807648829666125, 'price'), (0.019746416202644168, 'linkedin'), (0.019688923493255128, 'spell'), (0.019655918250304216, 'finish'), (0.019596493026405204, 'weekly'), (0.019549993473326754, 'computer'), (0.019503694742817546, 'thousand'), (0.01948266314283842, 'destroy'), (0.019431544670667433, 'radiohead'), (0.019343182740200193, 'decent'), (0.01932437844508006, 'bone'), (0.01930608533468181, 'regular'), (0.019194610552154323, 'nyc'), (0.019136627599434464, 'starbuck'), (0.01904508902105384, 'speed'), (0.018975239280151435, 'parent'), (0.018969944866962463, 'settle'), (0.01880711427650006, 'wrong'), (0.018760687749810545, '['), (0.018622722644739964, 'commercial'), (0.018566519710247587, 'to'), (0.018465113441078262, '11'), (0.01841100088696579, 'busy'), (0.01822082191389196, 'around'), (0.01814226377612327, 'suffer'), (0.018092204186590077, 'boys'), (0.018090761915410924, '#'), (0.017904998354066892, 'president'), (0.017788943850434835, 'germany'), (0.017763968825460053, 'scott'), (0.017719167135921365, 'retweet'), (0.017577240349258005, 'hill'), (0.017573844622177814, 'result'), (0.017568002511072933, 'course'), (0.01755912980483143, 'since'), (0.017536509880895723, 'available'), (0.017413478672338334, 'relationship'), (0.01734284389775631, '):'), (0.017316645680768517, 'tho'), (0.017180220573367988, 'mayor'), (0.017162779079533452, 'handle'), (0.01714965258614387, 'emotional'), (0.017144029554204687, 'return'), (0.01703003536126335, 'performance'), (0.017026949996335583, 'chicken'), (0.0170062105019122, 'apps'), (0.017005735830384783, ']'), (0.016922120615190606, 'space'), (0.01688564393397729, 'die'), (0.016875602805515255, 'useful'), (0.01686461233399794, 'antwerp'), (0.016625213574862574, 'java'), (0.016588499557885328, 'market'), (0.01654594342980298, 'less'), (0.016536760361409275, 'try'), (0.016535664965577013, 'tech'), (0.01647514434584596, 'flower'), (0.016467786937172768, 'apple'), (0.016347147342848922, ':'), (0.01628534876131349, 'yay'), (0.016265102195014203, 'aren'), (0.016249109415863883, 'three'), (0.01623436721362137, 'successful'), (0.016160583176021648, 'gay'), (0.01612379613265591, 'halloween'), (0.01611581799967743, 'open'), (0.015977761611621233, 'health'), (0.01596463511823143, 'mcdonald'), (0.015935771438051916, 'personality'), (0.015853698905321734, 'any'), (0.015836711141623327, 'chart'), (0.015834666402736675, 'expect'), (0.015826341394411525, 'est'), (0.01581003825310834, 'china'), (0.015765181793778282, 'pattern'), (0.01561945763489625, 'goal'), (0.015474135121152699, '?'), (0.01544313541910003, 'nope'), (0.015368831068480082, 'record'), (0.01535318516467643, 'alarm'), (0.015263499630911559, 'woman'), (0.015228419579384367, 'p'), (0.015213230090510566, 'maybe'), (0.01514013067530584, 'same'), (0.01511991149390246, 'sugar'), (0.015044959034081984, 'website'), (0.014970818992836143, 'brussel'), (0.014959426876181237, 'schedule'), (0.014949458774107693, 'app'), (0.014936786010854464, 'child'), (0.014936094944954403, 'special'), (0.014906920902622689, 'use'), (0.014836742542970516, 'receive'), (0.014808006658971529, 'flight'), (0.014724793088915833, '\".'), (0.01470215490838278, 'break'), (0.014624710423043696, 'hardest'), (0.014619963707770411, 'advance'), (0.014594258418907602, 'win'), (0.014528589438764783, 'steam'), (0.01441704162984836, 'machine'), (0.01434346754311644, 'interaction'), (0.01427328918346471, 'station'), (0.01416880667766618, 'must'), (0.01395643681069103, 'blood'), (0.0139327123626245, 'ted'), (0.013878216419970668, 'version'), (0.013839603716884419, 'hello'), (0.013786750867978759, 'nowplaying'), (0.0137536151440536, 'universe'), (0.013717494466485514, 'username'), (0.013711040759374127, 'fund'), (0.013611998719542662, 'trash'), (0.013590925236781626, '200'), (0.013585490140402223, '8'), (0.013569113972710323, 'beat'), (0.013523545506089363, 'asleep'), (0.013514122417138452, 'taste'), (0.013489296129734596, 'carry'), (0.013294206132012931, 'tear'), (0.013285342554070434, 'ocean'), (0.013267934888636423, 'doctor'), (0.013262330113295029, 'wind'), (0.013217382370978692, 'rock'), (0.01305646872322308, 'kill'), (0.013043506539208272, 'damn'), (0.012957499709780107, 'revolution'), (0.012877752208399595, 'their'), (0.012811063543607393, 'editor'), (0.01267737048228268, 'mirror'), (0.012658830907821939, 'strange'), (0.012561386320245838, 'confidence'), (0.012557844540388219, 'wednesday'), (0.012543348802208198, 'how'), (0.012505064717871495, 'shouldn'), (0.012445730776958763, 'w'), (0.012415534365183278, 'todo'), (0.012376802994215064, 'culture'), (0.012374530047863264, 'hug'), (0.0122994589201606, 'fear'), (0.012234501947308907, 'analytic'), (0.012145336726564704, 'fight'), (0.012139784036220957, 'rank'), (0.012129453486997122, 'bro'), (0.011978626609196574, 'beauty'), (0.011910721195896423, 'cell'), (0.011887371008072689, 'enter'), (0.011878735637595117, 'wolf'), (0.0117482739939756, 'talent'), (0.011709588264500637, 'leg'), (0.011667506807945482, 'th'), (0.01165428903156962, 'mile'), (0.011643864514566227, 'pleasure'), (0.011471065822030502, 'query'), (0.011424310676591443, 'important'), (0.01136735009331491, 'snow'), (0.01129092797741893, 'detail'), (0.01123373005837891, 'student'), (0.01117384841955027, 'intellectual'), (0.011154934584846954, 'engineer'), (0.01114496648277341, 'lot'), (0.011072506048470876, 'opportunity'), (0.01103816538913005, 'usual'), (0.010993235903411058, 'growth'), (0.010991026855149633, 'feel'), (0.010978210723912474, 'mucho'), (0.010944107400335357, 'good'), (0.010938411342007903, 'freak'), (0.01092844323993436, 'jack'), (0.010798383241453546, 'community'), (0.010766507222734978, 'calm'), (0.010750989115111853, 'ticket'), (0.010750532700181559, 'pa'), (0.010647958008791258, 'ass'), (0.010644753975981969, 'coldplay'), (0.010631134554467847, '33'), (0.010626908152215009, 'floor'), (0.01057844601493696, '//'), (0.010567857188558794, 'six'), (0.01053992459483677, 'hi'), (0.010379649927983126, 'professor'), (0.010348321607181132, 'describe'), (0.010307098210694532, 'hit'), (0.010216902170472109, 'those'), (0.010207964887876786, 'moment'), (0.01000338145960944, 'fav'), (0.009921263285386006, 'local'), (0.009915804562821817, 'true'), (0.009794525987596003, 'which'), (0.009793503618152677, '30'), (0.009722156836279527, 'strawberry'), (0.009649276500241166, 'lazy'), (0.009640422050597452, 'software'), (0.009638998036015423, 'regard'), (0.009608089616949256, 'ubuntu'), (0.009583625776695692, 'hand'), (0.009439709020937048, 'thing'), (0.009393519830011021, 'such'), (0.009373179295932177, 'suck'), (0.009346837710960454, 'snapchat'), (0.009330771905420754, 'next'), (0.009275490929087304, 'haha'), (0.009260374466602439, 'develop'), (0.00910652612196472, 'school'), (0.0090799080032411, 'journal'), (0.009058785120276136, 'toch'), (0.009020336726564437, 'reason'), (0.009011792639072924, 'preach'), (0.008916073299932714, 'debate'), (0.008832056439600278, 'tell'), (0.008819404617738114, 'h'), (0.008753972973359003, 'select'), (0.00874099253274685, 'generation'), (0.008722571626168074, 'car'), (0.008722407316793301, 'map'), (0.008696537718555275, 'isn'), (0.008647171879715643, 'insight'), (0.0085705306846533, 'yourself'), (0.008508166148604834, 'hq'), (0.008492191626051193, 'relate'), (0.008348439179667322, 'nation'), (0.008330803306768164, 'evidence'), (0.008272345682520887, 'follower'), (0.008269570679746208, 'st'), (0.008200743308286906, 'initiative'), (0.008166658241307356, 'wonderful'), (0.008025425205337555, 'vs'), (0.007985790132807757, 'im'), (0.00793405093633126, 'fill'), (0.007869258272854562, 'own'), (0.007820577056410372, 'vimeo'), (0.00781894309096054, 'publish'), (0.007772645038714998, 'key'), (0.0077492861182464345, 'code'), (0.007723004672653744, 'sing'), (0.0076957658296252784, 'skill'), (0.0074244910517715645, 'king'), (0.007351866308094257, 'brother'), (0.00734640758553029, ':/'), (0.007317708214725771, 'aka'), (0.007182171237083423, 'audio'), (0.007159240950995072, 'shoe'), (0.007135178755880434, 'recruit'), (0.007106698464242278, 'box'), (0.007061367333384805, 'winter'), (0.006996903288657652, 'awkward'), (0.006923949926230355, 'underestimate'), (0.006867317961704034, 'premier'), (0.006818664130155305, 'il'), (0.00681460203727724, 'afraid'), (0.006733606643781975, 'richard'), (0.006628466900484353, 'we'), (0.006621109491810939, 'remix'), (0.006575869643939747, 'felt'), (0.006537494276617206, 'hall'), (0.00652593785058686, 'graphic'), (0.006474600299249289, 'should'), (0.006466220521132771, 'korean'), (0.006353869421939384, 'sentiment'), (0.006349615634790862, 'currently'), (0.006335594568138481, 'una'), (0.006276899608127717, 'op'), (0.006231495450881308, 'thesi'), (0.006195493441194966, 'als'), (0.006171759864829873, 'delete'), (0.006163258734037669, 'phd'), (0.006154032708944879, 'almost'), (0.006086318989915407, 'semantic'), (0.006075237235412478, 'fool'), (0.005934497127567218, 'surround'), (0.00580650012457018, 'courtesy'), (0.0057852311888275665, ':-'), (0.005723268297917139, 'still'), (0.005722309826564187, 'sleep'), (0.005696613665999273, 'klout'), (0.0055230208114853685, 'developer'), (0.005414279921827969, 'lesson'), (0.005375808739931376, 'could'), (0.005360473198280147, 'legs'), (0.005188021381091312, 'blame'), (0.005095843821808499, 'fan'), (0.0050646615537841555, 'attempt'), (0.005059695759344951, 'flame'), (0.0050258297715313205, 'bill'), (0.005025738488545262, 'mee'), (0.004873222875503291, 'blue'), (0.004828644023842532, 'stop'), (0.004827025234542592, '?!'), (0.004738087542999647, 'method'), (0.004704550173936006, 'merry'), (0.00466733410053588, 'days'), (0.004626229371931023, 'urge'), (0.0046025688219546446, 'least'), (0.004524010684185953, 'hey'), (0.004521655583146877, 'spark'), (0.004452116204396894, 'size'), (0.0044188435559926464, 'wel'), (0.004377811853776503, 'problem'), (0.004367460363161824, 'post'), (0.004349568897902056, '21'), (0.004331859998614185, 'invent'), (0.004328938943061633, 'alright'), (0.004326236966675223, '17'), (0.004271412405271846, 'awesome'), (0.004179636491127514, 'mostly'), (0.0040930089373947265, 'american'), (0.004072525035332175, 'mean'), (0.0040360483541184156, 'include'), (0.00403188584995573, 'net'), (0.003934468647275535, 'ny'), (0.003910662044521729, 'audience'), (0.0037877221189501764, 'damage'), (0.0037856591234659565, 'racist'), (0.003783304022426881, 'honest'), (0.0037728429922287976, 'algorithm'), (0.0037591414160271786, 'connect'), (0.0037313092335899967, '!?'), (0.0037125597082614537, 'skype'), (0.0037018065725082927, 'innovation'), (0.0036682692034444297, 'twice'), (0.0035900214278283826, '?\"'), (0.0035774426323547104, 'think'), (0.0034942108057016696, 'sandwich'), (0.003423174385981387, 'ahead'), (0.003354109678758599, 'entire'), (0.003349362963485758, 'digital'), (0.003309800917344452, 'media'), (0.003231292179780354, 'cheat'), (0.0032089332177926977, \"',\"), (0.0031910782657273984, 'summer'), (0.0031723835101902242, 'impress'), (0.0031533966490979726, 'hot'), (0.0031231819807258088, 'jeremy'), (0.002981656839200708, 'ignore'), (0.0029518985857581725, 'maar'), (0.0029063301191372126, 'su'), (0.002893203625747409, 'gym'), (0.002872929674552438, 'give'), (0.0028147185143676534, 'book'), (0.0026350553412832767, 'drive'), (0.0025851418045277352, 'cm'), (0.0025185234813303037, 'ps'), (0.0024850591386553766, 'continue'), (0.0024841006673024246, 'confuse'), (0.0024613932190917787, 'flat'), (0.002386491770351329, 'collection'), (0.0023835707147987772, 'season'), (0.002357609833574692, 'she'), (0.002325235517144586, '.......'), (0.0022816623892061294, 'emotion'), (0.0022479607107677158, 'sale'), (0.002149229033088451, 'test'), (0.0021485170257975472, 'green'), (0.002106526852228452, 'miss'), (0.002106435569242393, 'le'), (0.0021015245445947794, 'dear'), (0.002083742618917972, 'fb'), (0.002056996704014047, 'engine'), (0.002031510494317157, 'along'), (0.0020310358227899616, 'los'), (0.0019815969575618375, 'pant'), (0.0018643530903177385, 'people'), (0.0018550760541446998, 'tonight'), (0.0017704411543006948, 'college'), (0.0017355527970437379, 'queen'), (0.0017023440467298645, 'tag'), (0.0016867475431303625, 'more'), (0.0016416317327281327, 'pretty'), (0.0016387198054739205, 'wont'), (0.0015015397340836234, 'trailer'), (0.0014908596247191763, 'room'), (0.0013448981300736573, 'project'), (0.0012293338697724199, 'everyone'), (0.0012132680642327198, 'forever'), (0.0011695070007349262, 'japan'), (0.0011013680103824086, 'charge'), (0.0011011543008032643, 'benefit'), (0.0010422220050290143, 'boom'), (0.0010237828418531159, 'voice'), (0.0009869957984869338, 'speak'), (0.0009414912299561262, '22'), (0.0009362059450654936, 'y'), (0.0008879537586554687, 'point'), (0.000883060990604756, 'professional'), (0.0008405596323139974, 'activity'), (0.0007309105095067991, 'tuesday'), (0.0006141413137903395, 'paul'), (0.0005609781027324789, 'honestly'), (0.0005560853346817662, '50'), (0.0005400195291425103, 'hunt'), (0.0004691291621992111, 'festival'), (0.00043369310702656705, 'greet'), (0.00042261135252341653, 'world'), (0.0003896399379732518, 'era'), (0.00031801017884336424, 'hold'), (0.0002612230332406096, 'several'), (0.0002555269749129341, 'john'), (0.00021985358397635935, 'door'), (0.00021236837912286255, 'type'), (0.00018021851144656154, 'liverpool'), (0.00011986220109005963, 'glass'), (9.01039476477461e-05, 'ah'), (2.5283899275141408e-05, 'nerd'), (1.4719772998716962e-05, 'change'), (0.0, 'hater')]\n",
      "2172\n",
      "3\n",
      "0.647887323943662\n",
      "0.6126760563380281\n",
      "0.5563380281690141\n",
      "0.6126760563380281\n",
      "0.6971830985915493\n",
      "0.6549295774647887\n",
      "0.6408450704225352\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6971830985915493\n",
      "0.6056338028169014\n",
      "0.5563380281690141\n",
      "0.647887323943662\n",
      "0.6830985915492958\n",
      "0.6830985915492958\n",
      "0.6549295774647887\n",
      "************************************ ****************************\n",
      "7\n",
      "0.6619718309859155\n",
      "0.6197183098591549\n",
      "0.5422535211267606\n",
      "0.6549295774647887\n",
      "0.6549295774647887\n",
      "0.6830985915492958\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "9\n",
      "0.6690140845070423\n",
      "0.6056338028169014\n",
      "0.528169014084507\n",
      "0.647887323943662\n",
      "0.6549295774647887\n",
      "0.6338028169014085\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6971830985915493\n",
      "0.5985915492957746\n",
      "0.5845070422535211\n",
      "0.6267605633802817\n",
      "0.6830985915492958\n",
      "0.676056338028169\n",
      "0.6619718309859155\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7112676056338029\n",
      "0.6126760563380281\n",
      "0.5492957746478874\n",
      "0.704225352112676\n",
      "0.704225352112676\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "************************************ ****************************\n",
      "15\n",
      "0.6901408450704225\n",
      "0.5845070422535211\n",
      "0.5633802816901409\n",
      "0.704225352112676\n",
      "0.7112676056338029\n",
      "0.6619718309859155\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "17\n",
      "0.6408450704225352\n",
      "0.5774647887323944\n",
      "0.5915492957746479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971830985915493\n",
      "0.704225352112676\n",
      "0.6690140845070423\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "19\n",
      "0.676056338028169\n",
      "0.5845070422535211\n",
      "0.6338028169014085\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.6830985915492958\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "0.6768388106416275\n",
      "0.6001564945226917\n",
      "0.5672926447574335\n",
      "0.6666666666666666\n",
      "0.690923317683881\n",
      "0.6697965571205008\n",
      "0.6807511737089202\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,21,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(mean(manhattanacc))\n",
    "print(mean(cosineacc))\n",
    "print(mean(euclideanacc))\n",
    "print(mean(braycurtisacc))\n",
    "print(mean(canberraacc))\n",
    "print(mean(jaccardacc))\n",
    "print(mean(yuleacc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.7605633802816901\n",
      "0.7746478873239436\n",
      "0.7464788732394366\n",
      "0.8028169014084507\n",
      "0.6690140845070423\n",
      "0.8169014084507042\n",
      "0.7816901408450704\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7464788732394366\n",
      "0.7535211267605634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704225352112676\n",
      "0.795774647887324\n",
      "0.647887323943662\n",
      "0.8028169014084507\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.6549295774647887\n",
      "0.7676056338028169\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7535211267605634\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7112676056338029\n",
      "0.6971830985915493\n",
      "0.6690140845070423\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "************************************ ****************************\n",
      "0.743\n",
      "0.7312\n",
      "0.7054\n",
      "0.73\n",
      "0.6526\n",
      "0.7617\n",
      "0.723\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "3\n",
      "0.7183098591549296\n",
      "0.6408450704225352\n",
      "0.6619718309859155\n",
      "0.6690140845070423\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6971830985915493\n",
      "0.6197183098591549\n",
      "0.647887323943662\n",
      "0.7394366197183099\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "7\n",
      "0.6971830985915493\n",
      "0.6549295774647887\n",
      "0.6267605633802817\n",
      "0.7253521126760564\n",
      "0.7676056338028169\n",
      "0.6830985915492958\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "9\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "0.6197183098591549\n",
      "0.7394366197183099\n",
      "0.7323943661971831\n",
      "0.704225352112676\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6619718309859155\n",
      "0.6267605633802817\n",
      "0.6338028169014085\n",
      "0.7253521126760564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7394366197183099\n",
      "0.6690140845070423\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6619718309859155\n",
      "0.5774647887323944\n",
      "0.6338028169014085\n",
      "0.7253521126760564\n",
      "0.7464788732394366\n",
      "0.6690140845070423\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "0.6854\n",
      "0.6256\n",
      "0.6373\n",
      "0.7207\n",
      "0.743\n",
      "0.6948\n",
      "0.7254\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
      "3\n",
      "0.7253521126760564\n",
      "0.6901408450704225\n",
      "0.7112676056338029\n",
      "0.7394366197183099\n",
      "0.6830985915492958\n",
      "0.7535211267605634\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7183098591549296\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7394366197183099\n",
      "0.6408450704225352\n",
      "0.7394366197183099\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7464788732394366\n",
      "0.7535211267605634\n",
      "0.676056338028169\n",
      "0.6971830985915493\n",
      "0.6549295774647887\n",
      "0.6901408450704225\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7605633802816901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112676056338029\n",
      "0.6830985915492958\n",
      "0.7323943661971831\n",
      "0.6690140845070423\n",
      "0.7323943661971831\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7323943661971831\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "0.7394366197183099\n",
      "0.6408450704225352\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7323943661971831\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.6549295774647887\n",
      "0.6830985915492958\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "0.7359\n",
      "0.7148\n",
      "0.7019\n",
      "0.7312\n",
      "0.6573\n",
      "0.7136\n",
      "0.7019\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
      "3\n",
      "0.7887323943661971\n",
      "0.7605633802816901\n",
      "0.7323943661971831\n",
      "0.795774647887324\n",
      "0.7112676056338029\n",
      "0.7676056338028169\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7887323943661971\n",
      "0.6971830985915493\n",
      "0.704225352112676\n",
      "0.7887323943661971\n",
      "0.704225352112676\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464788732394366\n",
      "0.7464788732394366\n",
      "0.7183098591549296\n",
      "0.7887323943661971\n",
      "0.6690140845070423\n",
      "0.7253521126760564\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7464788732394366\n",
      "0.6971830985915493\n",
      "0.7253521126760564\n",
      "0.7605633802816901\n",
      "0.6690140845070423\n",
      "0.7464788732394366\n",
      "0.7112676056338029\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7253521126760564\n",
      "0.704225352112676\n",
      "0.704225352112676\n",
      "0.7535211267605634\n",
      "0.6549295774647887\n",
      "0.7183098591549296\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "0.6901408450704225\n",
      "0.7605633802816901\n",
      "0.6549295774647887\n",
      "0.7323943661971831\n",
      "0.7253521126760564\n",
      "************************************ ****************************\n",
      "0.7547\n",
      "0.7218\n",
      "0.7124\n",
      "0.7746\n",
      "0.6772\n",
      "0.7371\n",
      "0.7124\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "3\n",
      "0.7464788732394366\n",
      "0.7464788732394366\n",
      "0.7394366197183099\n",
      "0.7746478873239436\n",
      "0.795774647887324\n",
      "0.7676056338028169\n",
      "0.795774647887324\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6830985915492958\n",
      "0.7323943661971831\n",
      "0.704225352112676\n",
      "0.7535211267605634\n",
      "0.7535211267605634\n",
      "0.7605633802816901\n",
      "0.7887323943661971\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7112676056338029\n",
      "0.7535211267605634\n",
      "0.704225352112676\n",
      "0.7676056338028169\n",
      "0.7746478873239436\n",
      "0.6971830985915493\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7112676056338029\n",
      "0.7183098591549296\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.7535211267605634\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "11\n",
      "0.704225352112676\n",
      "0.7253521126760564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971830985915493\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n",
      "0.6690140845070423\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7253521126760564\n",
      "0.704225352112676\n",
      "0.7183098591549296\n",
      "0.7605633802816901\n",
      "0.7253521126760564\n",
      "0.6338028169014085\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "0.7136\n",
      "0.73\n",
      "0.7113\n",
      "0.7547\n",
      "0.7547\n",
      "0.7066\n",
      "0.7336\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=300)\n",
      "3\n",
      "0.7183098591549296\n",
      "0.7535211267605634\n",
      "0.7323943661971831\n",
      "0.7535211267605634\n",
      "0.795774647887324\n",
      "0.8028169014084507\n",
      "0.823943661971831\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7112676056338029\n",
      "0.7112676056338029\n",
      "0.7887323943661971\n",
      "0.7605633802816901\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7112676056338029\n",
      "0.7464788732394366\n",
      "0.7394366197183099\n",
      "0.7394366197183099\n",
      "0.7746478873239436\n",
      "0.8028169014084507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7183098591549296\n",
      "0.6971830985915493\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7394366197183099\n",
      "0.7816901408450704\n",
      "0.8169014084507042\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7605633802816901\n",
      "0.7112676056338029\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.7605633802816901\n",
      "0.8098591549295775\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7464788732394366\n",
      "0.6971830985915493\n",
      "0.704225352112676\n",
      "0.7183098591549296\n",
      "0.7816901408450704\n",
      "0.7323943661971831\n",
      "0.7535211267605634\n",
      "************************************ ****************************\n",
      "0.7312\n",
      "0.723\n",
      "0.7171\n",
      "0.73\n",
      "0.7758\n",
      "0.7735\n",
      "0.7864\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "selector = SelectKBest(score_func=chi2, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=500)\n",
      "3\n",
      "0.7394366197183099\n",
      "0.7676056338028169\n",
      "0.7323943661971831\n",
      "0.7535211267605634\n",
      "0.795774647887324\n",
      "0.7816901408450704\n",
      "0.7887323943661971\n",
      "************************************ ****************************\n",
      "5\n",
      "0.6619718309859155\n",
      "0.7183098591549296\n",
      "0.6901408450704225\n",
      "0.704225352112676\n",
      "0.7816901408450704\n",
      "0.7887323943661971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8098591549295775\n",
      "************************************ ****************************\n",
      "7\n",
      "0.6619718309859155\n",
      "0.647887323943662\n",
      "0.6971830985915493\n",
      "0.6830985915492958\n",
      "0.7887323943661971\n",
      "0.7887323943661971\n",
      "0.7816901408450704\n",
      "************************************ ****************************\n",
      "9\n",
      "0.6901408450704225\n",
      "0.6408450704225352\n",
      "0.6690140845070423\n",
      "0.6901408450704225\n",
      "0.8098591549295775\n",
      "0.7464788732394366\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "11\n",
      "0.704225352112676\n",
      "0.647887323943662\n",
      "0.676056338028169\n",
      "0.6619718309859155\n",
      "0.795774647887324\n",
      "0.7887323943661971\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "13\n",
      "0.704225352112676\n",
      "0.6549295774647887\n",
      "0.6408450704225352\n",
      "0.6830985915492958\n",
      "0.7746478873239436\n",
      "0.7183098591549296\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "0.6937\n",
      "0.6796\n",
      "0.6843\n",
      "0.696\n",
      "0.7911\n",
      "0.7688\n",
      "0.7746\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=500)\")\n",
    "selector = SelectKBest(score_func=chi2, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv 11167\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv 11808\n",
      "length of features used = 22975\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.8309859154929577\n",
      "0.7887323943661971\n",
      "0.7535211267605634\n",
      "0.8309859154929577\n",
      "0.6126760563380281\n",
      "0.7394366197183099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6197183098591549\n",
      "************************************ ****************************\n",
      "5\n",
      "0.795774647887324\n",
      "0.7464788732394366\n",
      "0.704225352112676\n",
      "0.823943661971831\n",
      "0.647887323943662\n",
      "0.7464788732394366\n",
      "0.6197183098591549\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7887323943661971\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7394366197183099\n",
      "0.6408450704225352\n",
      "0.6971830985915493\n",
      "0.5845070422535211\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.6690140845070423\n",
      "0.7253521126760564\n",
      "0.6267605633802817\n",
      "0.676056338028169\n",
      "0.5845070422535211\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.5774647887323944\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "0.6619718309859155\n",
      "0.6690140845070423\n",
      "0.6267605633802817\n",
      "0.5492957746478874\n",
      "************************************ ****************************\n",
      "0.7465\n",
      "0.7324\n",
      "0.7113\n",
      "0.7523\n",
      "0.6397\n",
      "0.6901\n",
      "0.5892\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "mean length =  1424.9802631578948\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "0.7394366197183099\n",
      "0.7605633802816901\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7323943661971831\n",
      "0.7183098591549296\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7605633802816901\n",
      "0.7183098591549296\n",
      "0.7535211267605634\n",
      "0.8028169014084507\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7394366197183099\n",
      "0.7253521126760564\n",
      "0.7676056338028169\n",
      "0.7816901408450704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7183098591549296\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6830985915492958\n",
      "0.704225352112676\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7605633802816901\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "************************************ ****************************\n",
      "0.7254\n",
      "0.7277\n",
      "0.7523\n",
      "0.7805\n",
      "0.7347\n",
      "0.7124\n",
      "0.6925\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=300)\n",
      "3\n",
      "0.7676056338028169\n",
      "0.7112676056338029\n",
      "0.7464788732394366\n",
      "0.7464788732394366\n",
      "0.7676056338028169\n",
      "0.7323943661971831\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7676056338028169\n",
      "0.6971830985915493\n",
      "0.7676056338028169\n",
      "0.7816901408450704\n",
      "0.7816901408450704\n",
      "0.7183098591549296\n",
      "0.7253521126760564\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7112676056338029\n",
      "0.7746478873239436\n",
      "0.7253521126760564\n",
      "0.7605633802816901\n",
      "0.7253521126760564\n",
      "0.704225352112676\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464788732394366\n",
      "0.7605633802816901\n",
      "0.7746478873239436\n",
      "0.7535211267605634\n",
      "0.7816901408450704\n",
      "0.7183098591549296\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6971830985915493\n",
      "0.7394366197183099\n",
      "0.7464788732394366\n",
      "0.7746478873239436\n",
      "0.7887323943661971\n",
      "0.704225352112676\n",
      "0.7112676056338029\n",
      "************************************ ****************************\n",
      "13\n",
      "0.704225352112676\n",
      "0.7323943661971831\n",
      "0.7605633802816901\n",
      "0.7535211267605634\n",
      "0.7605633802816901\n",
      "0.7323943661971831\n",
      "0.7112676056338029\n",
      "************************************ ****************************\n",
      "0.7324\n",
      "0.7359\n",
      "0.7535\n",
      "0.7617\n",
      "0.7676\n",
      "0.7183\n",
      "0.7289\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=300)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=300)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=500)\n",
      "3\n",
      "0.7605633802816901\n",
      "0.7605633802816901\n",
      "0.7183098591549296\n",
      "0.7746478873239436\n",
      "0.704225352112676\n",
      "0.7323943661971831\n",
      "0.7746478873239436\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7676056338028169\n",
      "0.7394366197183099\n",
      "0.7605633802816901\n",
      "0.7676056338028169\n",
      "0.7253521126760564\n",
      "0.7816901408450704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7746478873239436\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7464788732394366\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.6971830985915493\n",
      "0.7676056338028169\n",
      "0.7535211267605634\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7253521126760564\n",
      "0.7676056338028169\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.6901408450704225\n",
      "0.7112676056338029\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6619718309859155\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7464788732394366\n",
      "0.6830985915492958\n",
      "0.704225352112676\n",
      "0.7394366197183099\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6408450704225352\n",
      "0.676056338028169\n",
      "0.7323943661971831\n",
      "0.7183098591549296\n",
      "0.6690140845070423\n",
      "0.704225352112676\n",
      "0.7183098591549296\n",
      "************************************ ****************************\n",
      "0.7171\n",
      "0.7371\n",
      "0.7324\n",
      "0.7535\n",
      "0.6948\n",
      "0.7336\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=500)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=500)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "0.7394366197183099\n",
      "0.7605633802816901\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "0.6830985915492958\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "5\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7323943661971831\n",
      "0.7183098591549296\n",
      "0.704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7605633802816901\n",
      "0.7183098591549296\n",
      "0.7535211267605634\n",
      "0.8028169014084507\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.6830985915492958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7394366197183099\n",
      "0.7253521126760564\n",
      "0.7676056338028169\n",
      "0.7816901408450704\n",
      "0.7183098591549296\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7816901408450704\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6830985915492958\n",
      "0.704225352112676\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7605633802816901\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "0.7254\n",
      "0.7277\n",
      "0.7523\n",
      "0.7805\n",
      "0.7347\n",
      "0.7124\n",
      "0.6925\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv 11167\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv 11808\n",
      "length of features used = 22975\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.8309859154929577\n",
      "0.7887323943661971\n",
      "0.7535211267605634\n",
      "0.8309859154929577\n",
      "0.6126760563380281\n",
      "0.7394366197183099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6197183098591549\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5704225352112676\n",
      "************************************ ****************************\n",
      "5\n",
      "0.795774647887324\n",
      "0.7464788732394366\n",
      "0.704225352112676\n",
      "0.823943661971831\n",
      "0.647887323943662\n",
      "0.7464788732394366\n",
      "0.6197183098591549\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7887323943661971\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7394366197183099\n",
      "0.6408450704225352\n",
      "0.6971830985915493\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6408450704225352\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.6690140845070423\n",
      "0.7253521126760564\n",
      "0.6267605633802817\n",
      "0.676056338028169\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "************************************ ****************************\n",
      "11\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.5774647887323944\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6267605633802817\n",
      "************************************ ****************************\n",
      "13\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "0.6619718309859155\n",
      "0.6690140845070423\n",
      "0.6267605633802817\n",
      "0.5492957746478874\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6338028169014085\n",
      "************************************ ****************************\n",
      "0.7465\n",
      "0.7324\n",
      "0.7113\n",
      "0.7523\n",
      "0.6397\n",
      "0.6901\n",
      "0.5892\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "0.7605633802816901\n",
      "0.7746478873239436\n",
      "0.7464788732394366\n",
      "0.8028169014084507\n",
      "0.6690140845070423\n",
      "0.8169014084507042\n",
      "0.7816901408450704\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.676056338028169\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464788732394366\n",
      "0.7535211267605634\n",
      "0.704225352112676\n",
      "0.795774647887324\n",
      "0.647887323943662\n",
      "0.8028169014084507\n",
      "0.7394366197183099\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "7\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.6549295774647887\n",
      "0.7676056338028169\n",
      "0.7464788732394366\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "9\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "11\n",
      "0.7535211267605634\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "13\n",
      "0.7112676056338029\n",
      "0.6971830985915493\n",
      "0.6690140845070423\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6549295774647887\n",
      "************************************ ****************************\n",
      "0.743\n",
      "0.7312\n",
      "0.7054\n",
      "0.73\n",
      "0.6526\n",
      "0.7617\n",
      "0.723\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M'\n",
      " 'M' 'M' 'M' 'M' 'F' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'F' 'M' 'M' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F'\n",
      " 'F' 'F' 'M' 'M' 'F' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F'\n",
      " 'F' 'F' 'M' 'M' 'F' 'F' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'F'\n",
      " 'M' 'F' 'F' 'M' 'M' 'F' 'M' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      "0.8028169014084507\n",
      "0.7605633802816901\n",
      "0.7746478873239436\n",
      "0.7464788732394366\n",
      "0.8028169014084507\n",
      "0.6690140845070423\n",
      "0.8169014084507042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816901408450704\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.676056338028169\n",
      "************************************ ****************************\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M'\n",
      " 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'F' 'F' 'M' 'F'\n",
      " 'F' 'M' 'F' 'F' 'M' 'F' 'M' 'M' 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'M'\n",
      " 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'M' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F']\n",
      "0.795774647887324\n",
      "0.7464788732394366\n",
      "0.7535211267605634\n",
      "0.704225352112676\n",
      "0.795774647887324\n",
      "0.647887323943662\n",
      "0.8028169014084507\n",
      "0.7394366197183099\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'F'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'F' 'F' 'F' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F'\n",
      " 'M' 'F' 'F' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'M'\n",
      " 'F' 'F' 'M' 'F' 'F' 'F' 'M' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      "0.7394366197183099\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.6549295774647887\n",
      "0.7676056338028169\n",
      "0.7464788732394366\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'M' 'F' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M'\n",
      " 'M' 'F' 'F' 'F' 'F' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'F' 'M' 'F' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'M' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      "0.7253521126760564\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'F' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'F' 'F' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M'\n",
      " 'M' 'F' 'F' 'F' 'F' 'M' 'F' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'M' 'F' 'M' 'F'\n",
      " 'F' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      "0.676056338028169\n",
      "0.7535211267605634\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "0.676056338028169\n",
      "0.6338028169014085\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours ['M' 'M' 'M' 'F' 'F' 'M' 'F' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'F' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M' 'M' 'F' 'M' 'M' 'M'\n",
      " 'M' 'F' 'F' 'F' 'M' 'M' 'F' 'F' 'M' 'M' 'M' 'M' 'M' 'F' 'M' 'F' 'M' 'M'\n",
      " 'M' 'F' 'F' 'F' 'F' 'M' 'F' 'M' 'M' 'M' 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'F'\n",
      " 'M' 'M' 'F' 'F' 'F' 'M' 'M' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'F'\n",
      " 'F' 'F' 'M' 'F' 'F' 'M' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'M' 'M' 'F'\n",
      " 'F' 'F' 'F' 'M' 'F' 'M' 'F' 'M' 'F' 'F' 'F' 'F' 'F' 'M' 'F' 'F' 'M' 'M'\n",
      " 'M' 'F' 'M' 'F' 'M' 'F' 'M' 'M' 'M' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      "0.6408450704225352\n",
      "0.7112676056338029\n",
      "0.6971830985915493\n",
      "0.6690140845070423\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.7253521126760564\n",
      "0.6549295774647887\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6549295774647887\n",
      "************************************ ****************************\n",
      "0.7365\n",
      "0.7312\n",
      "0.7054\n",
      "0.73\n",
      "0.6526\n",
      "0.7617\n",
      "0.723\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv 11167\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv 11808\n",
      "length of features used = 22975\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.8309859154929577\n",
      "0.8309859154929577\n",
      "0.7887323943661971\n",
      "0.7535211267605634\n",
      "0.8309859154929577\n",
      "0.6126760563380281\n",
      "0.7394366197183099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6197183098591549\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5704225352112676\n",
      "************************************ ****************************\n",
      "5\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.823943661971831\n",
      "0.795774647887324\n",
      "0.7464788732394366\n",
      "0.704225352112676\n",
      "0.823943661971831\n",
      "0.647887323943662\n",
      "0.7464788732394366\n",
      "0.6197183098591549\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7394366197183099\n",
      "0.7887323943661971\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7394366197183099\n",
      "0.6408450704225352\n",
      "0.6971830985915493\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6408450704225352\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7253521126760564\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.6690140845070423\n",
      "0.7253521126760564\n",
      "0.6267605633802817\n",
      "0.676056338028169\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7323943661971831\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.6408450704225352\n",
      "0.6549295774647887\n",
      "0.5774647887323944\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6267605633802817\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6619718309859155\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "0.6619718309859155\n",
      "0.6690140845070423\n",
      "0.6267605633802817\n",
      "0.5492957746478874\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6338028169014085\n",
      "************************************ ****************************\n",
      "0.7465\n",
      "0.7324\n",
      "0.7113\n",
      "0.7523\n",
      "0.6397\n",
      "0.6901\n",
      "0.5892\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "##print(\"selector = SelectKBest(score_func=chi2, k=300)\")\n",
    "##selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train =train_selected_features \n",
    "##X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "    \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.823943661971831\n",
      "0.7394366197183099\n",
      "0.7605633802816901\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "0.6830985915492958\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7323943661971831\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.823943661971831\n",
      "0.7464788732394366\n",
      "0.7323943661971831\n",
      "0.7605633802816901\n",
      "0.823943661971831\n",
      "0.7323943661971831\n",
      "0.7183098591549296\n",
      "0.704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.6830985915492958\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.8028169014084507\n",
      "0.7605633802816901\n",
      "0.7183098591549296\n",
      "0.7535211267605634\n",
      "0.8028169014084507\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.6830985915492958\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7816901408450704\n",
      "0.7394366197183099\n",
      "0.7253521126760564\n",
      "0.7676056338028169\n",
      "0.7816901408450704\n",
      "0.7183098591549296\n",
      "0.7112676056338029\n",
      "0.6901408450704225\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7323943661971831\n",
      "0.6830985915492958\n",
      "0.7253521126760564\n",
      "0.7323943661971831\n",
      "0.7323943661971831\n",
      "0.7464788732394366\n",
      "0.7253521126760564\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7816901408450704\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7183098591549296\n",
      "0.6830985915492958\n",
      "0.704225352112676\n",
      "0.7394366197183099\n",
      "0.7183098591549296\n",
      "0.7605633802816901\n",
      "0.704225352112676\n",
      "0.6971830985915493\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.7676056338028169\n",
      "************************************ ****************************\n",
      "0.7254\n",
      "0.7277\n",
      "0.7523\n",
      "0.7805\n",
      "0.7347\n",
      "0.7124\n",
      "0.6925\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt 200\n",
      "length of features used = 1309\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6267605633802817\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.7183098591549296\n",
      "0.5774647887323944\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "0.5633802816901409\n",
      "0.704225352112676\n",
      "0.6690140845070423\n",
      "0.7605633802816901\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "0.528169014084507\n",
      "0.7183098591549296\n",
      "0.6408450704225352\n",
      "0.7605633802816901\n",
      "0.5140845070422535\n",
      "0.6267605633802817\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "0.5070422535211268\n",
      "0.6549295774647887\n",
      "0.6549295774647887\n",
      "0.6901408450704225\n",
      "0.5352112676056338\n",
      "0.6338028169014085\n",
      "0.528169014084507\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "0.5070422535211268\n",
      "0.647887323943662\n",
      "0.6197183098591549\n",
      "0.6690140845070423\n",
      "0.5422535211267606\n",
      "0.6197183098591549\n",
      "0.5140845070422535\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6408450704225352\n",
      "0.5070422535211268\n",
      "0.6056338028169014\n",
      "0.6056338028169014\n",
      "0.6408450704225352\n",
      "0.5140845070422535\n",
      "0.6126760563380281\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5140845070422535\n",
      "************************************ ****************************\n",
      "0.5399\n",
      "0.6761\n",
      "0.6514\n",
      "0.716\n",
      "0.5246\n",
      "0.6467\n",
      "0.5411\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt 200\n",
      "length of features used = 1309\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6267605633802817\n",
      "0.7253521126760564\n",
      "0.7183098591549296\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.7183098591549296\n",
      "0.5774647887323944\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "0.5633802816901409\n",
      "0.704225352112676\n",
      "0.6690140845070423\n",
      "0.7605633802816901\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7605633802816901\n",
      "0.528169014084507\n",
      "0.7183098591549296\n",
      "0.6408450704225352\n",
      "0.7605633802816901\n",
      "0.5140845070422535\n",
      "0.6267605633802817\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "0.5070422535211268\n",
      "0.6549295774647887\n",
      "0.6549295774647887\n",
      "0.6901408450704225\n",
      "0.5352112676056338\n",
      "0.6338028169014085\n",
      "0.528169014084507\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6690140845070423\n",
      "0.5070422535211268\n",
      "0.647887323943662\n",
      "0.6197183098591549\n",
      "0.6690140845070423\n",
      "0.5422535211267606\n",
      "0.6197183098591549\n",
      "0.5140845070422535\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6408450704225352\n",
      "0.5070422535211268\n",
      "0.6056338028169014\n",
      "0.6056338028169014\n",
      "0.6408450704225352\n",
      "0.5140845070422535\n",
      "0.6126760563380281\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5140845070422535\n",
      "************************************ ****************************\n",
      "0.5399\n",
      "0.6761\n",
      "0.6514\n",
      "0.716\n",
      "0.5246\n",
      "0.6467\n",
      "0.5411\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt 200\n",
      "length of features used = 1309\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6197183098591549\n",
      "0.7112676056338029\n",
      "0.7183098591549296\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.704225352112676\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7535211267605634\n",
      "0.5633802816901409\n",
      "0.6901408450704225\n",
      "0.6901408450704225\n",
      "0.7535211267605634\n",
      "0.5140845070422535\n",
      "0.6830985915492958\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "0.5211267605633803\n",
      "0.7112676056338029\n",
      "0.6338028169014085\n",
      "0.7464788732394366\n",
      "0.5140845070422535\n",
      "0.6338028169014085\n",
      "0.5422535211267606\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.704225352112676\n",
      "0.5070422535211268\n",
      "0.6549295774647887\n",
      "0.647887323943662\n",
      "0.704225352112676\n",
      "0.5352112676056338\n",
      "0.647887323943662\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6830985915492958\n",
      "0.5070422535211268\n",
      "0.6338028169014085\n",
      "0.6197183098591549\n",
      "0.6830985915492958\n",
      "0.5422535211267606\n",
      "0.6267605633802817\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6619718309859155\n",
      "0.5070422535211268\n",
      "0.5915492957746479\n",
      "0.6056338028169014\n",
      "0.6619718309859155\n",
      "0.5140845070422535\n",
      "0.6056338028169014\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "0.5376\n",
      "0.6655\n",
      "0.6526\n",
      "0.7207\n",
      "0.5246\n",
      "0.6502\n",
      "0.5458\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt 100\n",
      "length of features used = 1209\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6408450704225352\n",
      "0.7112676056338029\n",
      "0.7183098591549296\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.7112676056338029\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7676056338028169\n",
      "0.5563380281690141\n",
      "0.704225352112676\n",
      "0.7112676056338029\n",
      "0.7676056338028169\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6971830985915493\n",
      "0.5211267605633803\n",
      "0.6690140845070423\n",
      "0.6549295774647887\n",
      "0.6971830985915493\n",
      "0.5140845070422535\n",
      "0.6549295774647887\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6619718309859155\n",
      "0.5140845070422535\n",
      "0.6830985915492958\n",
      "0.6267605633802817\n",
      "0.6619718309859155\n",
      "0.5422535211267606\n",
      "0.6408450704225352\n",
      "0.528169014084507\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "0.5070422535211268\n",
      "0.6267605633802817\n",
      "0.5985915492957746\n",
      "0.647887323943662\n",
      "0.5422535211267606\n",
      "0.6197183098591549\n",
      "0.5140845070422535\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5915492957746479\n",
      "0.5\n",
      "0.6126760563380281\n",
      "0.5845070422535211\n",
      "0.5915492957746479\n",
      "0.5140845070422535\n",
      "0.6197183098591549\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5140845070422535\n",
      "************************************ ****************************\n",
      "0.5399\n",
      "0.6678\n",
      "0.6491\n",
      "0.6901\n",
      "0.5258\n",
      "0.6526\n",
      "0.5399\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt 100\n",
      "length of features used = 1209\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.704225352112676\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "0.5492957746478874\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7464788732394366\n",
      "0.5140845070422535\n",
      "0.676056338028169\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.6338028169014085\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6619718309859155\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6549295774647887\n",
      "0.5070422535211268\n",
      "0.6901408450704225\n",
      "0.6267605633802817\n",
      "0.6549295774647887\n",
      "0.5352112676056338\n",
      "0.6338028169014085\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "0.5\n",
      "0.6408450704225352\n",
      "0.6056338028169014\n",
      "0.647887323943662\n",
      "0.5352112676056338\n",
      "0.6197183098591549\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5915492957746479\n",
      "0.5070422535211268\n",
      "0.6056338028169014\n",
      "0.5985915492957746\n",
      "0.5915492957746479\n",
      "0.5140845070422535\n",
      "0.6267605633802817\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "0.534\n",
      "0.6678\n",
      "0.6502\n",
      "0.6843\n",
      "0.5235\n",
      "0.6538\n",
      "0.5423\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt 100\n",
      "length of features used = 1209\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.704225352112676\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "0.5492957746478874\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7464788732394366\n",
      "0.5140845070422535\n",
      "0.676056338028169\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.6338028169014085\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6619718309859155\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6549295774647887\n",
      "0.5070422535211268\n",
      "0.6901408450704225\n",
      "0.6267605633802817\n",
      "0.6549295774647887\n",
      "0.5352112676056338\n",
      "0.6338028169014085\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "0.5\n",
      "0.6408450704225352\n",
      "0.6056338028169014\n",
      "0.647887323943662\n",
      "0.5352112676056338\n",
      "0.6197183098591549\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5915492957746479\n",
      "0.5070422535211268\n",
      "0.6056338028169014\n",
      "0.5985915492957746\n",
      "0.5915492957746479\n",
      "0.5140845070422535\n",
      "0.6267605633802817\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "0.534\n",
      "0.6678\n",
      "0.6502\n",
      "0.6843\n",
      "0.5235\n",
      "0.6538\n",
      "0.5423\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt', '/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt']\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt 100\n",
      "/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt 100\n",
      "length of features used = 1209\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "3\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7746478873239436\n",
      "0.6267605633802817\n",
      "0.704225352112676\n",
      "0.7253521126760564\n",
      "0.7746478873239436\n",
      "0.528169014084507\n",
      "0.704225352112676\n",
      "0.5845070422535211\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype float64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "0.5492957746478874\n",
      "0.6971830985915493\n",
      "0.7112676056338029\n",
      "0.7464788732394366\n",
      "0.5140845070422535\n",
      "0.676056338028169\n",
      "0.5704225352112676\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "7\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6690140845070423\n",
      "0.6338028169014085\n",
      "0.6901408450704225\n",
      "0.5140845070422535\n",
      "0.6619718309859155\n",
      "0.5352112676056338\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "9\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.6549295774647887\n",
      "0.5070422535211268\n",
      "0.6901408450704225\n",
      "0.6267605633802817\n",
      "0.6549295774647887\n",
      "0.5352112676056338\n",
      "0.6338028169014085\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "11\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.647887323943662\n",
      "0.5\n",
      "0.6408450704225352\n",
      "0.6056338028169014\n",
      "0.647887323943662\n",
      "0.5352112676056338\n",
      "0.6197183098591549\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.528169014084507\n",
      "************************************ ****************************\n",
      "13\n",
      "tanimoto_distance K-Nearest Neighbours\n",
      "0.5915492957746479\n",
      "0.5070422535211268\n",
      "0.6056338028169014\n",
      "0.5985915492957746\n",
      "0.5915492957746479\n",
      "0.5140845070422535\n",
      "0.6267605633802817\n",
      "0.5211267605633803\n",
      "rogerstanimoto K-Nearest Neighbours\n",
      "0.5211267605633803\n",
      "************************************ ****************************\n",
      "0.534\n",
      "0.6678\n",
      "0.6502\n",
      "0.6843\n",
      "0.5235\n",
      "0.6538\n",
      "0.5423\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015CHI.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015IG.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015GSS.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015OR.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015PMI.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015RF.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_TermSelection/2015TermSelection/outputNewVersion2015WLLR.2F.txt\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "#        print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        word_list =  ([(r.split(\":\"))[1] for r in f])[0:100]\n",
    "        print(txt_file, len(word_list))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "###def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "#selector = SelectKBest(score_func=f_classif, k=200)\n",
    "#fit = selector.fit(X_train,y_train)\n",
    "#train_selected_features = fit.transform(X_train)\n",
    "##x =(dict(zip(selector.scores_, word_list)))\n",
    "##sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "##print(sorted_x)\n",
    "##df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "##df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "##print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "#test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "#X_train =train_selected_features \n",
    "#X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "def tanimoto_distance(x,y):\n",
    "    \"\"\"Calculates the tanimoto (normalised manhattan) distance between the vectors x and y\n",
    "            Keyword arguments:\n",
    "                x,y -- the vectors between which the distance is to be calculated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument x is not iterable. None is returned')\n",
    "        return None\n",
    "    try:\n",
    "        iter(y)\n",
    "    except TypeError:\n",
    "        logging.warning( 'Argument y is not iterable. None is returned')\n",
    "        return None\n",
    "    numerator = sum(abs(a - b) for a, b in zip(x, y))\n",
    "    denominator = sum(max(a,b) for a,b in zip(x,y))\n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(3,15,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "    \n",
    "        \"\"\"\n",
    "        8.0. tanimoto_distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= tanimoto_distance)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"tanimoto_distance K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"manhattan K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        manhattanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. cosine K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"cosine\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"cosine K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        cosineacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        8. euclidean K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"euclidean\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"euclidean K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        euclideanacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"canberra\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"canberra K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        canberraacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"jaccard\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"jaccard K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        jaccardacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"yule\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"yule K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        yuleacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"rogerstanimoto\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        print(\"rogerstanimoto K-Nearest Neighbours\")\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"************************************ ****************************\")\n",
    "print(round(mean(manhattanacc), 4))\n",
    "print(round(mean(cosineacc), 4))\n",
    "print(round(mean(euclideanacc), 4))\n",
    "print(round(mean(braycurtisacc), 4))\n",
    "print(round(mean(canberraacc), 4))\n",
    "print(round(mean(jaccardacc), 4))\n",
    "print(round(mean(yuleacc), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=chi2, k=200)\n",
      "5\n",
      "0.7535211267605634\n",
      "[('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F')]\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2.f_classif.mutual_info_classif...........................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=chi2, k=200)\")\n",
    "selector = SelectKBest(score_func=chi2, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "#x =(dict(zip(selector.scores_, word_list)))\n",
    "#sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "#print(sorted_x)\n",
    "#df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "#df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "#print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(5,6,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "       \n",
    "  \n",
    "        print(\"************************************ ****************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=f_classif, k=200)\n",
      "5\n",
      "0.823943661971831\n",
      "[('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F')]\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2.f_classif.mutual_info_classif...........................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=f_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=f_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "#x =(dict(zip(selector.scores_, word_list)))\n",
    "#sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "#print(sorted_x)\n",
    "#df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "#df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "#print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(5,6,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "       \n",
    "  \n",
    "        print(\"************************************ ****************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
      "5\n",
      "0.5985915492957746\n",
      "[('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'M'), ('F', 'M'), ('F', 'M'), ('F', 'M'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F'), ('M', 'F'), ('M', 'F'), ('F', 'F'), ('F', 'F'), ('F', 'F')]\n",
      "************************************ ****************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using chi2.f_classif.mutual_info_classif...........................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##\n",
    "####def select_Bestfeatures(X, y):\n",
    "print(\"selector = SelectKBest(score_func=mutual_info_classif, k=200)\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=200)\n",
    "fit = selector.fit(X_train,y_train)\n",
    "train_selected_features = fit.transform(X_train)\n",
    "#x =(dict(zip(selector.scores_, word_list)))\n",
    "#sorted_x = sorted(x.items(), key=operator.itemgetter(0), reverse=True)\n",
    "#print(sorted_x)\n",
    "#df = pd.DataFrame({'sorted_tokens' : sorted_x})\n",
    "#df.to_csv('/Users/catherine/Desktop/NLP/PAN Datasets/PAN2015/2015txtfiles/2015_tweet_mutual_info_classif.csv', index=False, encoding='utf-8')\n",
    "#print([for x in list(sorted_x)])\n",
    "#print(len(fit.scores_))\n",
    "\n",
    "#print(selector.scores_)\n",
    "test_selected_features = fit.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using chi2............................................................\n",
    "\"\"\"\n",
    "##X_train = select_Bestfeatures(X_train, y_train)\n",
    "##X_test = select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using f_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def f_classif_select_Bestfeatures(X, y):\n",
    "##selector=SelectKBest(score_func=f_classif,k=500)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = f_classif_select_Bestfeatures(X_train, y_train)\n",
    "##X_test = f_classif_select_Bestfeatures(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using mutual_info_classif............................................................\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import *\n",
    "##def mutual_info_classif_select_Bestfeatures(X_tr, X_te, y):\n",
    "##selector=SelectKBest(score_func=mutual_info_classif,k=100)\n",
    "##fit = selector.fit(X_train,y_train)\n",
    "##train_selected_features = fit.transform(X_train)\n",
    "##test_selected_features = fit.transform(X_test)\n",
    "##    return train_selected_features, test_selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "X_train =train_selected_features \n",
    "X_test = test_selected_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Perform feature selection using pca............................................................\n",
    "\"\"\"\n",
    "\n",
    "##from sklearn.decomposition import PCA\n",
    "##def PCA_select_Bestfeatures(X):\n",
    "##    # feature extraction\n",
    "##    pca = PCA(n_components=200)\n",
    "##    fit = pca.fit(X, y)\n",
    "##    Train_selected_features = fit.transform(X)\n",
    "##    return selected_features\n",
    "\n",
    "\"\"\"\n",
    "x_train and x_test based on the new features using f_classif............................................................\n",
    "\"\"\"\n",
    "##X_train = PCA_select_Bestfeatures(X_train)\n",
    "##X_test = PCA_select_Bestfeatures(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Below are a list of Classification Algorithms with different distance measres ..............................................................................\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "K-Nearest Neighbours (average aa over all the nearest neighbour selectiopn)..............................................................................\n",
    "\"\"\"\n",
    "manhattanacc=[]\n",
    "cosineacc=[]\n",
    "euclideanacc=[]\n",
    "braycurtisacc=[]\n",
    "canberraacc=[]\n",
    "jaccardacc=[]\n",
    "yuleacc=[]\n",
    "from statistics import mean\n",
    "for k in range(5,6,2):\n",
    "##        acc=[]\n",
    "        print(k)\n",
    "        \"\"\"\n",
    "        8. manhattan K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        9. scipy.spatial.distance K-Nearest Neighbours..............................................................................\n",
    "        \"\"\"\n",
    "        from scipy.spatial.distance import *\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric= \"braycurtis\")\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        #print(\"braycurtis K-Nearest Neighbours\",y_pred)\n",
    "        print(metrics.accuracy_score(y_test, y_pred))\n",
    "        print([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "        braycurtisacc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        \n",
    "       \n",
    "  \n",
    "        print(\"************************************ ****************************\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "************************************ ****************************\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7112676056338029\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.6338028169014085\n",
      "SVM****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5915492957746479\n",
      "RandomForest_100****************************************************************\n",
      "0.7746478873239436\n",
      "RandomForest_200****************************************************************\n",
      "0.7535211267605634\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv 11167\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv 11808\n",
      "length of features used = 22975\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "************************************ ****************************\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.795774647887324\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6267605633802817\n",
      "****************************************************************\n",
      "Naive****************************************************************\n",
      "0.6408450704225352\n",
      "SVM****************************************************************\n",
      "0.5915492957746479\n",
      "RandomForest_100****************************************************************\n",
      "0.7816901408450704\n",
      "RandomForest_200****************************************************************\n",
      "0.7535211267605634\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv 11167\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv 11808\n",
      "length of features used = 22975\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "************************************ ****************************\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.795774647887324\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6267605633802817\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8098591549295775\n",
      "LogisticRegression****************************************************************\n",
      "0.5774647887323944\n",
      "SGDClassifier****************************************************************\n",
      "0.5985915492957746\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6197183098591549\n",
      "Naive****************************************************************\n",
      "0.6408450704225352\n",
      "SVM****************************************************************\n",
      "0.5915492957746479\n",
      "RandomForest_100****************************************************************\n",
      "0.7535211267605634\n",
      "RandomForest_200****************************************************************\n",
      "0.7746478873239436\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7605633802816901\n",
      "BaggingClassifier****************************************************************\n",
      "0.676056338028169\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7464788732394366\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivevoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 21 negative = 19\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 32 negative = 6\n",
      "MLPClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "LogisticRegression****************************************************************\n",
      "positive= 40 negative = 7\n",
      "SGDClassifier****************************************************************\n",
      "positive= 39 negative = 9\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 39 negative = 12\n",
      "Naive****************************************************************\n",
      "positive= 35 negative = 11\n",
      "SVM****************************************************************\n",
      "positive= 39 negative = 8\n",
      "RandomForest_100****************************************************************\n",
      "positive= 20 negative = 12\n",
      "RandomForest_200****************************************************************\n",
      "positive= 20 negative = 15\n",
      "XGBClassifier****************************************************************\n",
      "positive= 20 negative = 13\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 30 negative = 11\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 23 negative = 14\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k5voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k13voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= MLPClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= LogisticRegressionvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SGDClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= DecisionTreeClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= Naivevoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SVMvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_100voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_200voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= XGBClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= BaggingClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc\n",
    "y= AdaBoostClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "************************************ ****************************\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7464788732394366\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.7112676056338029\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7394366197183099\n",
      "LogisticRegression****************************************************************\n",
      "0.5704225352112676\n",
      "SGDClassifier****************************************************************\n",
      "0.5\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6126760563380281\n",
      "Naive****************************************************************\n",
      "0.6338028169014085\n",
      "SVM****************************************************************\n",
      "0.5915492957746479\n",
      "RandomForest_100****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816901408450704\n",
      "RandomForest_200****************************************************************\n",
      "0.7887323943661971\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7676056338028169\n",
      "BaggingClassifier****************************************************************\n",
      "0.6267605633802817\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7464788732394366\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivefs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 24 negative = 18\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 26 negative = 15\n",
      "MLPClassifier****************************************************************\n",
      "positive= 16 negative = 9\n",
      "LogisticRegression****************************************************************\n",
      "positive= 39 negative = 8\n",
      "SGDClassifier****************************************************************\n",
      "positive= 54 negative = 13\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 37 negative = 12\n",
      "Naive****************************************************************\n",
      "positive= 33 negative = 11\n",
      "SVM****************************************************************\n",
      "positive= 36 negative = 8\n",
      "RandomForest_100****************************************************************\n",
      "positive= 12 negative = 11\n",
      "RandomForest_200****************************************************************\n",
      "positive= 0 negative = 0\n",
      "XGBClassifier****************************************************************\n",
      "positive= 17 negative = 14\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 33 negative = 10\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 17 negative = 11\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= RandomForest_200fs \n",
    "y= manhattan_k5fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= RandomForest_200fs \n",
    "y= manhattan_k13fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= MLPClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= RandomForest_200fs\n",
    "y= LogisticRegressionfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= SGDClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= DecisionTreeClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= Naivefs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= SVMfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= RandomForest_100fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= RandomForest_200fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= XGBClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= RandomForest_200fs \n",
    "y= BaggingClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= RandomForest_200fs\n",
    "y= AdaBoostClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18373 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\\ntexts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\\nCLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :Tokenize and Prepare Vocabulary\n",
    "num_labels = 2\n",
    "#vocab_size = 15000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 136 samples, validate on 16 samples\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.7409 - acc: 0.5809 - val_loss: 3.0338 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.5575 - acc: 0.7500 - val_loss: 0.5191 - val_acc: 0.6875\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 0s 149us/step - loss: 0.2554 - acc: 0.8897 - val_loss: 0.5223 - val_acc: 0.6875\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 0s 145us/step - loss: 0.0716 - acc: 0.9926 - val_loss: 1.4101 - val_acc: 0.3750\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 0s 140us/step - loss: 0.0580 - acc: 1.0000 - val_loss: 1.7278 - val_acc: 0.2500\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 0s 134us/step - loss: 0.0523 - acc: 1.0000 - val_loss: 0.9719 - val_acc: 0.6250\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.5116 - val_acc: 0.7500\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 0s 151us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.3680 - val_acc: 0.7500\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.4038 - val_acc: 0.7500\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 0s 132us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.7500\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 0s 127us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.6875\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 0s 156us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9589 - val_acc: 0.6875\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.1834 - val_acc: 0.6875\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 0s 133us/step - loss: 8.9102e-04 - acc: 1.0000 - val_loss: 1.3921 - val_acc: 0.6250\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 0s 136us/step - loss: 8.0262e-04 - acc: 1.0000 - val_loss: 1.5676 - val_acc: 0.5625\n",
      "Epoch 16/30\n",
      "136/136 [==============================] - 0s 131us/step - loss: 7.2517e-04 - acc: 1.0000 - val_loss: 1.6957 - val_acc: 0.5625\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 6.3044e-04 - acc: 1.0000 - val_loss: 1.7837 - val_acc: 0.5625\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 0s 164us/step - loss: 5.3875e-04 - acc: 1.0000 - val_loss: 1.8261 - val_acc: 0.5625\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 0s 135us/step - loss: 4.5442e-04 - acc: 1.0000 - val_loss: 1.8348 - val_acc: 0.5625\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 3.7795e-04 - acc: 1.0000 - val_loss: 1.8235 - val_acc: 0.5625\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 0s 149us/step - loss: 3.0724e-04 - acc: 1.0000 - val_loss: 1.8024 - val_acc: 0.5625\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 0s 165us/step - loss: 2.5879e-04 - acc: 1.0000 - val_loss: 1.7751 - val_acc: 0.5625\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 0s 137us/step - loss: 2.1830e-04 - acc: 1.0000 - val_loss: 1.7443 - val_acc: 0.5625\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 0s 135us/step - loss: 1.8788e-04 - acc: 1.0000 - val_loss: 1.7122 - val_acc: 0.5625\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 0s 152us/step - loss: 1.6815e-04 - acc: 1.0000 - val_loss: 1.6813 - val_acc: 0.5625\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 0s 155us/step - loss: 1.4999e-04 - acc: 1.0000 - val_loss: 1.6535 - val_acc: 0.5625\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 0s 154us/step - loss: 1.3793e-04 - acc: 1.0000 - val_loss: 1.6295 - val_acc: 0.5625\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 0s 133us/step - loss: 1.2763e-04 - acc: 1.0000 - val_loss: 1.6127 - val_acc: 0.6250\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 0s 140us/step - loss: 1.1930e-04 - acc: 1.0000 - val_loss: 1.6036 - val_acc: 0.6250\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 0s 133us/step - loss: 1.1203e-04 - acc: 1.0000 - val_loss: 1.6031 - val_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "#epochs... number of epochs should not be too large to cause overfitting......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 35us/step\n",
      "Test accuracy: 0.7816901349685561\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "************************************ ****************************........................\n",
      "DecisionTreeClassifier****************************************************************\n",
      "Best Hyper Parameters: {'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 11, 'random_state': 123}\n",
      "Accuracy: 0.47183098591549294\n",
      "Confusion Metrix:\n",
      " [[41 45]\n",
      " [30 26]]\n",
      "Randomforest****************************************************************\n",
      "Best Hyper Parameters:\n",
      " {'criterion': 'gini', 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 25, 'n_jobs': -1, 'random_state': 123}\n",
      "Accuracy: 0.6971830985915493\n",
      "Confusion Metrix:\n",
      " [[45 17]\n",
      " [26 54]]\n",
      "RSVM****************************************************************\n",
      "Best Hyper Parameters:\n",
      " {'C': 6, 'kernel': 'linear'}\n",
      "Accuracy: 0.5915492957746479\n",
      "Confusion Metrix:\n",
      " [[47 34]\n",
      " [24 37]]\n",
      "kNearestNeighbors****************************************************************\n",
      "Best Hyper Parameters:\n",
      " {'algorithm': 'auto', 'leaf_size': 1, 'n_jobs': -1, 'n_neighbors': 6, 'weights': 'distance'}\n",
      "Accuracy: 0.7816901408450704\n",
      "Confusion Metrix:\n",
      " [[52 12]\n",
      " [19 59]]\n"
     ]
    }
   ],
   "source": [
    "#With Hyper Parameters Tuning\n",
    "#DesicionTree\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************........................\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "#clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "#clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "#y_pred = clf.predict(X_test)\n",
    "#print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "#importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#making the instance\n",
    "model= DecisionTreeClassifier(random_state=1234)\n",
    "#Hyper Parameters Set\n",
    "params = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n",
    "          'random_state':[123]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#Learning\n",
    "model1.fit(X_train, y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "\n",
    "print(\"Randomforest****************************************************************\")\n",
    "#With Hyper Parameters Tuning\n",
    "#Randomforest\n",
    "#importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#making the instance\n",
    "model=RandomForestClassifier()\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[10,15,20,25,30],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7], \n",
    "          'random_state':[123],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#learning\n",
    "model1.fit(X_train, y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "\n",
    "\n",
    "print(\"RSVM****************************************************************\")\n",
    "#With Hyper Parameters Tuning\n",
    "#SVM\n",
    "#importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "#making the instance\n",
    "model=svm.SVC()\n",
    "#Hyper Parameters Set\n",
    "params = {'C': [6,7,8,9,10,11,12], \n",
    "          'kernel': ['linear','rbf']}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#Learning\n",
    "model1.fit(X_train, y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "\n",
    "\n",
    "\n",
    "print(\"kNearestNeighbors****************************************************************\")\n",
    "#With Hyper Parameters Tuning\n",
    "#kNearestNeighbors\n",
    "#importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#making the instance\n",
    "model = KNeighborsClassifier(n_jobs=-1)\n",
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[5,6,7,8,9,10,13,15],\n",
    "          'leaf_size':[1,2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=1)\n",
    "#Learning\n",
    "model1.fit(X_train, y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(all_test_text) 142\n",
      "************************************ ****************************\n",
      "Found 18373 unique words.\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.704225352112676\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6619718309859155\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n",
      "0.8028169014084507\n",
      "LogisticRegression****************************************************************\n",
      "0.8591549295774648\n",
      "SGDClassifier****************************************************************\n",
      "0.8732394366197183\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6619718309859155\n",
      "Naive****************************************************************\n",
      "0.676056338028169\n",
      "SVM****************************************************************\n",
      "0.8450704225352113\n",
      "RandomForest_100****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823943661971831\n",
      "RandomForest_200****************************************************************\n",
      "0.8098591549295775\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7464788732394366\n",
      "BaggingClassifier****************************************************************\n",
      "0.7746478873239436\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7183098591549296\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 136 samples, validate on 16 samples\n",
      "Epoch 1/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.9822 - acc: 0.4706 - val_loss: 0.0866 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 0s 193us/step - loss: 0.8595 - acc: 0.5588 - val_loss: 2.7458 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 0s 139us/step - loss: 0.8165 - acc: 0.5662 - val_loss: 1.7604 - val_acc: 0.0625\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.3461 - acc: 0.7794 - val_loss: 0.5593 - val_acc: 0.6875\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.2335 - acc: 0.9485 - val_loss: 0.3081 - val_acc: 0.9375\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.2535 - acc: 0.9044 - val_loss: 0.4687 - val_acc: 0.7500\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.1264 - acc: 1.0000 - val_loss: 0.8877 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 0s 131us/step - loss: 0.0870 - acc: 1.0000 - val_loss: 1.3324 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 0s 141us/step - loss: 0.0930 - acc: 0.9926 - val_loss: 1.3061 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 0s 191us/step - loss: 0.0573 - acc: 1.0000 - val_loss: 0.9733 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 0s 169us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 0.6878 - val_acc: 0.6250\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 0s 135us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.5133 - val_acc: 0.6875\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.4261 - val_acc: 0.6875\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 0s 147us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.4039 - val_acc: 0.7500\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.4282 - val_acc: 0.6875\n",
      "Epoch 16/30\n",
      "136/136 [==============================] - 0s 134us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4867 - val_acc: 0.6250\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.6250\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 0s 148us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.6862 - val_acc: 0.6250\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.8019 - val_acc: 0.6250\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 0s 137us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9131 - val_acc: 0.6250\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 0s 136us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.0066 - val_acc: 0.6250\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 0s 134us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0757 - val_acc: 0.6250\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 0s 201us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.1202 - val_acc: 0.6250\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 0s 153us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.1428 - val_acc: 0.6250\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 0s 152us/step - loss: 9.5885e-04 - acc: 1.0000 - val_loss: 1.1477 - val_acc: 0.6250\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 0s 148us/step - loss: 8.3000e-04 - acc: 1.0000 - val_loss: 1.1376 - val_acc: 0.6250\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 0s 138us/step - loss: 7.1902e-04 - acc: 1.0000 - val_loss: 1.1186 - val_acc: 0.6250\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 0s 140us/step - loss: 6.2045e-04 - acc: 1.0000 - val_loss: 1.0946 - val_acc: 0.6250\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 0s 134us/step - loss: 5.5256e-04 - acc: 1.0000 - val_loss: 1.0670 - val_acc: 0.6250\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 0s 133us/step - loss: 4.8686e-04 - acc: 1.0000 - val_loss: 1.0409 - val_acc: 0.6250\n",
      "142/142 [==============================] - 0s 39us/step\n",
      "Test accuracy: 0.8028169131614793\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)#.... check out the problem this region causes\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 30 negative = 6\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 38 negative = 8\n",
      "MLPClassifier****************************************************************\n",
      "positive= 14 negative = 4\n",
      "LogisticRegression****************************************************************\n",
      "positive= 6 negative = 4\n",
      "SGDClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 39 negative = 9\n",
      "Naive****************************************************************\n",
      "positive= 33 negative = 5\n",
      "SVM****************************************************************\n",
      "positive= 8 negative = 4\n",
      "RandomForest_100****************************************************************\n",
      "positive= 15 negative = 8\n",
      "RandomForest_200****************************************************************\n",
      "positive= 18 negative = 9\n",
      "XGBClassifier****************************************************************\n",
      "positive= 25 negative = 7\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 24 negative = 10\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 31 negative = 9\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k5tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= manhattan_k13tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= MLPClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= LogisticRegressiontfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SGDClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= DecisionTreeClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= Naivetfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= SVMtfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= RandomForest_100tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= RandomForest_200tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= XGBClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf \n",
    "y= BaggingClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= SGDClassifiertfidf\n",
    "y= AdaBoostClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "\n",
    "k=5\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "print(\"k=5 manhattan K-Nearest Neighbours\")\n",
    "print(\"accuracy = \",metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "k=13\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "print(\"k=13 manhattan K-Nearest Neighbours\")\n",
    "print(\"accuracy = \",metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv 1063\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv 1109\n",
      "length of features used = 2172\n",
      "len(all_training_text) 152\n",
      "len(X_train)= 152 len(y_train)= 152\n",
      "len(all_test_text) 142\n",
      "len(X_test= 142 len(y_test)= 142\n",
      "****************************************************************\n",
      "vocab_size..... 18374\n",
      "Loaded 400000 word vectors.\n",
      "vocab_size, (len(embedding_matrix)) 18374 18374\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 1000, 100)         1837400   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 993, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 496, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                158730    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,021,773\n",
      "Trainable params: 2,021,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      " - 1s - loss: 0.6952 - acc: 0.4671\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.5789 - acc: 0.8289\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.4208 - acc: 0.9868\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.2732 - acc: 1.0000\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.1436 - acc: 1.0000\n",
      "model_1 Accuracy: 54.929577\n",
      "(152, 1000)\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000\n",
      "model_2 Accuracy: 50.000000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 1000, 100)         1837400   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 2,639,401\n",
      "Trainable params: 802,001\n",
      "Non-trainable params: 1,837,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 8s 54ms/step - loss: 0.6995 - acc: 0.4539\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.6617 - acc: 0.6053\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.6677 - acc: 0.5921\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 0.6060 - acc: 0.6776\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 7s 45ms/step - loss: 0.6523 - acc: 0.6382\n",
      "model_3 Accuracy: 57.04%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 1000, 100)         1837400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,203,993\n",
      "Trainable params: 366,593\n",
      "Non-trainable params: 1,837,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 4s 28ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 3s 18ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 3s 18ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 7.9712 - acc: 0.5000\n",
      "model_4 Accuracy: 50.000000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 1000, 100)         1837400   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 2,639,401\n",
      "Trainable params: 802,001\n",
      "Non-trainable params: 1,837,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "152/152 [==============================] - 8s 52ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "152/152 [==============================] - 7s 46ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "152/152 [==============================] - 7s 44ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "152/152 [==============================] - 7s 43ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "152/152 [==============================] - 6s 42ms/step - loss: 7.9712 - acc: 0.5000\n",
      "model_5 Accuracy: 50.000000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "#X_train = np.nan_to_num(X_train)\n",
    "#X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "labels = train_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab_size.....\", vocab_size)\n",
    "\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 1000\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "test_labels = test_tags\n",
    "\n",
    "# prepare tokenizer\n",
    "test_t = Tokenizer()\n",
    "test_t.fit_on_texts(test_docs)\n",
    "test_vocab_size = len(test_t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "\n",
    "test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "print(\"vocab_size, (len(embedding_matrix))\",vocab_size, (len(embedding_matrix) ) ) \n",
    "      \n",
    "      \n",
    "# define model_1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= True))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(padded_docs, labels, epochs=5, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_1 Accuracy: %f' % (acc*100))\n",
    "        \n",
    "# define model_2\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= True))\n",
    "#model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= False))\n",
    "model.add(Conv1D(50, 5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(padded_docs.shape)\n",
    "model.fit(padded_docs, labels, epochs=5, batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_2 Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# create the model_3\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "#model.add(Embedding(top_words, len(embedding_matrix), input_length=max_length))\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable= False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print(\"model_3 Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the model_4\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_4 Accuracy: %f' % (scores[1]*100))\n",
    "\n",
    "\n",
    "# create the model_5\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "#model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs, labels, epochs=5, batch_size=64)\n",
    "scores = model.evaluate(test_padded_docs, test_labels, verbose=0)\n",
    "print('model_5 Accuracy: %f' % (scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_training_text) 152\n",
      "len(all_test_text) 142\n",
      "****************************************************************\n",
      "train_labels.shape... (152, 2)\n",
      "length_train_sequences..... 2360 1213 1522\n",
      "length_train_data.shape..... 2500\n",
      "test_labels.shape.... (142, 2)\n",
      "Found 400,000 word vectors in GloVe.\n",
      "nb_words......  17423\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d66ad5a94ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNoon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;31m#trainable=False... Accuracy: 61.267606\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rate' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "#print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        #print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "#X_train = np.nan_to_num(X_train)\n",
    "#X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import layers\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_tags)\n",
    "print(\"train_labels.shape...\",train_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 50000\n",
    "vocab_size =len(word_index)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(docs)\n",
    "train_sequences = tokenizer.texts_to_sequences(docs) # Generate sequences\n",
    "print(\"length_train_sequences.....\", len(train_sequences[0]), len(train_sequences[10]), len(train_sequences[20]))\n",
    "\n",
    "# pad documents to a max length of 1000 words\n",
    "max_length = 2500\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length)\n",
    "print(\"length_train_data.shape.....\", len(train_data[0]))\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "#test_labels = test_tags\n",
    "test_labels = to_categorical(np.asarray(test_tags))\n",
    "print(\"test_labels.shape....\",test_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "# integer encode the documents\n",
    "#test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup test tokenizer\n",
    "tokenizer.fit_on_texts(test_docs)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_docs) # Generate sequences\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the whole embedding into memory\n",
    "#embeddings_index = dict()\n",
    "#glove_dir  = open('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "glove_dir  = '/Users/catherine/Downloads'\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))\n",
    "\n",
    "        \n",
    "        \n",
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "print(\"nb_words...... \",nb_words )\n",
    "nb_words = vocab_size\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "#trainable=False... Accuracy: 61.267606\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False))\n",
    "model.add(Conv1D(512, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "#model.fit(train_data, train_labels, epochs=10, validation_split=0.0)\n",
    "model.fit(train_data, train_labels,  validation_data=(test_data, test_labels), epochs=10)\n",
    "print(\"model.metrics_names\", model.metrics_names)\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('model_6 Accuracy: %f' % (scores[1]*100), scores)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=max_length, \n",
    "                           trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'  ))\n",
    "model.add(Dropout(0.3))\n",
    "#model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "#model.compile(optimizer='adam',\n",
    "              #loss='binary_crossentropy',\n",
    "              #metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"model.metrics_names\", model.metrics_names)\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "model.fit(train_data, train_labels,  validation_data=(test_data, test_labels), epochs=10)\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('model_7 Accuracy: %f' % (scores[1]*100), scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check the validation error during each epoch......\n",
    "fully connected layers works for out put that have got shapes but not for only labels...\n",
    "convolutional layer best layer when input structure is important\n",
    "\n",
    "##Create a bigger model\n",
    "bigger_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "    model.fit(train_data, train_labels,  epochs=10, batch_size=10)\n",
    "    scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_training_text) 152\n",
      "len(all_test_text) 142\n",
      "****************************************************************\n",
      "(152, 1)\n",
      "train_sequences_matrix 152 [ 28  36  30 ... 420  16  24]\n",
      "(142, 1)\n",
      "X_train (264, 1500) (264, 1)\n",
      "X_test (30, 1500) (30, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1500, 50)          50000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 430,417\n",
      "Trainable params: 430,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "264/264 [==============================] - 8s 31ms/step - loss: 0.6943 - acc: 0.4583\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 8s 29ms/step - loss: 0.6906 - acc: 0.5871\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 8s 30ms/step - loss: 0.6828 - acc: 0.5720\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 8s 31ms/step - loss: 2.0643 - acc: 0.6288\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 8s 31ms/step - loss: 0.6625 - acc: 0.7008\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 8s 32ms/step - loss: 0.6345 - acc: 0.8333\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 9s 33ms/step - loss: 0.5832 - acc: 0.8523\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 9s 35ms/step - loss: 0.5874 - acc: 0.7462\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 9s 32ms/step - loss: 0.4825 - acc: 0.8788\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 9s 33ms/step - loss: 0.4966 - acc: 0.8902\n",
      "30/30 [==============================] - 0s 15ms/step\n",
      "Test set\n",
      "  Loss: 0.766\n",
      "  Accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "#print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        #print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "#X_train = np.nan_to_num(X_train)\n",
    "#X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import layers\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define train documents\n",
    "docs = train_posts\n",
    "\n",
    "# define train class labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(train_tags)\n",
    "train_labels = Y.reshape(-1,1)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 1500\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(docs)\n",
    "train_sequences = tok.texts_to_sequences(docs)\n",
    "train_sequences_matrix = sequence.pad_sequences(train_sequences,maxlen=max_len)\n",
    "print(\"train_sequences_matrix\",len(train_sequences_matrix), train_sequences_matrix[0, :])\n",
    "\n",
    "\n",
    "\n",
    "# define test documents\n",
    "test_docs = test_posts\n",
    "\n",
    "# define train class labels\n",
    "#test_labels = test_tags\n",
    "la = LabelEncoder()\n",
    "Yy = la.fit_transform(test_tags)\n",
    "test_labels = Yy.reshape(-1,1)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# integer encode the documents\n",
    "#test_encoded_docs = test_t.texts_to_sequences(test_docs)\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(test_docs)\n",
    "test_sequences = tok.texts_to_sequences(test_docs)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "X = np.concatenate((train_sequences_matrix, test_sequences_matrix))\n",
    "Y= np.concatenate((train_labels, test_labels))\n",
    "\n",
    "#Train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(\"X_train\", X_train.shape,Y_train.shape)\n",
    "print(\"X_test\", X_test.shape,Y_test.shape)\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(256)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,batch_size=128,epochs=10)\n",
    "#model.fit(train_sequences_matrix,train_labels,batch_size=128,epochs=10)#,\n",
    "          #validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "\n",
    "\n",
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_training_text) 152\n",
      "len(all_test_text) 142\n",
      "****************************************************************\n",
      "len__vocab 2172\n",
      "len(train_tokens),len(fdist),fdist[0] 216597 18373 ('username', 8321)\n",
      "len(tokens)for vocab 1964\n",
      "vocab_size 2173\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cc5608a4b694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;31m# load embedding from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m \u001b[0mraw_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/catherine/Downloads/glove.6B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;31m# get vectors in the right order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0membedding_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weight_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "#word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "#print(\"txt_files of features used\",txt_files)\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        reader = csv.reader(f, delimiter=\",\")\n",
    "#        next(reader) # skip header\n",
    "#        word_list1 =  [r[0] for r in reader]\n",
    "        #print(txt_file, len(word_list1))\n",
    "#        word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "#X_train = np.nan_to_num(X_train)\n",
    "#X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import layers\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "train_tags=array([1 if x==\"M\" else 0 for x in train_tags])\n",
    "#train_tags=train_tags.reshape(-1, 1)\n",
    "#print(\"train_tags\", print(train_tags.shape), (train_tags))\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "test_tags=[1 if x==\"M\" else 0 for x in test_tags]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "# vocab_filename = ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_ vocubulary_set.1.csv']\n",
    "vocab_filename = ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv']\n",
    "\n",
    "for txt_file in vocab_filename:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        #print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "vocab = set(word_list)\n",
    "print(\"len__vocab\",len(vocab))\n",
    "\n",
    "# load all training reviews\n",
    "#positive_docs = process_docs('/Users/catherine/Downloads/review_polarity/txt_sentoken/pos', vocab, True)\n",
    "#negative_docs = process_docs('/Users/catherine/Downloads/review_polarity/txt_sentoken/neg', vocab, True)\n",
    "#train_docs = negative_docs + positive_docs\n",
    "train_docs=train_posts\n",
    "\n",
    "train_tokens=[x  for z in train_docs for x  in z]\n",
    "fdist=nltk.FreqDist(train_tokens)\n",
    "fdist = sorted(dict(fdist).items(), key=operator.itemgetter(1), reverse=True)\n",
    "#print(fdist)\n",
    "print(\"len(train_tokens),len(fdist),fdist[0]\",len(train_tokens),len(fdist),fdist[0])\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 9\n",
    "tokens = [k for k,c in dict(fdist).items() if c >= min_occurane]\n",
    "print(\"len(tokens)for vocab\",len(tokens))\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')\n",
    "\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load the vocabulary\n",
    "# vocab_filename = 'vocab.txt'\n",
    "# vocab = load_doc(vocab_filename)\n",
    "# vocab = vocab.split()\n",
    "# vocab = set(vocab)\n",
    "\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "train_docs= [[w for w in tokens if w in vocab ] for tokens in train_docs]\n",
    "#[[number+1 for number in group] for group in x]\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "\n",
    "# pad sequences\n",
    "max_length = max([len(s) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytrain = train_tags\n",
    "\n",
    "test_docs = test_posts\n",
    "#test_docs = [w for w in tokens for tokens in test_docs if w in vocab ]\n",
    "test_docs= [[w for w in tokens if w in vocab ] for tokens in test_docs]\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "ytest = test_tags\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab_size\", vocab_size)\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('/Users/catherine/Downloads/glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "\n",
    "# define model1\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "# define model2.... HAS NO WEIGHTS\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "# define model3\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_length])\n",
    "    layer = Embedding(vocab_size,100,input_length=max_length)(inputs)\n",
    "    layer = LSTM(256)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "model.fit(Xtrain, ytrain,batch_size=128,epochs=10)\n",
    "#model.fit(train_sequences_matrix,train_labels,batch_size=128,epochs=10)#,\n",
    "          #validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "\n",
    "accr = model.evaluate(Xtest, ytest)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "\n",
    "# define model4\n",
    "#trainable=False... Accuracy: 61.267606\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_vectors], \n",
    "                    trainable=False))\n",
    "model.add(Conv1D(512, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "#model.fit(train_data, train_labels, epochs=10, validation_split=0.0)\n",
    "model.fit(Xtrain, ytrain,  validation_data=(Xtest, ytest), epochs=10)\n",
    "print(\"model.metrics_names\", model.metrics_names)\n",
    "scores = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('model_4 Accuracy: %f' % (scores[1]*100), scores)\n",
    "\n",
    "                        \n",
    "\n",
    "# define model_1\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= True))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('model_5 Accuracy: %f' % (acc*100))\n",
    "        \n",
    "# define model_2\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable= True))\n",
    "#model.add(Embedding(vocab_size, 100, input_length=max_length, trainable= False))\n",
    "model.add(Conv1D(50, 5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(padded_docs.shape)\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=64)\n",
    "\n",
    "loss, accuracy = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('model_6 Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# create the model_3\n",
    "\n",
    "# create the model\n",
    "#embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "#model.add(Embedding(top_words, len(embedding_matrix), input_length=max_length))\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable= False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print(\"model_7 Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the model_4\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_vectors), 100, weights=[embedding_vectors], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('model_8 Accuracy: %f' % (scores[1]*100))\n",
    "\n",
    "\n",
    "# create the model_5\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_vectors), 100, weights=[embedding_vectors], input_length=max_length, trainable=False))\n",
    "#model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(Xtrain, ytrain, epochs=10, batch_size=64)\n",
    "scores = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('model_9 Accuracy: %f' % (scores[1]*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_train.1.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_train.1.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_male_test.1.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles//PAN2015_tweet_female_test.1.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "#txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015ProbD.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2015/PAN2015.1_textfiles/PAN2015_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "#txt_files =[\"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2015/output2015GSS.txt\"]\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "#for txt_file in txt_files:\n",
    "#    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#        word_list =  ([(r.split(\":\"))[1] for r in f])[0:200]\n",
    "#        print(txt_file, len(word_list))\n",
    "#        #word_list = word_list + word_list1\n",
    "#print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "all_training_text = ( [((all_txt(M_train_file,  F_train_file))[i][1])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [((all_txt(M_train_file,  F_train_file))[i][0])  for i in range(len((all_txt(M_train_file,  F_train_file))))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "all_test_text = ( [((all_txt(M_test_file,  F_test_file))[i][1])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [((all_txt(M_test_file,  F_test_file))[i][0])  for i in range(len((all_txt(M_test_file,  F_test_file))))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "from fuzzywuzzy import fuzz\n",
    "for i in range(len(all_training_text)):\n",
    "    for j in range(len(all_test_text)):\n",
    "        print(fuzz.ratio(all_training_text[i], all_test_text[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
