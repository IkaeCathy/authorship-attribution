{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019textfiles_Type/PAN2019_tweet_human1_chosen_set.2.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019textfiles_Type/PAN2019_tweet_human1_chosen_set.2.csv 5828\n",
      "length of features used = 5828\n",
      "len(all_training_text) 1440\n",
      "len(X_train)= 1440 len(y_train)= 1440\n",
      "len(all_test_text) 620\n",
      "len(X_test= 620 len(y_test)= 620\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6887096774193548\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.682258064516129\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8451612903225807\n",
      "LogisticRegression****************************************************************\n",
      "0.6951612903225807\n",
      "SGDClassifier****************************************************************\n",
      "0.6129032258064516\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.614516129032258\n",
      "Naive****************************************************************\n",
      "0.6661290322580645\n",
      "SVM****************************************************************\n",
      "0.6064516129032258\n",
      "RandomForest_100****************************************************************\n",
      "0.7903225806451613\n",
      "RandomForest_200****************************************************************\n",
      "0.785483870967742\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7612903225806451\n",
      "BaggingClassifier****************************************************************\n",
      "0.7161290322580646\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7241935483870968\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_train.3.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_train.3.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_test.3.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_test.3.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.2.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.2.csv\"]\n",
    "txt_files =[\"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019textfiles_Type/PAN2019_tweet_human1_chosen_set.2.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.2.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.2.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.2.csv 6783\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.2.csv 7214\n",
      "length of features used = 13997\n",
      "len(all_training_text) 1440\n",
      "len(X_train)= 1440 len(y_train)= 1440\n",
      "len(all_test_text) 620\n",
      "len(X_test= 620 len(y_test)= 620\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6483870967741936\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6548387096774193\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832258064516129\n",
      "LogisticRegression****************************************************************\n",
      "0.6951612903225807\n",
      "SGDClassifier****************************************************************\n",
      "0.5112903225806451\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6564516129032258\n",
      "Naive****************************************************************\n",
      "0.7274193548387097\n",
      "SVM****************************************************************\n",
      "0.5758064516129032\n",
      "RandomForest_100****************************************************************\n",
      "0.7629032258064516\n",
      "RandomForest_200****************************************************************\n",
      "0.7838709677419354\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226\n",
      "BaggingClassifier****************************************************************\n",
      "0.7612903225806451\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7370967741935484\n",
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv 71677\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv 67723\n",
      "length of features used = 139400\n",
      "len(all_training_text) 1440\n",
      "len(X_train)= 1440 len(y_train)= 1440\n",
      "len(all_test_text) 620\n",
      "len(X_test= 620 len(y_test)= 620\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6645161290322581\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6725806451612903\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8290322580645161\n",
      "LogisticRegression****************************************************************\n",
      "0.7112903225806452\n",
      "SGDClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048387096774194\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6548387096774193\n",
      "Naive****************************************************************\n",
      "0.7274193548387097\n",
      "SVM****************************************************************\n",
      "0.635483870967742\n",
      "RandomForest_100****************************************************************\n",
      "0.7693548387096775\n",
      "RandomForest_200****************************************************************\n",
      "0.785483870967742\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226\n",
      "BaggingClassifier****************************************************************\n",
      "0.7048387096774194\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7370967741935484\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_train.3.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_train.3.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_test.3.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_test.3.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.2.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.2.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivefs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200fs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifierfs=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.2.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.2.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k5voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "manhattan_k13voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressionvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivevoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMvoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200voc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifiervoc=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 142 negative = 28\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 139 negative = 29\n",
      "MLPClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "LogisticRegression****************************************************************\n",
      "positive= 118 negative = 33\n",
      "SGDClassifier****************************************************************\n",
      "positive= 245 negative = 46\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 152 negative = 43\n",
      "Naive****************************************************************\n",
      "positive= 98 negative = 33\n",
      "SVM****************************************************************\n",
      "positive= 203 negative = 44\n",
      "RandomForest_100****************************************************************\n",
      "positive= 82 negative = 39\n",
      "RandomForest_200****************************************************************\n",
      "positive= 67 negative = 37\n",
      "XGBClassifier****************************************************************\n",
      "positive= 73 negative = 41\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 89 negative = 45\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 99 negative = 40\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= MLPClassifierfs \n",
    "y= manhattan_k5fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= MLPClassifierfs \n",
    "y= manhattan_k13fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= MLPClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= LogisticRegressionfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= SGDClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= DecisionTreeClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= Naivefs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= SVMfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= RandomForest_100fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= RandomForest_200fs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= XGBClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= BaggingClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= MLPClassifierfs \n",
    "y= AdaBoostClassifierfs\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 140 negative = 38\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 129 negative = 32\n",
      "MLPClassifier****************************************************************\n",
      "positive= 0 negative = 0\n",
      "LogisticRegression****************************************************************\n",
      "positive= 107 negative = 34\n",
      "SGDClassifier****************************************************************\n",
      "positive= 106 negative = 29\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 148 negative = 40\n",
      "Naive****************************************************************\n",
      "positive= 98 negative = 35\n",
      "SVM****************************************************************\n",
      "positive= 153 negative = 33\n",
      "RandomForest_100****************************************************************\n",
      "positive= 78 negative = 41\n",
      "RandomForest_200****************************************************************\n",
      "positive= 72 negative = 45\n",
      "XGBClassifier****************************************************************\n",
      "positive= 69 negative = 39\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 111 negative = 34\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 95 negative = 38\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k5voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= manhattan_k13voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= MLPClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= LogisticRegressionvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SGDClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= DecisionTreeClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= Naivevoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= SVMvoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_100voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= RandomForest_200voc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= XGBClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= BaggingClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= MLPClassifiervoc \n",
    "y= AdaBoostClassifiervoc\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.1.csv 3584\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.1.csv 3710\n",
      "length of features used = 7294\n",
      "len(all_training_text) 1440\n",
      "len(X_train)= 1440 len(y_train)= 1440\n",
      "len(all_test_text) 620\n",
      "len(X_test= 620 len(y_test)= 620\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6451612903225806\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6629032258064517\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338709677419355\n",
      "LogisticRegression****************************************************************\n",
      "0.6951612903225807\n",
      "SGDClassifier****************************************************************\n",
      "0.6967741935483871\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6306451612903226\n",
      "Naive****************************************************************\n",
      "0.7225806451612903\n",
      "SVM****************************************************************\n",
      "0.5758064516129032\n",
      "RandomForest_100****************************************************************\n",
      "0.7693548387096775\n",
      "RandomForest_200****************************************************************\n",
      "0.7919354838709678\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226\n",
      "BaggingClassifier****************************************************************\n",
      "0.7306451612903225\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7370967741935484\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_train.3.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_train.3.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_test.3.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_test.3.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, y_test)])\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv 71677\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv 67723\n",
      "length of features used = 139400\n",
      "mean length =  2066.742361111111\n",
      "len(all_training_text) 1440\n",
      "len(X_train)= 1440 len(y_train)= 1440\n",
      "len(all_test_text) 620\n",
      "len(X_test= 620 len(y_test)= 620\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6645161290322581\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6725806451612903\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8225806451612904\n",
      "LogisticRegression****************************************************************\n",
      "0.7112903225806452\n",
      "SGDClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5209677419354839\n",
      "DecisionTreeClassifier****************************************************************\n",
      "0.6661290322580645\n",
      "Naive****************************************************************\n",
      "0.7274193548387097\n",
      "SVM****************************************************************\n",
      "0.635483870967742\n",
      "RandomForest_100****************************************************************\n",
      "0.7790322580645161\n",
      "RandomForest_200****************************************************************\n",
      "0.7887096774193548\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226\n",
      "BaggingClassifier****************************************************************\n",
      "0.7016129032258065\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7370967741935484\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_train.3.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_train.3.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_test.3.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_test.3.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train, y_train)\n",
    "y_pred = Naive.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_files of features used ['/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv', '/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv']\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv 71677\n",
      "/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv 67723\n",
      "length of features used = 139400\n",
      "mean length =  2066.742361111111\n",
      "len(all_training_text) 1440\n",
      "len(all_test_text) 620\n",
      "Found 115144 unique words.\n",
      "5\n",
      "manhattan_k=5************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6645161290322581\n",
      "************************************ ****************************\n",
      "13\n",
      "manhattan_k=13************************************ ****************************\n",
      "manhattan K-Nearest Neighbours\n",
      "0.6451612903225806\n",
      "****************************************************************\n",
      "MLPClassifier****************************************************************\n",
      "0.785483870967742\n",
      "LogisticRegression****************************************************************\n",
      "0.7774193548387097\n",
      "SGDClassifier****************************************************************\n",
      "0.75\n",
      "DecisionTreeClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6483870967741936\n",
      "Naive****************************************************************\n",
      "0.8048387096774193\n",
      "SVM****************************************************************\n",
      "0.7693548387096775\n",
      "RandomForest_100****************************************************************\n",
      "0.782258064516129\n",
      "RandomForest_200****************************************************************\n",
      "0.8016129032258065\n",
      "XGBClassifier****************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7919354838709678\n",
      "BaggingClassifier****************************************************************\n",
      "0.7483870967741936\n",
      "AdaBoostClassifier****************************************************************\n",
      "0.7467741935483871\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 776,194\n",
      "Trainable params: 776,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1296 samples, validate on 144 samples\n",
      "Epoch 1/30\n",
      "1296/1296 [==============================] - 1s 406us/step - loss: 0.9031 - acc: 0.6073 - val_loss: 0.9515 - val_acc: 0.3472\n",
      "Epoch 2/30\n",
      "1296/1296 [==============================] - 0s 87us/step - loss: 0.3263 - acc: 0.8696 - val_loss: 1.0629 - val_acc: 0.4861\n",
      "Epoch 3/30\n",
      "1296/1296 [==============================] - 0s 89us/step - loss: 0.1766 - acc: 0.9352 - val_loss: 0.5465 - val_acc: 0.8403\n",
      "Epoch 4/30\n",
      "1296/1296 [==============================] - 0s 103us/step - loss: 0.0691 - acc: 0.9823 - val_loss: 0.9154 - val_acc: 0.7778\n",
      "Epoch 5/30\n",
      "1296/1296 [==============================] - 0s 121us/step - loss: 0.0217 - acc: 0.9985 - val_loss: 1.2403 - val_acc: 0.7500\n",
      "Epoch 6/30\n",
      "1296/1296 [==============================] - 0s 90us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 1.4101 - val_acc: 0.7639\n",
      "Epoch 7/30\n",
      "1296/1296 [==============================] - 0s 104us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.4377 - val_acc: 0.7708\n",
      "Epoch 8/30\n",
      "1296/1296 [==============================] - 0s 90us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.6028 - val_acc: 0.7431\n",
      "Epoch 9/30\n",
      "1296/1296 [==============================] - 0s 122us/step - loss: 8.2456e-04 - acc: 1.0000 - val_loss: 1.6322 - val_acc: 0.7431\n",
      "Epoch 10/30\n",
      "1296/1296 [==============================] - 0s 144us/step - loss: 5.9489e-04 - acc: 1.0000 - val_loss: 1.6493 - val_acc: 0.7431\n",
      "Epoch 11/30\n",
      "1296/1296 [==============================] - 0s 143us/step - loss: 4.6971e-04 - acc: 1.0000 - val_loss: 1.6481 - val_acc: 0.7569\n",
      "Epoch 12/30\n",
      "1296/1296 [==============================] - 0s 90us/step - loss: 3.8575e-04 - acc: 1.0000 - val_loss: 1.6746 - val_acc: 0.7569\n",
      "Epoch 13/30\n",
      "1296/1296 [==============================] - 0s 89us/step - loss: 3.2215e-04 - acc: 1.0000 - val_loss: 1.6886 - val_acc: 0.7569\n",
      "Epoch 14/30\n",
      "1296/1296 [==============================] - 0s 92us/step - loss: 2.7443e-04 - acc: 1.0000 - val_loss: 1.7249 - val_acc: 0.7431\n",
      "Epoch 15/30\n",
      "1296/1296 [==============================] - 0s 186us/step - loss: 2.3716e-04 - acc: 1.0000 - val_loss: 1.7187 - val_acc: 0.7569\n",
      "Epoch 16/30\n",
      "1296/1296 [==============================] - 0s 179us/step - loss: 2.0784e-04 - acc: 1.0000 - val_loss: 1.7298 - val_acc: 0.7569\n",
      "Epoch 17/30\n",
      "1296/1296 [==============================] - 0s 162us/step - loss: 1.8277e-04 - acc: 1.0000 - val_loss: 1.7647 - val_acc: 0.7431\n",
      "Epoch 18/30\n",
      "1296/1296 [==============================] - 0s 198us/step - loss: 1.6314e-04 - acc: 1.0000 - val_loss: 1.7571 - val_acc: 0.7500\n",
      "Epoch 19/30\n",
      "1296/1296 [==============================] - 0s 178us/step - loss: 1.4435e-04 - acc: 1.0000 - val_loss: 1.7790 - val_acc: 0.7500\n",
      "Epoch 20/30\n",
      "1296/1296 [==============================] - 0s 180us/step - loss: 1.3110e-04 - acc: 1.0000 - val_loss: 1.7642 - val_acc: 0.7500\n",
      "Epoch 21/30\n",
      "1296/1296 [==============================] - 0s 179us/step - loss: 1.1755e-04 - acc: 1.0000 - val_loss: 1.7843 - val_acc: 0.7500\n",
      "Epoch 22/30\n",
      "1296/1296 [==============================] - 0s 197us/step - loss: 1.0679e-04 - acc: 1.0000 - val_loss: 1.8020 - val_acc: 0.7500\n",
      "Epoch 23/30\n",
      "1296/1296 [==============================] - 0s 185us/step - loss: 9.7603e-05 - acc: 1.0000 - val_loss: 1.8126 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "1296/1296 [==============================] - 0s 176us/step - loss: 8.9478e-05 - acc: 1.0000 - val_loss: 1.8030 - val_acc: 0.7500\n",
      "Epoch 25/30\n",
      "1296/1296 [==============================] - 0s 182us/step - loss: 8.2010e-05 - acc: 1.0000 - val_loss: 1.8294 - val_acc: 0.7500\n",
      "Epoch 26/30\n",
      "1296/1296 [==============================] - 0s 182us/step - loss: 7.6019e-05 - acc: 1.0000 - val_loss: 1.8316 - val_acc: 0.7500\n",
      "Epoch 27/30\n",
      "1296/1296 [==============================] - 0s 205us/step - loss: 7.0094e-05 - acc: 1.0000 - val_loss: 1.8348 - val_acc: 0.7500\n",
      "Epoch 28/30\n",
      "1296/1296 [==============================] - 0s 183us/step - loss: 6.5183e-05 - acc: 1.0000 - val_loss: 1.8456 - val_acc: 0.7500\n",
      "Epoch 29/30\n",
      "1296/1296 [==============================] - 0s 173us/step - loss: 6.0236e-05 - acc: 1.0000 - val_loss: 1.8353 - val_acc: 0.7500\n",
      "Epoch 30/30\n",
      "1296/1296 [==============================] - 0s 201us/step - loss: 5.6676e-05 - acc: 1.0000 - val_loss: 1.8652 - val_acc: 0.7500\n",
      "620/620 [==============================] - 0s 52us/step\n",
      "Test accuracy: 0.7500000057681915\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/anaconda3/lib/python3.7/site-packages\")\n",
    "import numpy as np\n",
    "import numpy\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "#from read_xml_files import *\n",
    "##from s_stemmer import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "##from pattern.text.en import singularize\n",
    "##nltk.download('vader_lexicon')\n",
    "##nltk.download('punkt')\n",
    "#import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##tt = ToktokTokenizer()#keeps the url as is...................................................\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "Porter_stemmer = PorterStemmer()\n",
    "Lancaster_stemmer = LancasterStemmer()\n",
    "WordNet_lemmatizer = WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "##Train_path=\"D:/Data/PAN2015-Profiling/pan15-author-profiling-training-dataset-english-2015-04-23\"#path for train data\n",
    "##Test_path=\"D:/Data/PAN2015-Profiling/pan-ap2015-test/en\" #path for test data\n",
    "\n",
    "##Train_path=\"D:/NLP/PAN2014/pan14-author-profiling-training-corpus-english-twitter-2014-04-16\"#path for train data D:\\Data\\PAN2013-Profiling\\Training\\en  D:\\NLP\\PAN2014\n",
    "##train_truth_path=\"D:/NLP/PAN2014/truth_train\"\n",
    "##\n",
    "##test_truth_path=\"D:/NLP/PAN2014/pan14_test\"\n",
    "##Test_path=\"D:/NLP/PAN2014/pan14_test/en\"\n",
    "##Content = open('results_of_clssification_all.txt', 'w')\n",
    "\n",
    "M_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_train.3.txt\"\n",
    "F_train_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_train.3.txt\"\n",
    "\n",
    "M_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_test.3.txt\"\n",
    "F_test_file= \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_test.3.txt\"\n",
    "\n",
    "def all_txt(M_txt_file,  F_txt_file):\n",
    "    with open(M_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            male_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in male_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    male_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    male_dict[line[1]] = [line[3]]\n",
    "    male_content= [(\"M\", (' '.join(a)).split()) for a in list(male_dict.values())]\n",
    "  \n",
    "    with open(F_txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_list = [(line[:-1]).split(\"\\t\") for line in f]\n",
    "            female_dict = dict()\n",
    "            content=[]\n",
    "            for line in txt_list:\n",
    "                if line[1] in female_dict:\n",
    "                # append the new number to the existing array at this slot\n",
    "                    female_dict[line[1]].append(line[3])\n",
    "                else:\n",
    "                # create a new array in this slot\n",
    "                    female_dict[line[1]] = [line[3]]\n",
    "    female_content= [(\"F\", (' '.join(a)).split()) for a in list(female_dict.values())]\n",
    "    all_txt_per_person = male_content + female_content\n",
    "    return all_txt_per_person\n",
    "\n",
    "##print( [((all_txt(M_txt_file,  F_txt_file))[i][0])  for i in range(len((all_txt(M_txt_file,  F_txt_file))))]) helps to determine the gender....\n",
    "\n",
    "      \n",
    "\"\"\"\n",
    "create feature vector from the training set.....................................................................................\n",
    "\"\"\"\n",
    "\n",
    "def WordFeatures(word_list, all_training_text):\n",
    "\n",
    "    fvs_words = np.array([[author.count(word) for word in word_list] for author in all_training_text]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens for each author........\n",
    "    fvs_words /= np.c_[np.array([len(author) for author in all_training_text])]\n",
    "\n",
    "    return fvs_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From the saved csv file, recover the saved features to be used...............................................................\n",
    "\"\"\"\n",
    "import csv\n",
    "word_list=[]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.1M.txt\", \"/Users/catherine/Desktop/NLP/PAN Datasets/PAN_chosen_tokens/2014/output2014OR.2F.txt\"]\n",
    "txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_vocubulary.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_ vocubulary_set.1.csv\"]\n",
    "##txt_files =[ \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_female_chosen_set.1.csv\", \"/Users/catherine/Desktop/NLP/PAN_Datasets/PAN2019/PAN_2019_txtfiles/PAN2019_tweet_male_chosen_set.1.csv\"]\n",
    "\n",
    "print(\"txt_files of features used\",txt_files)\n",
    "##for txt_file in txt_files:\n",
    "##    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "##        reader = csv.reader(f, delimiter=\",\")\n",
    "##        next(reader) # skip header\n",
    "##        word_list1 =  ([(r.split(\":\"))[1] for r in f])\n",
    "##        print(txt_file, len(word_list1))\n",
    "##        word_list = word_list + word_list1\n",
    "##print(\"length of features used =\", len(word_list))\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        next(reader) # skip header\n",
    "        word_list1 =  [r[0] for r in reader]\n",
    "        print(txt_file, len(word_list1))\n",
    "        word_list = word_list + word_list1\n",
    "print(\"length of features used =\", len(word_list))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare the training and test sets to be parsed to the classifies............................................................\n",
    "\"\"\"\n",
    "A=(all_txt(M_train_file,  F_train_file))\n",
    "all_training_text = ( [(A[i][1])  for i in range(len(A))])\n",
    "print(\"mean length = \",(len([x for item in all_training_text for x in item]))/len(all_training_text))\n",
    "print(\"len(all_training_text)\",len(all_training_text))\n",
    "#X_train=WordFeatures(word_list, all_training_text)\n",
    "y_train=np.array( [(A[i][0])  for i in range(len(A))])\n",
    "#print(\"len(X_train)=\",len(X_train), \"len(y_train)=\", len(y_train))\n",
    "\n",
    "B=(all_txt(M_test_file,  F_test_file))\n",
    "all_test_text = ( [(B[i][1])  for i in range(len(B))])\n",
    "print(\"len(all_test_text)\",len(all_test_text))\n",
    "#X_test = WordFeatures(word_list, all_test_text)\n",
    "y_test = np.array( [(B[i][0])  for i in range(len(B))])\n",
    "#print(\"len(X_test=\",len(X_test), \"len(y_test)=\",len(y_test))\n",
    "\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "data_tags = [\"type\",\"gender\",\"tweet\"]\n",
    "train_data_list = []\n",
    "i=0\n",
    "for f in y_train:\n",
    "   \n",
    "    train_data_list.append((f, y_train[i],all_training_text[i]))\n",
    "    i += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "train_data = pd.DataFrame.from_records(train_data_list, columns=data_tags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "test_data_list = []\n",
    "ii=0\n",
    "for f in y_test:\n",
    "   \n",
    "    test_data_list.append((f, y_test[ii], all_test_text[ii]))\n",
    "    ii += 1\n",
    "# We have training data available as dictionary filename, category, data\n",
    "test_data = pd.DataFrame.from_records(test_data_list, columns=data_tags)\n",
    "\n",
    "#..................................................................................\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "# :Tokenize and Prepare Vocabulary\n",
    "\n",
    "train_posts = train_data['tweet']\n",
    "train_tags = train_data['type']\n",
    "\n",
    "test_posts = test_data['tweet']\n",
    "test_tags = test_data['type']\n",
    "\n",
    "\n",
    "num_labels = 2\n",
    "vocab_size = 1000#len(word_list)#most common number of words will be then kept for use in the vector\n",
    "#vocab_size = 1000#by changing the vocubulary size... acc count_type =  0.932258064516129#acc count_gender = 0.8298387096774194\n",
    "#vocab_size = 100#acc count_type =0.9274193548387096 acc count_gender = 0.792741935483871\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#IF I USE OTHER TOKENIZER...\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "#print(tokenizer.word_index)\n",
    "#print((tokenizer.word_counts))#provides a dictionary of the words and the count......................\n",
    "#sorted_x = sorted((tokenizer.word_counts).items(), key=operator.itemgetter(1),reverse=True)\n",
    "#print(sorted_x)\n",
    "#print((tokenizer.document_count))#number of documents..............\n",
    "#print((tokenizer.word_docs))#provides a dictionary of the words and the number of documents they appear in......................\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))#shows total vocubulary of the text dataset\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')#WHAT HAPPENS WHEN I GIVE IT THE HAND CRAFTED TOKENS??\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "##--------what about applying knn at this point-------------------------------\n",
    "##--------what about changing the vocubulary size-------------------------------works well with reduced sized\n",
    "##----------One popular method for hyperparameter optimization is grid search.-----------\n",
    "##-----determine the best set of parameters with the highest accuracy..........this is for the keras model------\n",
    "\n",
    "\n",
    "      \n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "\n",
    "encoder.fit(train_tags)\n",
    "\n",
    "y_train = encoder.transform(train_tags)#same as the y train generated with my model\n",
    "y_train = np.hstack((y_train, 1 - y_train))#used for two label cases........\n",
    "\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_test = np.hstack((y_test, 1 - y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 0; word_index[\"cat\"] = 1 it is word -> index dictionary so every word gets a unique integer value. So lower integer means more frequent word (often the first few are punctuation because they appear a lot).\n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "CLEAN TEXT TO THE DESIRED LEVEL AND USE KERAS INBUILF TFIDF TO GENERATE A MATRIX....\n",
    "\"\"\"\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\n",
    "k=5\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k5tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"************************************ ****************************\")\n",
    "\n",
    "k=13\n",
    "\n",
    "print(k)\n",
    "\"\"\"\n",
    "8. manhattan K-Nearest Neighbours..............................................................................\n",
    "\"\"\"\n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k, metric= \"manhattan\")\n",
    "knn.fit(x_train, train_tags)\n",
    "y_pred = knn.predict(x_test)\n",
    "manhattan_k13tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(\"manhattan K-Nearest Neighbours\")\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "print(\"****************************************************************\")\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "clf = MLPClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "MLPClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "LogisticRegressiontfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "SGDClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train, train_tags)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "DecisionTreeClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(x_train, train_tags)\n",
    "y_pred = Naive.predict(x_test)\n",
    "Naivetfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train, train_tags)\n",
    "y_pred = SVM.predict(x_test)\n",
    "SVMtfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_100tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "clf=RandomForestClassifier(n_estimators=200)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "RandomForest_200tfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "clf=XGBClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "XGBClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "clf=BaggingClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "BaggingClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "clf=AdaBoostClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train, train_tags)\n",
    "y_pred=clf.predict(x_test)\n",
    "AdaBoostClassifiertfidf=([(i,j) for i, j in zip(y_pred, test_tags)])\n",
    "print(metrics.accuracy_score(test_tags, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))#512 neurons in the first hidden layer\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate model.............................\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sign test...****************************************************************\n",
      "manhattan_k=5************************************ ****************************\n",
      "positive= 154 negative = 67\n",
      "manhattan_k=13************************************ ****************************\n",
      "positive= 172 negative = 73\n",
      "MLPClassifier****************************************************************\n",
      "positive= 57 negative = 45\n",
      "LogisticRegression****************************************************************\n",
      "positive= 73 negative = 56\n",
      "SGDClassifier****************************************************************\n",
      "positive= 67 negative = 33\n",
      "DecisionTreeClassifier****************************************************************\n",
      "positive= 146 negative = 49\n",
      "Naive****************************************************************\n",
      "positive= 0 negative = 0\n",
      "SVM****************************************************************\n",
      "positive= 79 negative = 57\n",
      "RandomForest_100****************************************************************\n",
      "positive= 53 negative = 39\n",
      "RandomForest_200****************************************************************\n",
      "positive= 46 negative = 44\n",
      "XGBClassifier****************************************************************\n",
      "positive= 60 negative = 52\n",
      "BaggingClassifier****************************************************************\n",
      "positive= 80 negative = 45\n",
      "AdaBoostClassifier****************************************************************\n",
      "positive= 81 negative = 45\n"
     ]
    }
   ],
   "source": [
    "print(\"The sign test...****************************************************************\")\n",
    "print(\"manhattan_k=5************************************ ****************************\")\n",
    "x= Naivetfidf \n",
    "y= manhattan_k5tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "        \n",
    "print(\"manhattan_k=13************************************ ****************************\")\n",
    "x= Naivetfidf \n",
    "y= manhattan_k13tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "print(\"MLPClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= MLPClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"LogisticRegression****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= LogisticRegressiontfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"SGDClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= SGDClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"DecisionTreeClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= DecisionTreeClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Naive****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= Naivetfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVM****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= SVMtfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_100****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= RandomForest_100tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"RandomForest_200****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= RandomForest_200tfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGBClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= XGBClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"BaggingClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= BaggingClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"AdaBoostClassifier****************************************************************\")\n",
    "x= Naivetfidf \n",
    "y= AdaBoostClassifiertfidf\n",
    "\n",
    "positive=0\n",
    "negative=0\n",
    "for i in range(len(x)):\n",
    "    if (x[i]!=y[i]):\n",
    "        if (x[i][0]==x[i][1] and y[i][0]!=y[i][1]):\n",
    "            positive=positive+1\n",
    "        else: \n",
    "            negative=negative+1\n",
    "print(\"positive=\",positive, \"negative =\" , negative)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
